// This file is part of OpenCV project.
// It is subject to the license terms in the LICENSE file found in the top-level
// directory of this distribution and at http://opencv.org/license.html.

#include "smooth_kelefouras.hpp"

namespace cv {
// after every 5 mask elements I insert a zero. This is because there is no mul
// command for 8bit in AVX/SSE. The only command I can use is maddubs which
// mults and adds the intermediate results. Thus, the 5th element will be added
// with the 6th which is always zero.
signed char gaussian_filter_5x5[9][32] __attribute__((aligned(64))) = {
    {2, 4, 5, 4, 2, 0, 2, 4, 5, 4, 2, 0, 2, 4, 5, 4,
     2, 0, 2, 4, 5, 4, 2, 0, 2, 4, 5, 4, 2, 0, 0, 0},
    {4, 9, 12, 9, 4,  0, 4, 9, 12, 9, 4,  0, 4, 9, 12, 9,
     4, 0, 4,  9, 12, 9, 4, 0, 4,  9, 12, 9, 4, 0, 0,  0},
    {5, 12, 15, 12, 5,  0,  5, 12, 15, 12, 5,  0,  5, 12, 15, 12,
     5, 0,  5,  12, 15, 12, 5, 0,  5,  12, 15, 12, 5, 0,  0,  0},

    {0, 2, 4, 5, 4, 2, 0, 2, 4, 5, 4, 2, 0, 2, 4, 5,
     4, 2, 0, 2, 4, 5, 4, 2, 0, 2, 4, 5, 4, 2, 0, 0},
    {0, 4, 9, 12, 9, 4,  0, 4, 9, 12, 9, 4,  0, 4, 9, 12,
     9, 4, 0, 4,  9, 12, 9, 4, 0, 4,  9, 12, 9, 4, 0, 0},
    {0,  5, 12, 15, 12, 5,  0,  5, 12, 15, 12, 5,  0,  5, 12, 15,
     12, 5, 0,  5,  12, 15, 12, 5, 0,  5,  12, 15, 12, 5, 0,  0},

    {0, 0, 2, 4, 5, 4, 2, 0, 2, 4, 5, 4, 2, 0, 2, 4,
     5, 4, 2, 0, 2, 4, 5, 4, 2, 0, 2, 4, 5, 4, 2, 0},
    {0,  0, 4, 9, 12, 9, 4,  0, 4, 9, 12, 9, 4,  0, 4, 9,
     12, 9, 4, 0, 4,  9, 12, 9, 4, 0, 4,  9, 12, 9, 4, 0},
    {0,  0,  5, 12, 15, 12, 5,  0,  5, 12, 15, 12, 5,  0,  5, 12,
     15, 12, 5, 0,  5,  12, 15, 12, 5, 0,  5,  12, 15, 12, 5, 0},
};

__attribute__((aligned(64))) unsigned short int
    f_vector[16]; // initialized by 'prepare_for_division()' routine, for 2D
                  // routines
__attribute__((aligned(64))) unsigned short int
    f_vector_x[16]; // initialized by 'prepare_for_division()' routine, for
                    // separable routines only
__attribute__((aligned(64))) unsigned short int
    f_vector_y[16]; // initialized by 'prepare_for_division()' routine, for
                    // separable routines only

unsigned int b;

// this is for 9x9 case Y mask only
const unsigned char reminder_mask_9x9[31][32] __attribute__((aligned(64))) = {
    // this lookup table is used in the loop reminder
    {255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 0,   0,   0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 0,   0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 0},
};

// this is for 7x7 case
const unsigned char reminder_mask_7x7[32][32] __attribute__((aligned(64))) = {
    // this lookup table is used in the loop reminder
    {255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 0,   0,   0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 0,   0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255},
};

// this is for 5x5 case
const unsigned char reminder_msk1[31][32] __attribute__((aligned(64))) = {
    // this lookup table is used in the loop reminder
    {255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 0,   0,   0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 0,   0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255},
};

// this is for 5x5 case
const unsigned char reminder_msk2[31][32] __attribute__((aligned(64))) = {
    // this lookup table is used in the loop reminder
    {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 0,   0,   0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 0,   0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 0,   0},
};

const unsigned char reminder_msk1_3x3[33][32] __attribute__((aligned(64))) = {
    // this lookup table is used in the loop reminder
    {255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0, 0, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 0, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 0,   0,   0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 0,   0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 0,   0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 0,   0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 0,   0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 0,   0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 0,   0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 0,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     0,   0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 0,   0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 0,   0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 0,   0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 0,   0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 0,   0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 0,   0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 0,   0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 0,   0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 0},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255},
    {255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
     255, 255, 255, 255, 255, 255, 255, 255, 255, 255},

};

const signed char gaussianMask_1d[5] = {1, 4, 6, 4, 1};

signed char gaussian_filter_5x5_seperable_y[6][32]
    __attribute__((aligned(64))) = {
        {1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,
         1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0},
        {4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0,
         4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0},
        {6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0,
         6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0},
        {0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,
         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1},
        {0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4,
         0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4, 0, 4},
        {0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6,
         0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6, 0, 6},
};

signed char gaussian_filter_5x5_seperable_x[6][32]
    __attribute__((aligned(64))) = {
        {1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4,
         1, 0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0, 0, 0},
        {0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0, 1, 4, 6,
         4, 1, 0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0, 0},
        {0, 0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0, 1, 4,
         6, 4, 1, 0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0},
        {0, 0, 0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0, 1,
         4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1},
        {0, 0, 0, 0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0,
         1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0, 0, 0, 0, 0},
        {0, 0, 0, 0, 0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1,
         0, 1, 4, 6, 4, 1, 0, 1, 4, 6, 4, 1, 0, 0, 0, 0},
};

// (x/divisor) is transformed into ((x * ceil(f) ) >> w+b) by using the
// following algorithm. Since the divisor does not change I apply the following
// procedure just once
unsigned int prepare_for_division(const unsigned short int divisor) {
  /*
   * * Mathematical formula, used for unsigned division with fixed divisor:
   * (From Terje Mathisen, unpublished)
   * x = dividend
   * d = divisor
   * w = integer word size, bits
   * b = floor(log2(d)) = bit_scan_reverse(d)
   * f = 2^(w+b) / d                                [exact division]
   * If f is an integer then d is a power of 2 then go to case A
   * If the fractional part of f is < 0.5 then go to case B
   * If the fractional part of f is > 0.5 then go to case C
   * Case A:  [shift only]
   * result = x >> b
   * Case B:  [round down f and compensate by adding one to x]
   * result = ((x+1)*floor(f)) >> (w+b)             [high part of unsigned
   * multiplication with 2w bits] Case C:  [round up f, no compensation for
   * rounding error] result = (x*ceil(f)) >> (w+b)                  [high part
   * of unsigned multiplication with 2w bits]
   *
   */
  __m256i f_vec;
  unsigned short int tmp;

  if (divisor == 0) {
    printf("\n cannot divide with zero, process is terminated");
    exit(EXIT_FAILURE);
  } else if (divisor == 1)
    return 4;

  const unsigned int w =
      16; // integer word size in bits of the division operands
  b = (unsigned int)floorf(log2f(divisor));
  float f = powf(2, w + b) / divisor;
  int integer_part_f = (int)f;
  float float_part_f = f - integer_part_f;

  if (float_part_f < 0.0001) { // if f==0.0
    return 1;                  // case A
  } else if (float_part_f < 0.5) {
    tmp = (unsigned short int)floorf(f);
    f_vec = _mm256_set1_epi16(tmp);
    _mm256_store_si256((__m256i *)&f_vector[0], f_vec);
    return 2; // case B
  } else {
    tmp = (unsigned short int)ceilf(f);
    f_vec = _mm256_set1_epi16(tmp);
    _mm256_store_si256((__m256i *)&f_vector[0], f_vec);
    return 3; // case C
  }
}

inline __m256i division(const unsigned int division_case, __m256i m2,
                        const __m256i f) {
  __m256i m1, m3;

  if (division_case == 1) {            // case A
    return (_mm256_srli_epi16(m2, b)); // shift right logical with b
  } else if (division_case == 2) {     // case B
    m2 = _mm256_add_epi16(m2, _mm256_set1_epi16(1)); // m2=m2+1

    m3 = _mm256_mulhi_epu16(m2, f);    // multiply high unsigned words
    m1 = _mm256_sub_epi16(m2, m3);     // subtract
    m1 = _mm256_srli_epi16(m1, 16);    // shift right logical with w
    m3 = _mm256_add_epi16(m3, m1);     // add
    return (_mm256_srli_epi16(m3, b)); // shift right logical with b
  } else if (division_case == 3) {     // case C
    m3 = _mm256_mulhi_epu16(m2, f);    // multiply high unsigned words
    m1 = _mm256_sub_epi16(m2, m3);     // subtract
    m1 = _mm256_srli_epi16(m1, 16);    // shift right logical with w
    m3 = _mm256_add_epi16(m3, m1);     // add
    return (_mm256_srli_epi16(m3, b)); // shift right logical with b
  } else                               // division with 1
    return m2;
}

void Gaussian_Blur_optimized_5x5_16_reg_blocking(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned short int divisor,
    signed char **filter5x5) {

  const signed char f00 = filter5x5[0][0];
  const signed char f01 = filter5x5[0][1];
  const signed char f02 = filter5x5[0][2];
  const signed char f03 = filter5x5[0][3];
  const signed char f04 = filter5x5[0][4];

  const signed char f10 = filter5x5[1][0];
  const signed char f11 = filter5x5[1][1];
  const signed char f12 = filter5x5[1][2];
  const signed char f13 = filter5x5[1][3];
  const signed char f14 = filter5x5[1][4];

  const signed char f20 = filter5x5[2][0];
  const signed char f21 = filter5x5[2][1];
  const signed char f22 = filter5x5[2][2];
  const signed char f23 = filter5x5[2][3];
  const signed char f24 = filter5x5[2][4];

  const __m256i c0 = _mm256_set_epi8(
      0, 0, 0, f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0, f04, f03,
      f02, f01, f00, 0, f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00);
  const __m256i c1 = _mm256_set_epi8(
      0, 0, 0, f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0, f14, f13,
      f12, f11, f10, 0, f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10);
  const __m256i c2 = _mm256_set_epi8(
      0, 0, 0, f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0, f24, f23,
      f22, f21, f20, 0, f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20);

  const __m256i c0_sh1 = _mm256_set_epi8(
      0, 0, f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0, f04, f03,
      f02, f01, f00, 0, f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0);
  const __m256i c1_sh1 = _mm256_set_epi8(
      0, 0, f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0, f14, f13,
      f12, f11, f10, 0, f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0);
  const __m256i c2_sh1 = _mm256_set_epi8(
      0, 0, f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0, f24, f23,
      f22, f21, f20, 0, f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0);

  const __m256i c0_sh2 = _mm256_set_epi8(
      0, f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0, f04, f03, f02,
      f01, f00, 0, f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0, 0);
  const __m256i c1_sh2 = _mm256_set_epi8(
      0, f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0, f14, f13, f12,
      f11, f10, 0, f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0, 0);
  const __m256i c2_sh2 = _mm256_set_epi8(
      0, f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0, f24, f23, f22,
      f21, f20, 0, f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0, 0);

  const __m256i c0_sh3 =
      _mm256_set_epi8(f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0,
                      f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0,
                      f04, f03, f02, f01, f00, 0, 0, 0);
  const __m256i c1_sh3 =
      _mm256_set_epi8(f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0,
                      f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0,
                      f14, f13, f12, f11, f10, 0, 0, 0);
  const __m256i c2_sh3 =
      _mm256_set_epi8(f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0,
                      f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0,
                      f24, f23, f22, f21, f20, 0, 0, 0);

  const __m256i c0_sh4 = _mm256_set_epi8(
      0, 0, 0, 0, 0, f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0,
      f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0, 0, 0, 0);
  const __m256i c1_sh4 = _mm256_set_epi8(
      0, 0, 0, 0, 0, f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0,
      f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0, 0, 0, 0);
  const __m256i c2_sh4 = _mm256_set_epi8(
      0, 0, 0, 0, 0, f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0,
      f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0, 0, 0, 0);

  const __m256i c0_sh5 = _mm256_set_epi8(
      0, 0, 0, 0, f04, f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0, f04,
      f03, f02, f01, f00, 0, f04, f03, f02, f01, f00, 0, 0, 0, 0, 0);
  const __m256i c1_sh5 = _mm256_set_epi8(
      0, 0, 0, 0, f14, f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0, f14,
      f13, f12, f11, f10, 0, f14, f13, f12, f11, f10, 0, 0, 0, 0, 0);
  const __m256i c2_sh5 = _mm256_set_epi8(
      0, 0, 0, 0, f24, f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0, f24,
      f23, f22, f21, f20, 0, f24, f23, f22, f21, f20, 0, 0, 0, 0, 0);

  // const __m256i f  =
  // _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
  // const __m256i mask  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
  // const __m256i mask2  =
  // _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
  // const __m256i mask6  =
  // _mm256_set_epi8(0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0);

  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i output_mask = _mm256_set_epi16(
      0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535);
  const __m256i output_mask_sh1 = _mm256_set_epi16(
      0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0);
  const __m256i output_mask_sh2 = _mm256_set_epi16(
      0, 0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0);

  const unsigned int REMINDER_ITERATIONS =
      (M - ((((M - 32) / 28) * 28) + 28)); // M-(last_col_value+28)
  // printf("\n%d",REMINDER_ITERATIONS);

  const unsigned int division_case = prepare_for_division(
      divisor); // determine which is the division case (A, B or C)
  const __m256i f = _mm256_load_si256(
      (__m256i *)&f_vector[0]); // initialize the division vector

  // printf("\n%d %d",division_case,b);

#pragma omp parallel
  {

    unsigned int row, col;
    register __m256i r0, r1, r2, r3, r4, r5, m0, m1, m2, m3, m4, m5;
    __m256i output_row0_even, output_row1_even, output_row0_odd,
        output_row1_odd; // these are for processing row #0 and #1 only.
    __m256i output_even, output_odd;
    __m256i m0_prelude_row0, m0_prelude_row1;

    /*---------------------- Gaussian Blur ---------------------------------*/

#pragma omp for schedule(static)
    for (row = 1; row <= N - 2; row += 2) {

      if (row == 1) { // in this case I calculate filt[0][:] and filt[1][:] too.
                      // No extra loads or multiplications are required for this
        for (col = 0; col <= M - 32; col += 28) {

          if (col == 0) { // this is a special case as the mask gets outside of
                          // the array (it is like adding two zeros in the
                          // beginning of frame[][]

            // load the 5 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[0][0]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[1][0]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[2][0]);
            r3 = _mm256_loadu_si256((__m256i *)&frame1[3][0]);
            r4 = _mm256_loadu_si256((__m256i *)&frame1[4][0]);

            // START - extra code needed for prelude
            m0 = _mm256_slli_si256(r0,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m1 = _mm256_slli_si256(r1,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m2 = _mm256_slli_si256(r2,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m3 = _mm256_slli_si256(r3,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m4 = _mm256_slli_si256(r4,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning

            r0 = _mm256_and_si256(r0, mask_prelude);
            r0 = _mm256_permute2f128_si256(r0, r0, 1);
            r0 = _mm256_srli_si256(r0, 14); // shift 14 elements
            r0 = _mm256_add_epi16(m0, r0);

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 14); // shift 14 elements
            r1 = _mm256_add_epi16(m1, r1);

            r2 = _mm256_and_si256(r2, mask_prelude);
            r2 = _mm256_permute2f128_si256(r2, r2, 1);
            r2 = _mm256_srli_si256(r2, 14); // shift 14 elements
            r2 = _mm256_add_epi16(m2, r2);

            r3 = _mm256_and_si256(r3, mask_prelude);
            r3 = _mm256_permute2f128_si256(r3, r3, 1);
            r3 = _mm256_srli_si256(r3, 14); // shift 14 elements
            r3 = _mm256_add_epi16(m3, r3);

            r4 = _mm256_and_si256(r4, mask_prelude);
            r4 = _mm256_permute2f128_si256(r4, r4, 1);
            r4 = _mm256_srli_si256(r4, 14); // shift 14 elements
            r4 = _mm256_add_epi16(m4, r4);

            // END - extra code needed for prelude
          } else {
            // load the 5 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col - 2]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col - 2]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col - 2]);
            r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col - 2]);
            r4 = _mm256_loadu_si256((__m256i *)&frame1[4][col - 2]);
          }

          // col iteration computes output pixels of 2,8,14,20,26
          //  col+1 iteration computes output pixels of 3,9,15,21,27
          //  col+2 iteration computes output pixels of 4,10,16,22,28
          //  col+3 iteration computes output pixels of 5,11,17,23,29
          //  col+4 iteration computes output pixels of 6,12,18,24,30
          //  col+5 iteration computes output pixels of 7,13,19,25,31
          // afterwards, col2 becomes 32 and repeat the above process

          // 1st col iteration

          //--------------row=2
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0);
          m1 = _mm256_maddubs_epi16(r1, c1);
          m2 = _mm256_maddubs_epi16(r2, c2);
          m3 = _mm256_maddubs_epi16(r3, c1);
          m4 = _mm256_maddubs_epi16(r4, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_even = _mm256_and_si256(m2, output_mask);

          //--------------row=0
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c2);
          m1 = _mm256_maddubs_epi16(r1, c1);
          m2 = _mm256_maddubs_epi16(r2, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row0_even = _mm256_and_si256(m2, output_mask);

          //--------------row=1
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c1);
          m1 = _mm256_maddubs_epi16(r1, c2);
          m2 = _mm256_maddubs_epi16(r2, c1);
          m3 = _mm256_maddubs_epi16(r3, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row1_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask

          //----row=2
          m0 = _mm256_maddubs_epi16(r0, c0_sh1);
          m1 = _mm256_maddubs_epi16(r1, c1_sh1);
          m2 = _mm256_maddubs_epi16(r2, c2_sh1);
          m3 = _mm256_maddubs_epi16(r3, c1_sh1);
          m4 = _mm256_maddubs_epi16(r4, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_odd = _mm256_and_si256(m2, output_mask);

          //----row=0
          m0 = _mm256_maddubs_epi16(r0, c2_sh1);
          m1 = _mm256_maddubs_epi16(r1, c1_sh1);
          m2 = _mm256_maddubs_epi16(r2, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row0_odd = _mm256_and_si256(m2, output_mask);

          //----row=1
          m0 = _mm256_maddubs_epi16(r0, c1_sh1);
          m1 = _mm256_maddubs_epi16(r1, c2_sh1);
          m2 = _mm256_maddubs_epi16(r2, c1_sh1);
          m3 = _mm256_maddubs_epi16(r3, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row1_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          //---row=2
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh2);
          m1 = _mm256_maddubs_epi16(r1, c1_sh2);
          m2 = _mm256_maddubs_epi16(r2, c2_sh2);
          m3 = _mm256_maddubs_epi16(r3, c1_sh2);
          m4 = _mm256_maddubs_epi16(r4, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
          m4 = _mm256_and_si256(m2, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          //---row=0
          // multiply with the mask

          m0 = _mm256_maddubs_epi16(r0, c2_sh2);
          m1 = _mm256_maddubs_epi16(r1, c1_sh2);
          m2 = _mm256_maddubs_epi16(r2, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
          m4 = _mm256_and_si256(m2, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw
          // diairesi me ola mazi

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row0_even = _mm256_add_epi16(output_row0_even, m2);

          //---row=1
          // multiply with the mask

          m0 = _mm256_maddubs_epi16(r0, c1_sh2);
          m1 = _mm256_maddubs_epi16(r1, c2_sh2);
          m2 = _mm256_maddubs_epi16(r2, c1_sh2);
          m3 = _mm256_maddubs_epi16(r3, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
          m4 = _mm256_and_si256(m2, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw
          // diairesi me ola mazi

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row1_even = _mm256_add_epi16(output_row1_even, m2);

          // 4th col iteration

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh3);
          m1 = _mm256_maddubs_epi16(r1, c1_sh3);
          m2 = _mm256_maddubs_epi16(r2, c2_sh3);
          m3 = _mm256_maddubs_epi16(r3, c1_sh3);
          m4 = _mm256_maddubs_epi16(r4, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(3:8)
          // hadd(9:14)
          // hadd(15:20)
          // hadd(21:26)
          // hadd(27:31)
          // result after division will be in 2,8,14,20,26

          m1 = _mm256_srli_si256(m0, 2);
          m4 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m4);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m4, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // row0
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c2_sh3);
          m1 = _mm256_maddubs_epi16(r1, c1_sh3);
          m2 = _mm256_maddubs_epi16(r2, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          m1 = _mm256_srli_si256(m0, 2);
          m4 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m4);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m4, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

          //-----------row1
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c1_sh3);
          m1 = _mm256_maddubs_epi16(r1, c2_sh3);
          m2 = _mm256_maddubs_epi16(r2, c1_sh3);
          m3 = _mm256_maddubs_epi16(r3, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);

          m1 = _mm256_srli_si256(m0, 2);
          m4 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m4);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m4, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

          // 5th col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh4);
          m1 = _mm256_maddubs_epi16(r1, c1_sh4);
          m2 = _mm256_maddubs_epi16(r2, c2_sh4);
          m3 = _mm256_maddubs_epi16(r3, c1_sh4);
          m4 = _mm256_maddubs_epi16(r4, c0_sh4);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(4:9)
          // hadd(10:15)
          // hadd(16:21)
          // hadd(22:27)
          // result after division will be in 4,10,16,22

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_even = _mm256_add_epi16(output_even, m2);

          // row0
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c2_sh4);
          m1 = _mm256_maddubs_epi16(r1, c1_sh4);
          m2 = _mm256_maddubs_epi16(r2, c0_sh4);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          // hadd(4:9)
          // hadd(10:15)
          // hadd(16:21)
          // hadd(22:27)
          // result after division will be in 4,10,16,22

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row0_even = _mm256_add_epi16(output_row0_even, m2);

          // row1
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c1_sh4);
          m1 = _mm256_maddubs_epi16(r1, c2_sh4);
          m2 = _mm256_maddubs_epi16(r2, c1_sh4);
          m3 = _mm256_maddubs_epi16(r3, c0_sh4);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row1_even = _mm256_add_epi16(output_row1_even, m2);

          // 6th col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh5);
          m1 = _mm256_maddubs_epi16(r1, c1_sh5);
          m2 = _mm256_maddubs_epi16(r2, c2_sh5);
          m3 = _mm256_maddubs_epi16(r3, c1_sh5);
          m4 = _mm256_maddubs_epi16(r4, c0_sh5);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(5:10)
          // hadd(11:16)
          // hadd(17:22)
          // hadd(23:28)
          // result after division will be in 4,10,16,22

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[2][col], output_even);

          // row0
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c2_sh5);
          m1 = _mm256_maddubs_epi16(r1, c1_sh5);
          m2 = _mm256_maddubs_epi16(r2, c0_sh5);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

          // now division follows
          output_row0_even = division(division_case, output_row0_even, f);
          output_row0_odd = division(division_case, output_row0_odd, f);

          // shift odd 1 position and add to even
          output_row0_odd = _mm256_slli_si256(output_row0_odd, 1);
          output_row0_even = _mm256_add_epi8(output_row0_even, output_row0_odd);

          _mm256_storeu_si256((__m256i *)&filt[0][col], output_row0_even);

          // row1

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c1_sh5);
          m1 = _mm256_maddubs_epi16(r1, c2_sh5);
          m2 = _mm256_maddubs_epi16(r2, c1_sh5);
          m3 = _mm256_maddubs_epi16(r3, c0_sh5);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

          // now division follows
          output_row1_even = division(division_case, output_row1_even, f);
          output_row1_odd = division(division_case, output_row1_odd, f);

          // shift odd 1 position and add to even
          output_row1_odd = _mm256_slli_si256(output_row1_odd, 1);
          output_row1_even = _mm256_add_epi8(output_row1_even, output_row1_odd);

          _mm256_storeu_si256((__m256i *)&filt[1][col], output_row1_even);
        }

        loop_reminder_first_less_div(frame1, filt, M, N, col,
                                     REMINDER_ITERATIONS, division_case, c0, c1,
                                     c2, c0_sh1, c1_sh1, c2_sh1, c0_sh2, c1_sh2,
                                     c2_sh2, f, divisor, filter5x5);
      }

      else if ((row == N - 3)) { // in this case I calculate filt[N-3][:],
                                 // filt[N-2][:] and filt[N-1][:]. Below row0
                                 // refers to row=N-2 and row1 refers to row=N-1
        // printf("\nN=%d row=%d N-2=%d",N,row,N-2);
        for (col = 0; col <= M - 32; col += 28) {
          // last col value that does not read outside of the array bounds is
          // (col<M-29)

          if (col == 0) {

            // load the 5 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 5][0]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][0]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][0]);
            r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][0]);
            r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][0]);

            // START - extra code needed for prelude
            m0 = _mm256_slli_si256(r0,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m1 = _mm256_slli_si256(r1,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m2 = _mm256_slli_si256(r2,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m3 = _mm256_slli_si256(r3,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m4 = _mm256_slli_si256(r4,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning

            r0 = _mm256_and_si256(r0, mask_prelude);
            r0 = _mm256_permute2f128_si256(r0, r0, 1);
            r0 = _mm256_srli_si256(r0, 14); // shift 14 elements
            r0 = _mm256_add_epi16(m0, r0);

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 14); // shift 14 elements
            r1 = _mm256_add_epi16(m1, r1);

            r2 = _mm256_and_si256(r2, mask_prelude);
            r2 = _mm256_permute2f128_si256(r2, r2, 1);
            r2 = _mm256_srli_si256(r2, 14); // shift 14 elements
            r2 = _mm256_add_epi16(m2, r2);

            r3 = _mm256_and_si256(r3, mask_prelude);
            r3 = _mm256_permute2f128_si256(r3, r3, 1);
            r3 = _mm256_srli_si256(r3, 14); // shift 14 elements
            r3 = _mm256_add_epi16(m3, r3);

            r4 = _mm256_and_si256(r4, mask_prelude);
            r4 = _mm256_permute2f128_si256(r4, r4, 1);
            r4 = _mm256_srli_si256(r4, 14); // shift 14 elements
            r4 = _mm256_add_epi16(m4, r4);

            // END - extra code needed for prelude
          } else {
            // load the 5 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 5][col - 2]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col - 2]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col - 2]);
            r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 2]);
            r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 2]);
          }

          // col iteration computes output pixels of 2,8,14,20,26
          //  col+1 iteration computes output pixels of 3,9,15,21,27
          //  col+2 iteration computes output pixels of 4,10,16,22,28
          //  col+3 iteration computes output pixels of 5,11,17,23,29
          //  col+4 iteration computes output pixels of 6,12,18,24,30
          //  col+5 iteration computes output pixels of 7,13,19,25,31
          // afterwards, col2 becomes 32 and repeat the above process

          // 1st col iteration
          //--------------row=2

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0);
          m1 = _mm256_maddubs_epi16(r1, c1);
          m2 = _mm256_maddubs_epi16(r2, c2);
          m3 = _mm256_maddubs_epi16(r3, c1);
          m4 = _mm256_maddubs_epi16(r4, c0);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m4 = _mm256_add_epi16(m4, m1);
          m0 = _mm256_add_epi16(m4, m0);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_even = _mm256_and_si256(m2, output_mask);

          //--------------row=0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0);
          m2 = _mm256_maddubs_epi16(r2, c1);
          m3 = _mm256_maddubs_epi16(r3, c2);
          m4 = _mm256_maddubs_epi16(r4, c1);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m4 = _mm256_add_epi16(m4, m1);
          m0_prelude_row0 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row0, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row0);

          m1 = _mm256_srli_si256(m0_prelude_row0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0_prelude_row0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row0_even = _mm256_and_si256(m2, output_mask);

          //--------------row=1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0);
          m3 = _mm256_maddubs_epi16(r3, c1);
          m4 = _mm256_maddubs_epi16(r4, c2);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m0_prelude_row1 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row1, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row1);

          m1 = _mm256_srli_si256(m0_prelude_row1, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0_prelude_row1, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row1_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          //----row=2

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh1);
          m1 = _mm256_maddubs_epi16(r1, c1_sh1);
          m2 = _mm256_maddubs_epi16(r2, c2_sh1);
          m3 = _mm256_maddubs_epi16(r3, c1_sh1);
          m4 = _mm256_maddubs_epi16(r4, c0_sh1);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m4 = _mm256_add_epi16(m4, m1);
          m0 = _mm256_add_epi16(m4, m0);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_odd = _mm256_and_si256(m2, output_mask);

          //----row=0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh1);
          m2 = _mm256_maddubs_epi16(r2, c1_sh1);
          m3 = _mm256_maddubs_epi16(r3, c2_sh1);
          m4 = _mm256_maddubs_epi16(r4, c1_sh1);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m4 = _mm256_add_epi16(m4, m1);
          m0_prelude_row0 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row0, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row0);

          m1 = _mm256_srli_si256(m0_prelude_row0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0_prelude_row0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row0_odd = _mm256_and_si256(m2, output_mask);

          //----row=1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0_sh1);
          m3 = _mm256_maddubs_epi16(r3, c1_sh1);
          m4 = _mm256_maddubs_epi16(r4, c2_sh1);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m0_prelude_row1 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row1, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row1);

          m1 = _mm256_srli_si256(m0_prelude_row1, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0_prelude_row1, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row1_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          //---row=2
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh2);
          m1 = _mm256_maddubs_epi16(r1, c1_sh2);
          m2 = _mm256_maddubs_epi16(r2, c2_sh2);
          m3 = _mm256_maddubs_epi16(r3, c1_sh2);
          m4 = _mm256_maddubs_epi16(r4, c0_sh2);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m4 = _mm256_add_epi16(m4, m1);
          m0 = _mm256_add_epi16(m4, m0);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
          m4 = _mm256_and_si256(m2, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw
          // diairesi me ola mazi

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          //---row=0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh2);
          m2 = _mm256_maddubs_epi16(r2, c1_sh2);
          m3 = _mm256_maddubs_epi16(r3, c2_sh2);
          m4 = _mm256_maddubs_epi16(r4, c1_sh2);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m4 = _mm256_add_epi16(m4, m1);
          m0_prelude_row0 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row0, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row0);

          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
          m4 = _mm256_and_si256(m2, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m1 = _mm256_srli_si256(m0_prelude_row0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw
          // diairesi me ola mazi

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row0_even = _mm256_add_epi16(output_row0_even, m2);

          //---row=1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0_sh2);
          m3 = _mm256_maddubs_epi16(r3, c1_sh2);
          m4 = _mm256_maddubs_epi16(r4, c2_sh2);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m0_prelude_row1 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row1, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row1);

          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
          m4 = _mm256_and_si256(m2, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m1 = _mm256_srli_si256(m0_prelude_row1, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw
          // diairesi me ola mazi

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row1_even = _mm256_add_epi16(output_row1_even, m2);

          // 4th col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh3);
          m1 = _mm256_maddubs_epi16(r1, c1_sh3);
          m2 = _mm256_maddubs_epi16(r2, c2_sh3);
          m3 = _mm256_maddubs_epi16(r3, c1_sh3);
          m4 = _mm256_maddubs_epi16(r4, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(3:8)
          // hadd(9:14)
          // hadd(15:20)
          // hadd(21:26)
          // hadd(27:31)
          // result after division will be in 2,8,14,20,26

          m1 = _mm256_srli_si256(m0, 2);
          m4 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m4);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m4, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // row0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh3);
          m2 = _mm256_maddubs_epi16(r2, c1_sh3);
          m3 = _mm256_maddubs_epi16(r3, c2_sh3);
          m4 = _mm256_maddubs_epi16(r4, c1_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m4 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m4);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m4, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

          // row1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0_sh3);
          m3 = _mm256_maddubs_epi16(r3, c1_sh3);
          m4 = _mm256_maddubs_epi16(r4, c2_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m4 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m4);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m4, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

          // 5th col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh4);
          m1 = _mm256_maddubs_epi16(r1, c1_sh4);
          m2 = _mm256_maddubs_epi16(r2, c2_sh4);
          m3 = _mm256_maddubs_epi16(r3, c1_sh4);
          m4 = _mm256_maddubs_epi16(r4, c0_sh4);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(4:9)
          // hadd(10:15)
          // hadd(16:21)
          // hadd(22:27)
          // result after division will be in 4,10,16,22

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_even = _mm256_add_epi16(output_even, m2);

          // row0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh4);
          m2 = _mm256_maddubs_epi16(r2, c1_sh4);
          m3 = _mm256_maddubs_epi16(r3, c2_sh4);
          m4 = _mm256_maddubs_epi16(r4, c1_sh4);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row0_even = _mm256_add_epi16(output_row0_even, m2);

          // row1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0_sh4);
          m3 = _mm256_maddubs_epi16(r3, c1_sh4);
          m4 = _mm256_maddubs_epi16(r4, c2_sh4);

          // vertical add
          m0 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row1_even = _mm256_add_epi16(output_row1_even, m2);

          // 6th col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh5);
          m1 = _mm256_maddubs_epi16(r1, c1_sh5);
          m2 = _mm256_maddubs_epi16(r2, c2_sh5);
          m3 = _mm256_maddubs_epi16(r3, c1_sh5);
          m4 = _mm256_maddubs_epi16(r4, c0_sh5);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(5:10)
          // hadd(11:16)
          // hadd(17:22)
          // hadd(23:28)
          // result after division will be in 4,10,16,22

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 3][col], output_even);

          // row0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh5);
          m2 = _mm256_maddubs_epi16(r2, c1_sh5);
          m3 = _mm256_maddubs_epi16(r3, c2_sh5);
          m4 = _mm256_maddubs_epi16(r4, c1_sh5);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

          // now division follows
          output_row0_even = division(division_case, output_row0_even, f);
          output_row0_odd = division(division_case, output_row0_odd, f);

          // shift odd 1 position and add to even
          output_row0_odd = _mm256_slli_si256(output_row0_odd, 1);
          output_row0_even = _mm256_add_epi8(output_row0_even, output_row0_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 2][col], output_row0_even);

          // row1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0_sh5);
          m3 = _mm256_maddubs_epi16(r3, c1_sh5);
          m4 = _mm256_maddubs_epi16(r4, c2_sh5);

          // vertical add
          m0 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

          // now division follows
          output_row1_even = division(division_case, output_row1_even, f);
          output_row1_odd = division(division_case, output_row1_odd, f);

          // shift odd 1 position and add to even
          output_row1_odd = _mm256_slli_si256(output_row1_odd, 1);
          output_row1_even = _mm256_add_epi8(output_row1_even, output_row1_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 1][col], output_row1_even);
        }

        loop_reminder_last_less_div(frame1, filt, M, N, col,
                                    REMINDER_ITERATIONS, division_case, c0, c1,
                                    c2, c0_sh1, c1_sh1, c2_sh1, c0_sh2, c1_sh2,
                                    c2_sh2, f, divisor, filter5x5);

      }
      // an extra condition is needed because of register blocking
      else if ((row == N - 2)) { // in this case I calculate just filt[N-2][:]
                                 // and filt[N-1][:]. Below row0 refers to
                                 // row=N-2 and row1 refers to row=N-1
        // printf("\nN=%d row=%d N-2=%d",N,row,N-2);
        for (col = 0; col <= M - 32; col += 28) {
          // last col value that does not read outside of the array bounds is
          // (col<M-29)

          if (col == 0) {

            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][0]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][0]);
            r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][0]);
            r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][0]);

            // START - extra code needed for prelude
            m1 = _mm256_slli_si256(r1,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m2 = _mm256_slli_si256(r2,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m3 = _mm256_slli_si256(r3,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m4 = _mm256_slli_si256(r4,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 14); // shift 14 elements
            r1 = _mm256_add_epi16(m1, r1);

            r2 = _mm256_and_si256(r2, mask_prelude);
            r2 = _mm256_permute2f128_si256(r2, r2, 1);
            r2 = _mm256_srli_si256(r2, 14); // shift 14 elements
            r2 = _mm256_add_epi16(m2, r2);

            r3 = _mm256_and_si256(r3, mask_prelude);
            r3 = _mm256_permute2f128_si256(r3, r3, 1);
            r3 = _mm256_srli_si256(r3, 14); // shift 14 elements
            r3 = _mm256_add_epi16(m3, r3);

            r4 = _mm256_and_si256(r4, mask_prelude);
            r4 = _mm256_permute2f128_si256(r4, r4, 1);
            r4 = _mm256_srli_si256(r4, 14); // shift 14 elements
            r4 = _mm256_add_epi16(m4, r4);

            // END - extra code needed for prelude
          } else {

            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col - 2]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col - 2]);
            r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 2]);
            r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 2]);
          }

          // col iteration computes output pixels of 2,8,14,20,26
          //  col+1 iteration computes output pixels of 3,9,15,21,27
          //  col+2 iteration computes output pixels of 4,10,16,22,28
          //  col+3 iteration computes output pixels of 5,11,17,23,29
          //  col+4 iteration computes output pixels of 6,12,18,24,30
          //  col+5 iteration computes output pixels of 7,13,19,25,31
          // afterwards, col2 becomes 32 and repeat the above process

          // 1st col iteration

          //--------------row=0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0);
          m2 = _mm256_maddubs_epi16(r2, c1);
          m3 = _mm256_maddubs_epi16(r3, c2);
          m4 = _mm256_maddubs_epi16(r4, c1);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m4 = _mm256_add_epi16(m4, m1);
          m0_prelude_row0 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row0, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row0);

          m1 = _mm256_srli_si256(m0_prelude_row0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0_prelude_row0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row0_even = _mm256_and_si256(m2, output_mask);

          //--------------row=1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0);
          m3 = _mm256_maddubs_epi16(r3, c1);
          m4 = _mm256_maddubs_epi16(r4, c2);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m0_prelude_row1 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row1, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row1);

          m1 = _mm256_srli_si256(m0_prelude_row1, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0_prelude_row1, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row1_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration

          //----row=0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh1);
          m2 = _mm256_maddubs_epi16(r2, c1_sh1);
          m3 = _mm256_maddubs_epi16(r3, c2_sh1);
          m4 = _mm256_maddubs_epi16(r4, c1_sh1);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m4 = _mm256_add_epi16(m4, m1);
          m0_prelude_row0 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row0, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row0);

          m1 = _mm256_srli_si256(m0_prelude_row0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0_prelude_row0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row0_odd = _mm256_and_si256(m2, output_mask);

          //----row=1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0_sh1);
          m3 = _mm256_maddubs_epi16(r3, c1_sh1);
          m4 = _mm256_maddubs_epi16(r4, c2_sh1);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m0_prelude_row1 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row1, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row1);

          m1 = _mm256_srli_si256(m0_prelude_row1, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m0_prelude_row1, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_row1_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration

          //---row=0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh2);
          m2 = _mm256_maddubs_epi16(r2, c1_sh2);
          m3 = _mm256_maddubs_epi16(r3, c2_sh2);
          m4 = _mm256_maddubs_epi16(r4, c1_sh2);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m4 = _mm256_add_epi16(m4, m1);
          m0_prelude_row0 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row0, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row0);

          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
          m4 = _mm256_and_si256(m2, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m1 = _mm256_srli_si256(m0_prelude_row0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw
          // diairesi me ola mazi

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row0_even = _mm256_add_epi16(output_row0_even, m2);

          //---row=1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0_sh2);
          m3 = _mm256_maddubs_epi16(r3, c1_sh2);
          m4 = _mm256_maddubs_epi16(r4, c2_sh2);

          // vertical add
          m4 = _mm256_add_epi16(m4, m3);
          m4 = _mm256_add_epi16(m4, m2);
          m0_prelude_row1 = m4;

          m1 = _mm256_srli_si256(m0_prelude_row1, 2);
          m2 = _mm256_add_epi16(m1, m0_prelude_row1);

          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
          m4 = _mm256_and_si256(m2, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m1 = _mm256_srli_si256(m0_prelude_row1, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw
          // diairesi me ola mazi

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row1_even = _mm256_add_epi16(output_row1_even, m2);

          // 4th col iteration

          // row0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh3);
          m2 = _mm256_maddubs_epi16(r2, c1_sh3);
          m3 = _mm256_maddubs_epi16(r3, c2_sh3);
          m4 = _mm256_maddubs_epi16(r4, c1_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m4 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m4);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m4, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

          // row1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0_sh3);
          m3 = _mm256_maddubs_epi16(r3, c1_sh3);
          m4 = _mm256_maddubs_epi16(r4, c2_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m4 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m4);

          // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw
          // diairesi me ola mazi
          // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
          m4 = _mm256_and_si256(m4, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

          // 5th col iteration

          // row0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh4);
          m2 = _mm256_maddubs_epi16(r2, c1_sh4);
          m3 = _mm256_maddubs_epi16(r3, c2_sh4);
          m4 = _mm256_maddubs_epi16(r4, c1_sh4);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row0_even = _mm256_add_epi16(output_row0_even, m2);

          // row1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0_sh4);
          m3 = _mm256_maddubs_epi16(r3, c1_sh4);
          m4 = _mm256_maddubs_epi16(r4, c2_sh4);

          // vertical add
          m0 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row1_even = _mm256_add_epi16(output_row1_even, m2);

          // 6th col iteration

          // row0
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh5);
          m2 = _mm256_maddubs_epi16(r2, c1_sh5);
          m3 = _mm256_maddubs_epi16(r3, c2_sh5);
          m4 = _mm256_maddubs_epi16(r4, c1_sh5);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

          // now division follows
          output_row0_even = division(division_case, output_row0_even, f);
          output_row0_odd = division(division_case, output_row0_odd, f);

          // shift odd 1 position and add to even
          output_row0_odd = _mm256_slli_si256(output_row0_odd, 1);
          output_row0_even = _mm256_add_epi8(output_row0_even, output_row0_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 2][col], output_row0_even);

          // row1
          // multiply with the mask
          m2 = _mm256_maddubs_epi16(r2, c0_sh5);
          m3 = _mm256_maddubs_epi16(r3, c1_sh5);
          m4 = _mm256_maddubs_epi16(r4, c2_sh5);

          // vertical add
          m0 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m4);

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

          // now division follows
          output_row1_even = division(division_case, output_row1_even, f);
          output_row1_odd = division(division_case, output_row1_odd, f);

          // shift odd 1 position and add to even
          output_row1_odd = _mm256_slli_si256(output_row1_odd, 1);
          output_row1_even = _mm256_add_epi8(output_row1_even, output_row1_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 1][col], output_row1_even);
        }

        loop_reminder_last_less_div_special_case(
            frame1, filt, M, N, col, REMINDER_ITERATIONS, division_case, c0, c1,
            c2, c0_sh1, c1_sh1, c2_sh1, c0_sh2, c1_sh2, c2_sh2, f, divisor,
            filter5x5);

      }

      else {

        for (col = 0; col <= M - 32; col += 28) {
          // last col value that does not read outside of the array bounds is
          // (col<M-29)

          if (col == 0) {

            // load the 5 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 2][0]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][0]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[row][0]);
            r3 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][0]);
            r4 = _mm256_loadu_si256((__m256i *)&frame1[row + 2][0]);
            r5 = _mm256_loadu_si256((__m256i *)&frame1[row + 3][0]);

            // START - extra code needed for prelude
            m0 = _mm256_slli_si256(r0,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m1 = _mm256_slli_si256(r1,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m2 = _mm256_slli_si256(r2,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m3 = _mm256_slli_si256(r3,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m4 = _mm256_slli_si256(r4,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning
            m5 = _mm256_slli_si256(r5,
                                   2); // shift 2 elements left - equivalent to
                                       // filling with two zeros inthe beginning

            r0 = _mm256_and_si256(r0, mask_prelude);
            r0 = _mm256_permute2f128_si256(r0, r0, 1);
            r0 = _mm256_srli_si256(r0, 14); // shift 14 elements
            r0 = _mm256_add_epi16(m0, r0);

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 14); // shift 14 elements
            r1 = _mm256_add_epi16(m1, r1);

            r2 = _mm256_and_si256(r2, mask_prelude);
            r2 = _mm256_permute2f128_si256(r2, r2, 1);
            r2 = _mm256_srli_si256(r2, 14); // shift 14 elements
            r2 = _mm256_add_epi16(m2, r2);

            r3 = _mm256_and_si256(r3, mask_prelude);
            r3 = _mm256_permute2f128_si256(r3, r3, 1);
            r3 = _mm256_srli_si256(r3, 14); // shift 14 elements
            r3 = _mm256_add_epi16(m3, r3);

            r4 = _mm256_and_si256(r4, mask_prelude);
            r4 = _mm256_permute2f128_si256(r4, r4, 1);
            r4 = _mm256_srli_si256(r4, 14); // shift 14 elements
            r4 = _mm256_add_epi16(m4, r4);

            r5 = _mm256_and_si256(r5, mask_prelude);
            r5 = _mm256_permute2f128_si256(r5, r5, 1);
            r5 = _mm256_srli_si256(r5, 14); // shift 14 elements
            r5 = _mm256_add_epi16(m5, r5);

            // END - extra code needed for prelude
          } else {
            // load the 5 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 2][col - 2]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col - 2]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[row][col - 2]);
            r3 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col - 2]);
            r4 = _mm256_loadu_si256((__m256i *)&frame1[row + 2][col - 2]);
            r5 = _mm256_loadu_si256((__m256i *)&frame1[row + 3][col - 2]);
          }

          // col iteration computes output pixels of 2,8,14,20,26
          //  col+1 iteration computes output pixels of 3,9,15,21,27
          //  col+2 iteration computes output pixels of 4,10,16,22,28
          //  col+3 iteration computes output pixels of 5,11,17,23,29
          //  col+4 iteration computes output pixels of 6,12,18,24
          //  col+5 iteration computes output pixels of 7,13,19,25
          // afterwards, col becomes 30 and repeat the above process

          // 1st col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0);
          m1 = _mm256_maddubs_epi16(r1, c1);
          m2 = _mm256_maddubs_epi16(r2, c2);
          m3 = _mm256_maddubs_epi16(r3, c1);
          m4 = _mm256_maddubs_epi16(r4, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(0:5)
          // hadd(6:11)
          // hadd(12:17)
          // hadd(18:23)
          // hadd(24:28)

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // after shifts the 15th element is lost as it cannot be propagated to
          // the 16th position (AVX registers are managed as two seperate SSE
          // registers)
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh1);
          m1 = _mm256_maddubs_epi16(r1, c1_sh1);
          m2 = _mm256_maddubs_epi16(r2, c2_sh1);
          m3 = _mm256_maddubs_epi16(r3, c1_sh1);
          m4 = _mm256_maddubs_epi16(r4, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(1:6)
          // hadd(7:12)
          // hadd(13:18)
          // hadd(19:24)
          // hadd(25:29)

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // after shifts the 15th element is lost as it cannot be propagated to
          // the 16th position (AVX registers are managed as two seperate SSE
          // registers)
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh2);
          m1 = _mm256_maddubs_epi16(r1, c1_sh2);
          m2 = _mm256_maddubs_epi16(r2, c2_sh2);
          m3 = _mm256_maddubs_epi16(r3, c1_sh2);
          m4 = _mm256_maddubs_epi16(r4, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(2:7)
          // hadd(8:13)
          // hadd(14:19)
          // hadd(20:25)
          // hadd(26:30)

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m2, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh3);
          m1 = _mm256_maddubs_epi16(r1, c1_sh3);
          m2 = _mm256_maddubs_epi16(r2, c2_sh3);
          m3 = _mm256_maddubs_epi16(r3, c1_sh3);
          m4 = _mm256_maddubs_epi16(r4, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(3:8)
          // hadd(9:14)
          // hadd(15:20)
          // hadd(21:26)
          // hadd(27:31)
          // result after division will be in 2,8,14,20,26

          m1 = _mm256_srli_si256(m0, 2);
          m4 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m4);

          m4 = _mm256_and_si256(m4, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // 5th col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh4);
          m1 = _mm256_maddubs_epi16(r1, c1_sh4);
          m2 = _mm256_maddubs_epi16(r2, c2_sh4);
          m3 = _mm256_maddubs_epi16(r3, c1_sh4);
          m4 = _mm256_maddubs_epi16(r4, c0_sh4);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(4:9)
          // hadd(10:15)
          // hadd(16:21)
          // hadd(22:27)
          // result after division will be in 4,10,16,22

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_even = _mm256_add_epi16(output_even, m2);

          // 6th col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh5);
          m1 = _mm256_maddubs_epi16(r1, c1_sh5);
          m2 = _mm256_maddubs_epi16(r2, c2_sh5);
          m3 = _mm256_maddubs_epi16(r3, c1_sh5);
          m4 = _mm256_maddubs_epi16(r4, c0_sh5);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(5:10)
          // hadd(11:16)
          // hadd(17:22)
          // hadd(23:28)
          // result after division will be in 4,10,16,22

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[row][col], output_even);

          // row+1

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0);
          m1 = _mm256_maddubs_epi16(r2, c1);
          m2 = _mm256_maddubs_epi16(r3, c2);
          m3 = _mm256_maddubs_epi16(r4, c1);
          m4 = _mm256_maddubs_epi16(r5, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(0:5)
          // hadd(6:11)
          // hadd(12:17)
          // hadd(18:23)
          // hadd(24:28)

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // after shifts the 15th element is lost as it cannot be propagated to
          // the 16th position (AVX registers are managed as two seperate SSE
          // registers)
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh1);
          m1 = _mm256_maddubs_epi16(r2, c1_sh1);
          m2 = _mm256_maddubs_epi16(r3, c2_sh1);
          m3 = _mm256_maddubs_epi16(r4, c1_sh1);
          m4 = _mm256_maddubs_epi16(r5, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(1:6)
          // hadd(7:12)
          // hadd(13:18)
          // hadd(19:24)
          // hadd(25:29)

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // after shifts the 15th element is lost as it cannot be propagated to
          // the 16th position (AVX registers are managed as two seperate SSE
          // registers)
          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 12); // shift 6 short int positions or 12 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,3,6,9,12. keep only those, discard others
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh2);
          m1 = _mm256_maddubs_epi16(r2, c1_sh2);
          m2 = _mm256_maddubs_epi16(r3, c2_sh2);
          m3 = _mm256_maddubs_epi16(r4, c1_sh2);
          m4 = _mm256_maddubs_epi16(r5, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(2:7)
          // hadd(8:13)
          // hadd(14:19)
          // hadd(20:25)
          // hadd(26:30)

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m2, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh3);
          m1 = _mm256_maddubs_epi16(r2, c1_sh3);
          m2 = _mm256_maddubs_epi16(r3, c2_sh3);
          m3 = _mm256_maddubs_epi16(r4, c1_sh3);
          m4 = _mm256_maddubs_epi16(r5, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(3:8)
          // hadd(9:14)
          // hadd(15:20)
          // hadd(21:26)
          // hadd(27:31)
          // result after division will be in 2,8,14,20,26

          m1 = _mm256_srli_si256(m0, 2);
          m4 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m4);

          m4 = _mm256_and_si256(m4, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions

          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,4,7,10,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // 5th col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh4);
          m1 = _mm256_maddubs_epi16(r2, c1_sh4);
          m2 = _mm256_maddubs_epi16(r3, c2_sh4);
          m3 = _mm256_maddubs_epi16(r4, c1_sh4);
          m4 = _mm256_maddubs_epi16(r5, c0_sh4);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(4:9)
          // hadd(10:15)
          // hadd(16:21)
          // hadd(22:27)
          // result after division will be in 4,10,16,22

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_even = _mm256_add_epi16(output_even, m2);

          // 6th col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh5);
          m1 = _mm256_maddubs_epi16(r2, c1_sh5);
          m2 = _mm256_maddubs_epi16(r3, c2_sh5);
          m3 = _mm256_maddubs_epi16(r4, c1_sh5);
          m4 = _mm256_maddubs_epi16(r5, c0_sh5);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m2 = _mm256_add_epi16(m2, m3);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m4);

          // hozizontal additions
          // hadd(5:10)
          // hadd(11:16)
          // hadd(17:22)
          // hadd(23:28)
          // result after division will be in 4,10,16,22

          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m1 = _mm256_srli_si256(m0, 4);
          m2 = _mm256_add_epi16(m1, m2);

          // m2 has 16 16bit values now. the results I need are in positions
          // 2,5,8,11. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh2);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[row + 1][col], output_even);
        }

        if (REMINDER_ITERATIONS >= 29) {
          loop_reminder_high_reminder_values_less_div(
              frame1, filt, M, N, row, col, REMINDER_ITERATIONS, division_case,
              c0, c1, c2, c0_sh1, c1_sh1, c2_sh1, c0_sh2, c1_sh2, c2_sh2, f,
              divisor, filter5x5);
          loop_reminder_high_reminder_values_less_div(
              frame1, filt, M, N, row + 1, col, REMINDER_ITERATIONS,
              division_case, c0, c1, c2, c0_sh1, c1_sh1, c2_sh1, c0_sh2, c1_sh2,
              c2_sh2, f, divisor, filter5x5);

        } else {
          loop_reminder_low_reminder_values_less_div(
              frame1, filt, M, N, row, col, REMINDER_ITERATIONS, division_case,
              c0, c1, c2, c0_sh1, c1_sh1, c2_sh1, c0_sh2, c1_sh2, c2_sh2,
              c0_sh3, c1_sh3, c2_sh3, c0_sh4, c1_sh4, c2_sh4, c0_sh5, c1_sh5,
              c2_sh5, f);
          loop_reminder_low_reminder_values_less_div(
              frame1, filt, M, N, row + 1, col, REMINDER_ITERATIONS,
              division_case, c0, c1, c2, c0_sh1, c1_sh1, c2_sh1, c0_sh2, c1_sh2,
              c2_sh2, c0_sh3, c1_sh3, c2_sh3, c0_sh4, c1_sh4, c2_sh4, c0_sh5,
              c1_sh5, c2_sh5, f);
        }
        // padding code. The last three iterations  ABOVE get outside of the
        // array bounds. The last three col iterations read outside but the
        // garbage values are multiplied by zeros (last three mask zeros). From
        // now on, I must include another mask with extra zeros.
        // It is faster to read outside of the array's bounds and then fill the
        // right values. there is an insert command that inserts a value to the
        // vector TOMORROW: USE Gaussian_Blur_AVX_ver4_plus_less_load ROUTINE
        // FOR LOOP REMINDER. LOAD OUTSIDE OF THE ARRAY AND THEN ZERO THE VALUES
        // NEEDED.
      }
    }

  } // end of parallel
}

// for REMINDER_ITERATIONS >=29 only
int loop_reminder_high_reminder_values_less_div(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned int row, const unsigned int col,
    const unsigned int REMINDER_ITERATIONS, const unsigned int division_case,
    const __m256i c0, const __m256i c1, const __m256i c2, const __m256i c0_sh1,
    const __m256i c1_sh1, const __m256i c2_sh1, const __m256i c0_sh2,
    const __m256i c1_sh2, const __m256i c2_sh2, const __m256i f,
    const unsigned short int divisor, signed char **filter) {

  __m256i r0, r1, r2, r3, r4, m0, m1, m2, m3, m4;
  __m256i output_even, output_odd;
  int newPixel = 0;

  // const __m256i f  =
  // _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
  // const __m256i mask  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
  // const __m256i mask2  =
  // _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  // const __m256i mask4  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
  // const __m256i mask5  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

  const __m256i output_mask = _mm256_set_epi16(
      0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535);
  const __m256i output_mask_sh1 = _mm256_set_epi16(
      0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0);
  // const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,
  // 0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
  const __m256i mask_new =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0);
  const __m256i mask_new2 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0);

  __m256i reminder_mask1, reminder_mask2;

  reminder_mask1 =
      _mm256_load_si256((__m256i *)&reminder_msk1[REMINDER_ITERATIONS - 1][0]);
  reminder_mask2 =
      _mm256_load_si256((__m256i *)&reminder_msk2[REMINDER_ITERATIONS - 1][0]);

  // 1st col iteration
  // load the 5 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 2][col - 2]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col - 2]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[row][col - 2]);
  r3 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col - 2]);
  r4 = _mm256_loadu_si256((__m256i *)&frame1[row + 2][col - 2]);

  /*
   * The above load operations load outside of the array bounds; these elements
   * are filled with zeros just after Furthermore, I add two extra zeros in the
   * end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
   */

  // AND r0-r4 with reminder_mask
  r0 = _mm256_and_si256(r0, reminder_mask1);
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);
  r3 = _mm256_and_si256(r3, reminder_mask1);
  r4 = _mm256_and_si256(r4, reminder_mask1);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c2);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c2_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  // hozizontal additions
  // hadd(0:5)   and store filt[row[col]
  // hadd(6:11)   and store filt[row[col+8]
  // hadd(12:17) and store filt[row[col+14]
  // hadd(18:23) and store filt[row[col+20]
  // hadd(24:30) and store filt[row[col+26]

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c2_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  // hozizontal additions
  // hadd(0:5)   and store filt[row[col]
  // hadd(6:11)   and store filt[row[col+8]
  // hadd(12:17) and store filt[row[col+14]
  // hadd(18:23) and store filt[row[col+20]
  // hadd(24:30) and store filt[row[col+26]

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me
  // ola mazi

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_even = _mm256_add_epi16(output_even, m2);

  // 4th col iteration
  // load the 5 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 2][col + 3 - 2]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col + 3 - 2]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[row][col + 3 - 2]);
  r3 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col + 3 - 2]);
  r4 = _mm256_loadu_si256((__m256i *)&frame1[row + 2][col + 3 - 2]);

  // AND r0-r4 with reminder_mask
  r0 = _mm256_and_si256(r0, reminder_mask2);
  r1 = _mm256_and_si256(r1, reminder_mask2);
  r2 = _mm256_and_si256(r2, reminder_mask2);
  r3 = _mm256_and_si256(r3, reminder_mask2);
  r4 = _mm256_and_si256(r4, reminder_mask2);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c2);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);
  m2 = _mm256_slli_si256(m2, 2);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // 5th col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c2_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(m2,
                        output_mask); // holds results in positions 0,3,6,9,12
  m1 = _mm256_slli_si256(
      m2, 4); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new); // extract only the 6th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 12);
  m2 = _mm256_add_epi16(m1, m2);
  output_even = _mm256_add_epi16(output_even, m2);

  // 6th col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c2_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(
      m2, output_mask_sh1); // holds results in positions 1,4,7,10,13
  m1 = _mm256_slli_si256(
      m2, 2); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new2); // extract only the 7th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 14);
  m2 = _mm256_add_epi16(m1, m2);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  _mm_storeu_si128(
      (__m128i *)&filt[row][col],
      _mm256_extractf128_si256(output_even, 0)); // store low 128bit - 16pixels

  switch (REMINDER_ITERATIONS) {
  case 29:
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    break;
  case 30:
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output_even, 29);
    break;
  case 31:
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output_even, 29);

    // the filt[row][M-1] pixel will be computed without vectorization
    newPixel = 0;
    newPixel += frame1[row - 2][M - 1 - 2] * filter[0][0];
    newPixel += frame1[row - 2][M - 1 - 1] * filter[0][1];
    newPixel += frame1[row - 2][M - 1 - 0] * filter[0][2];

    newPixel += frame1[row - 1][M - 1 - 2] * filter[1][0];
    newPixel += frame1[row - 1][M - 1 - 1] * filter[1][1];
    newPixel += frame1[row - 1][M - 1 - 0] * filter[1][2];

    newPixel += frame1[row - 0][M - 1 - 2] * filter[2][0];
    newPixel += frame1[row - 0][M - 1 - 1] * filter[2][1];
    newPixel += frame1[row - 0][M - 1 - 0] * filter[2][2];

    newPixel += frame1[row + 1][M - 1 - 2] * filter[3][0];
    newPixel += frame1[row + 1][M - 1 - 1] * filter[3][1];
    newPixel += frame1[row + 1][M - 1 - 0] * filter[3][2];

    newPixel += frame1[row + 2][M - 1 - 2] * filter[4][0];
    newPixel += frame1[row + 2][M - 1 - 1] * filter[4][1];
    newPixel += frame1[row + 2][M - 1 - 0] * filter[4][2];

    filt[row][M - 1] = (unsigned char)(newPixel / divisor);

    break;
  default:
    printf("\n something went wrong");
  }

  return 0;
}

// for loop reminder<29 only
int loop_reminder_low_reminder_values_less_div(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned int row, const unsigned int col,
    const unsigned int REMINDER_ITERATIONS, const unsigned int division_case,
    const __m256i c0, const __m256i c1, const __m256i c2, const __m256i c0_sh1,
    const __m256i c1_sh1, const __m256i c2_sh1, const __m256i c0_sh2,
    const __m256i c1_sh2, const __m256i c2_sh2, const __m256i c0_sh3,
    const __m256i c1_sh3, const __m256i c2_sh3, const __m256i c0_sh4,
    const __m256i c1_sh4, const __m256i c2_sh4, const __m256i c0_sh5,
    const __m256i c1_sh5, const __m256i c2_sh5, const __m256i f) {

  register __m256i r0, r1, r2, r3, r4, m0, m1, m2, m3, m4, output_even,
      output_odd;

  // const __m256i f  =
  // _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
  const __m256i output_mask = _mm256_set_epi16(
      0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535);
  const __m256i output_mask_sh1 = _mm256_set_epi16(
      0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0);
  const __m256i output_mask_sh2 = _mm256_set_epi16(
      0, 0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0);
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);

  __m256i reminder_mask1;

  if (REMINDER_ITERATIONS == 0) {
    return 0; // no loop reminder is needed
  }

  reminder_mask1 =
      _mm256_load_si256((__m256i *)&reminder_msk1[REMINDER_ITERATIONS - 1][0]);
  // reminder_mask2=_mm256_load_si256( (__m256i *)
  // &reminder_msk2[REMINDER_ITERATIONS-1][0]);

  // 1st col iteration
  // load the 5 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 2][col - 2]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col - 2]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[row][col - 2]);
  r3 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col - 2]);
  r4 = _mm256_loadu_si256((__m256i *)&frame1[row + 2][col - 2]);

  /*
   * The above load operations load outside of the array bounds; these elements
   * are filled with zeros just after Furthermore, I add two extra zeros in the
   * end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
   */

  // AND r0-r4 with reminder_mask
  r0 = _mm256_and_si256(r0, reminder_mask1);
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);
  r3 = _mm256_and_si256(r3, reminder_mask1);
  r4 = _mm256_and_si256(r4, reminder_mask1);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c2);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // after shifts the 15th element is lost as it cannot be propagated to the
  // 16th position (AVX registers are managed as two seperate SSE registers)
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c2_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // after shifts the 15th element is lost as it cannot be propagated to the
  // 16th position (AVX registers are managed as two seperate SSE registers)
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c2_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_even = _mm256_add_epi16(output_even, m2);

  // 4th col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh3);
  m1 = _mm256_maddubs_epi16(r1, c1_sh3);
  m2 = _mm256_maddubs_epi16(r2, c2_sh3);
  m3 = _mm256_maddubs_epi16(r3, c1_sh3);
  m4 = _mm256_maddubs_epi16(r4, c0_sh3);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m4);

  m4 = _mm256_and_si256(m4, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // 5th col iteration

  m0 = _mm256_maddubs_epi16(r0, c0_sh4);
  m1 = _mm256_maddubs_epi16(r1, c1_sh4);
  m2 = _mm256_maddubs_epi16(r2, c2_sh4);
  m3 = _mm256_maddubs_epi16(r3, c1_sh4);
  m4 = _mm256_maddubs_epi16(r4, c0_sh4);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  // hozizontal additions
  // hadd(4:9)
  // hadd(10:15)
  // hadd(16:21)
  // hadd(22:27)
  // result after division will be in 4,10,16,22

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // m2 has 16 16bit values now. the results I need are in positions 2,5,8,11.
  // keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh2);
  output_even = _mm256_add_epi16(output_even, m2);

  // 6th col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh5);
  m1 = _mm256_maddubs_epi16(r1, c1_sh5);
  m2 = _mm256_maddubs_epi16(r2, c2_sh5);
  m3 = _mm256_maddubs_epi16(r3, c1_sh5);
  m4 = _mm256_maddubs_epi16(r4, c0_sh5);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // m2 has 16 16bit values now. the results I need are in positions 2,5,8,11.
  // keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh2);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  switch (REMINDER_ITERATIONS) {
  case 1:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    break;
  case 2:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    break;
  case 3:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    break;
  case 4:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    break;
  case 5:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    break;
  case 6:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    break;
  case 7:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    break;
  case 8:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    break;
  case 9:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    break;
  case 10:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    break;
  case 11:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    break;
  case 12:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    break;
  case 13:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    break;
  case 14:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);
    break;
  case 15:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);
    filt[row][col + 14] = (unsigned char)_mm256_extract_epi8(output_even, 14);
    break;
  case 16:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    break;
  case 17:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    break;
  case 18:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    break;
  case 19:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    break;
  case 20:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    break;
  case 21:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    break;
  case 22:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    break;
  case 23:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    break;
  case 24:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    break;
  case 25:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    break;
  case 26:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    break;
  case 27:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    break;
  case 28:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    break;
  default:
    printf("\nsomething went wrong");
    return -1;
  }

  return 0;
}
// for row=0,1,2 only
int loop_reminder_first_less_div(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned int col,
    const unsigned int REMINDER_ITERATIONS, const unsigned int division_case,
    const __m256i c0, const __m256i c1, const __m256i c2, const __m256i c0_sh1,
    const __m256i c1_sh1, const __m256i c2_sh1, const __m256i c0_sh2,
    const __m256i c1_sh2, const __m256i c2_sh2, const __m256i f,
    const unsigned short int divisor, signed char **filter) {

  __m256i r0, r1, r2, r3, r4, m0, m1, m2, m3, m4;
  __m256i output_even, output_odd;
  __m256i output_row0_even, output_row1_even, output_row0_odd, output_row1_odd;

  // const __m256i f  =
  // _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
  // const __m256i mask  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
  // const __m256i mask2  =
  // _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  // const __m256i mask4  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
  // const __m256i mask5  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

  const __m256i output_mask = _mm256_set_epi16(
      0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535);
  const __m256i output_mask_sh1 = _mm256_set_epi16(
      0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0);
  // const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,
  // 0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
  const __m256i mask_new =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0);
  const __m256i mask_new2 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0);

  __m256i reminder_mask1, reminder_mask2;

  reminder_mask1 =
      _mm256_load_si256((__m256i *)&reminder_msk1[REMINDER_ITERATIONS - 1][0]);
  reminder_mask2 =
      _mm256_load_si256((__m256i *)&reminder_msk2[REMINDER_ITERATIONS - 1][0]);

  // 1st col iteration
  // load the 5 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col - 2]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col - 2]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col - 2]);
  r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col - 2]);
  r4 = _mm256_loadu_si256((__m256i *)&frame1[4][col - 2]);

  /*
   * The above load operations load outside of the array bounds; these elements
   * are filled with zeros just after Furthermore, I add two extra zeros in the
   * end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
   */

  // AND r0-r4 with reminder_mask
  r0 = _mm256_and_si256(r0, reminder_mask1);
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);
  r3 = _mm256_and_si256(r3, reminder_mask1);
  r4 = _mm256_and_si256(r4, reminder_mask1);

  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c2);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_even = _mm256_and_si256(m2, output_mask);

  // row=0
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c2);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row0_even = _mm256_and_si256(m2, output_mask);

  // row=1
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c1);
  m1 = _mm256_maddubs_epi16(r1, c2);
  m2 = _mm256_maddubs_epi16(r2, c1);
  m3 = _mm256_maddubs_epi16(r3, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m3);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row1_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration

  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c2_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_odd = _mm256_and_si256(m2, output_mask);

  // row=0
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c2_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row0_odd = _mm256_and_si256(m2, output_mask);

  // row=1
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c1_sh1);
  m1 = _mm256_maddubs_epi16(r1, c2_sh1);
  m2 = _mm256_maddubs_epi16(r2, c1_sh1);
  m3 = _mm256_maddubs_epi16(r3, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m3);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row1_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration

  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c2_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me
  // ola mazi

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_even = _mm256_add_epi16(output_even, m2);

  // row=0
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c2_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me
  // ola mazi

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_row0_even = _mm256_add_epi16(output_row0_even, m2);

  // row=1
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c1_sh2);
  m1 = _mm256_maddubs_epi16(r1, c2_sh2);
  m2 = _mm256_maddubs_epi16(r2, c1_sh2);
  m3 = _mm256_maddubs_epi16(r3, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m3);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me
  // ola mazi

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_row1_even = _mm256_add_epi16(output_row1_even, m2);

  // 4th col iteration
  // load the 5 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col + 3 - 2]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col + 3 - 2]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col + 3 - 2]);
  r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col + 3 - 2]);
  r4 = _mm256_loadu_si256((__m256i *)&frame1[4][col + 3 - 2]);

  // AND r0-r4 with reminder_mask
  r0 = _mm256_and_si256(r0, reminder_mask2);
  r1 = _mm256_and_si256(r1, reminder_mask2);
  r2 = _mm256_and_si256(r2, reminder_mask2);
  r3 = _mm256_and_si256(r3, reminder_mask2);
  r4 = _mm256_and_si256(r4, reminder_mask2);

  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c2);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);
  m2 = _mm256_slli_si256(m2, 2);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // row=0
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r2, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r0, c2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m2, m0);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);
  m2 = _mm256_slli_si256(m2, 2);
  output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

  // row=1
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c1);
  m1 = _mm256_maddubs_epi16(r1, c2);
  m2 = _mm256_maddubs_epi16(r2, c1);
  m3 = _mm256_maddubs_epi16(r3, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m2, m0);
  m0 = _mm256_add_epi16(m0, m3);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);
  m2 = _mm256_slli_si256(m2, 2);
  output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

  // 5th col iteration

  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c2_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(m2,
                        output_mask); // holds results in positions 0,3,6,9,12
  m1 = _mm256_slli_si256(
      m2, 4); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new); // extract only the 6th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 12);
  m2 = _mm256_add_epi16(m1, m2);
  output_even = _mm256_add_epi16(output_even, m2);

  // row=0
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c2_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m2, m0);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(m2,
                        output_mask); // holds results in positions 0,3,6,9,12
  m1 = _mm256_slli_si256(
      m2, 4); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new); // extract only the 6th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 12);
  m2 = _mm256_add_epi16(m1, m2);
  output_row0_even = _mm256_add_epi16(output_row0_even, m2);

  // row=1
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c1_sh1);
  m1 = _mm256_maddubs_epi16(r1, c2_sh1);
  m2 = _mm256_maddubs_epi16(r2, c1_sh1);
  m3 = _mm256_maddubs_epi16(r3, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m2, m0);
  m0 = _mm256_add_epi16(m0, m3);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(m2,
                        output_mask); // holds results in positions 0,3,6,9,12
  m1 = _mm256_slli_si256(
      m2, 4); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new); // extract only the 6th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 12);
  m2 = _mm256_add_epi16(m1, m2);
  output_row1_even = _mm256_add_epi16(output_row1_even, m2);

  // 6th col iteration
  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c2_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(
      m2, output_mask_sh1); // holds results in positions 1,4,7,10,13
  m1 = _mm256_slli_si256(
      m2, 2); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new2); // extract only the 7th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 14);
  m2 = _mm256_add_epi16(m1, m2);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  // row=0
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c2_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m2, m0);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(
      m2, output_mask_sh1); // holds results in positions 1,4,7,10,13
  m1 = _mm256_slli_si256(
      m2, 2); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new2); // extract only the 7th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 14);
  m2 = _mm256_add_epi16(m1, m2);
  output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

  // now division follows
  output_row0_even = division(division_case, output_row0_even, f);
  output_row0_odd = division(division_case, output_row0_odd, f);

  // shift odd 1 position and add to even
  output_row0_odd = _mm256_slli_si256(output_row0_odd, 1);
  output_row0_even = _mm256_add_epi8(output_row0_even, output_row0_odd);

  // row=1
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c1_sh2);
  m1 = _mm256_maddubs_epi16(r1, c2_sh2);
  m2 = _mm256_maddubs_epi16(r2, c1_sh2);
  m3 = _mm256_maddubs_epi16(r3, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m2, m0);
  m0 = _mm256_add_epi16(m0, m3);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(
      m2, output_mask_sh1); // holds results in positions 1,4,7,10,13
  m1 = _mm256_slli_si256(
      m2, 2); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new2); // extract only the 7th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 14);
  m2 = _mm256_add_epi16(m1, m2);
  output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

  // now division follows
  output_row1_even = division(division_case, output_row1_even, f);
  output_row1_odd = division(division_case, output_row1_odd, f);

  // shift odd 1 position and add to even
  output_row1_odd = _mm256_slli_si256(output_row1_odd, 1);
  output_row1_even = _mm256_add_epi8(output_row1_even, output_row1_odd);

  switch (REMINDER_ITERATIONS) {
  case 1:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    break;
  case 2:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    break;
  case 3:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    break;
  case 4:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    break;
  case 5:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    break;
  case 6:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[2][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[0][col + 5] = (unsigned char)_mm256_extract_epi8(output_row0_even, 5);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[1][col + 5] = (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    break;
  case 7:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[2][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[2][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[0][col + 5] = (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[0][col + 6] = (unsigned char)_mm256_extract_epi8(output_row0_even, 6);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[1][col + 5] = (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[1][col + 6] = (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    break;
  case 8:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[2][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[2][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[2][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[0][col + 5] = (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[0][col + 6] = (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[0][col + 7] = (unsigned char)_mm256_extract_epi8(output_row0_even, 7);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[1][col + 5] = (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[1][col + 6] = (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[1][col + 7] = (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    break;
  case 9:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[2][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[2][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[2][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[2][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[0][col + 5] = (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[0][col + 6] = (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[0][col + 7] = (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[0][col + 8] = (unsigned char)_mm256_extract_epi8(output_row0_even, 8);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[1][col + 5] = (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[1][col + 6] = (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[1][col + 7] = (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[1][col + 8] = (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    break;
  case 10:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[2][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[2][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[2][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[2][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[2][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[0][col + 5] = (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[0][col + 6] = (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[0][col + 7] = (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[0][col + 8] = (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[0][col + 9] = (unsigned char)_mm256_extract_epi8(output_row0_even, 9);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[1][col + 5] = (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[1][col + 6] = (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[1][col + 7] = (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[1][col + 8] = (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[1][col + 9] = (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    break;
  case 11:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[2][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[2][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[2][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[2][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[2][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[2][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[0][col + 5] = (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[0][col + 6] = (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[0][col + 7] = (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[0][col + 8] = (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[0][col + 9] = (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[0][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[1][col + 5] = (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[1][col + 6] = (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[1][col + 7] = (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[1][col + 8] = (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[1][col + 9] = (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    break;
  case 12:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[2][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[2][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[2][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[2][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[2][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[2][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[2][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[0][col + 5] = (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[0][col + 6] = (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[0][col + 7] = (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[0][col + 8] = (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[0][col + 9] = (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[0][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[0][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[1][col + 5] = (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[1][col + 6] = (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[1][col + 7] = (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[1][col + 8] = (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[1][col + 9] = (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    break;
  case 13:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[2][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[2][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[2][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[2][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[2][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[2][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[2][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[2][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[0][col + 5] = (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[0][col + 6] = (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[0][col + 7] = (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[0][col + 8] = (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[0][col + 9] = (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[0][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[0][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);
    filt[0][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 12);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[1][col + 5] = (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[1][col + 6] = (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[1][col + 7] = (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[1][col + 8] = (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[1][col + 9] = (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    filt[1][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 12);
    break;
  case 14:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[2][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[2][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[2][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[2][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[2][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[2][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[2][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[2][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[2][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[0][col + 5] = (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[0][col + 6] = (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[0][col + 7] = (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[0][col + 8] = (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[0][col + 9] = (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[0][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[0][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);
    filt[0][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 12);
    filt[0][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 13);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[1][col + 5] = (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[1][col + 6] = (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[1][col + 7] = (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[1][col + 8] = (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[1][col + 9] = (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    filt[1][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 12);
    filt[1][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 13);
    break;
  case 15:
    filt[2][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[2][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[2][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[2][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[2][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[2][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[2][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[2][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[2][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[2][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[2][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[2][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[2][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[2][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);
    filt[2][col + 14] = (unsigned char)_mm256_extract_epi8(output_even, 14);

    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[0][col + 1] = (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[0][col + 2] = (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[0][col + 3] = (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[0][col + 4] = (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[0][col + 5] = (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[0][col + 6] = (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[0][col + 7] = (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[0][col + 8] = (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[0][col + 9] = (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[0][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[0][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);
    filt[0][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 12);
    filt[0][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 13);
    filt[0][col + 14] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 14);

    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[1][col + 1] = (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[1][col + 2] = (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[1][col + 3] = (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[1][col + 4] = (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[1][col + 5] = (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[1][col + 6] = (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[1][col + 7] = (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[1][col + 8] = (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[1][col + 9] = (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    filt[1][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 12);
    filt[1][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 13);
    filt[1][col + 14] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 14);
    break;
  case 16:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    break;
  case 17:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    break;
  case 18:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    break;
  case 19:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    break;
  case 20:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    break;
  case 21:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    break;
  case 22:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[2][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[0][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    break;
  case 23:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[2][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[2][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[0][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[0][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    break;
  case 24:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[2][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[2][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[2][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[0][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[0][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[0][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    break;
  case 25:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[2][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[2][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[2][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[2][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[0][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[0][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[0][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[0][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    break;
  case 26:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[2][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[2][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[2][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[2][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[2][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[0][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[0][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[0][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[0][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[0][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    break;
  case 27:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[2][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[2][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[2][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[2][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[2][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[2][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[0][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[0][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[0][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[0][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[0][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[0][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    break;
  case 28:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[2][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[2][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[2][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[2][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[2][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[2][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[2][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[0][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[0][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[0][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[0][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[0][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[0][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[0][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    break;
  case 29:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[2][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[2][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[2][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[2][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[2][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[2][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[2][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[2][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[0][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[0][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[0][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[0][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[0][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[0][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[0][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);
    filt[0][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 28);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    filt[1][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 28);
    break;
  case 30:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[2][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[2][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[2][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[2][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[2][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[2][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[2][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[2][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    filt[2][col + 29] = (unsigned char)_mm256_extract_epi8(output_even, 29);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[0][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[0][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[0][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[0][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[0][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[0][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[0][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);
    filt[0][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 28);
    filt[0][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 29);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    filt[1][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 28);
    filt[1][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 29);
    break;
  case 31:
    _mm_storeu_si128((__m128i *)&filt[2][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[2][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[2][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[2][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[2][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[2][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[2][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[2][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[2][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[2][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[2][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[2][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[2][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[2][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    filt[2][col + 29] = (unsigned char)_mm256_extract_epi8(output_even, 29);

    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[0][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[0][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[0][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[0][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[0][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[0][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[0][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[0][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[0][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[0][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[0][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[0][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);
    filt[0][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 28);
    filt[0][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 29);

    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    filt[1][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 28);
    filt[1][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 29);

    // the filt[0:2][M-1] pixel will be computed without vectorization
    int newPixel;
    for (int row = 0; row <= 2; row++) {
      newPixel = 0;
      for (int rowOffset = -2; rowOffset <= 2; rowOffset++) {
        for (int colOffset = -2; colOffset <= 2; colOffset++) {

          if (((row + rowOffset) < N) && ((M - 1 + colOffset) < M) &&
              ((row + rowOffset) >= 0) && ((M - 1 + colOffset) >= 0))
            newPixel += frame1[row + rowOffset][M - 1 + colOffset] *
                        filter[2 + rowOffset][2 + colOffset];
        }
      }

      filt[row][M - 1] = (unsigned char)(newPixel / divisor);
    }

    break;
  default:
    printf("\n something went wrong");
  }

  return 0;
}

// for row=N-3,N-2,N-1 only
// row0 refers to N-2, while row1 refers to N-1
int loop_reminder_last_less_div(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned int col,
    const unsigned int REMINDER_ITERATIONS, const unsigned int division_case,
    const __m256i c0, const __m256i c1, const __m256i c2, const __m256i c0_sh1,
    const __m256i c1_sh1, const __m256i c2_sh1, const __m256i c0_sh2,
    const __m256i c1_sh2, const __m256i c2_sh2, const __m256i f,
    const unsigned short int divisor, signed char **filter) {

  __m256i r0, r1, r2, r3, r4, m0, m1, m2, m3, m4;
  __m256i output_even, output_odd;
  __m256i output_row0_even, output_row1_even, output_row0_odd, output_row1_odd;

  // const __m256i f  =
  // _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
  // const __m256i mask  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
  // const __m256i mask2  =
  // _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  // const __m256i mask4  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
  // const __m256i mask5  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

  const __m256i output_mask = _mm256_set_epi16(
      0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535);
  const __m256i output_mask_sh1 = _mm256_set_epi16(
      0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0);
  // const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,
  // 0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
  const __m256i mask_new =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0);
  const __m256i mask_new2 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0);

  __m256i reminder_mask1, reminder_mask2;

  reminder_mask1 =
      _mm256_load_si256((__m256i *)&reminder_msk1[REMINDER_ITERATIONS - 1][0]);
  reminder_mask2 =
      _mm256_load_si256((__m256i *)&reminder_msk2[REMINDER_ITERATIONS - 1][0]);

  // 1st col iteration
  // load the 5 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 5][col - 2]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col - 2]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col - 2]);
  r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 2]);
  r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 2]);

  /*
   * The above load operations load outside of the array bounds; these elements
   * are filled with zeros just after Furthermore, I add two extra zeros in the
   * end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
   */

  // AND r0-r4 with reminder_mask
  r0 = _mm256_and_si256(r0, reminder_mask1);
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);
  r3 = _mm256_and_si256(r3, reminder_mask1);
  r4 = _mm256_and_si256(r4, reminder_mask1);

  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c2);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_even = _mm256_and_si256(m2, output_mask);

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0);
  m2 = _mm256_maddubs_epi16(r2, c1);
  m3 = _mm256_maddubs_epi16(r3, c2);
  m4 = _mm256_maddubs_epi16(r4, c1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row0_even = _mm256_and_si256(m2, output_mask);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c2);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row1_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration

  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c2_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_odd = _mm256_and_si256(m2, output_mask);

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh1);
  m2 = _mm256_maddubs_epi16(r2, c1_sh1);
  m3 = _mm256_maddubs_epi16(r3, c2_sh1);
  m4 = _mm256_maddubs_epi16(r4, c1_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row0_odd = _mm256_and_si256(m2, output_mask);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c2_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row1_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration

  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c2_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me
  // ola mazi

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_even = _mm256_add_epi16(output_even, m2);

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh2);
  m2 = _mm256_maddubs_epi16(r2, c1_sh2);
  m3 = _mm256_maddubs_epi16(r3, c2_sh2);
  m4 = _mm256_maddubs_epi16(r4, c1_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me
  // ola mazi

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_row0_even = _mm256_add_epi16(output_row0_even, m2);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c2_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me
  // ola mazi

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_row1_even = _mm256_add_epi16(output_row1_even, m2);

  // 4th col iteration
  // load the 5 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 5][col + 3 - 2]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col + 3 - 2]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col + 3 - 2]);
  r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col + 3 - 2]);
  r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col + 3 - 2]);

  // AND r0-r4 with reminder_mask
  r0 = _mm256_and_si256(r0, reminder_mask2);
  r1 = _mm256_and_si256(r1, reminder_mask2);
  r2 = _mm256_and_si256(r2, reminder_mask2);
  r3 = _mm256_and_si256(r3, reminder_mask2);
  r4 = _mm256_and_si256(r4, reminder_mask2);

  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c2);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);
  m2 = _mm256_slli_si256(m2, 2);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0);
  m2 = _mm256_maddubs_epi16(r2, c1);
  m3 = _mm256_maddubs_epi16(r3, c2);
  m4 = _mm256_maddubs_epi16(r4, c1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);
  m2 = _mm256_slli_si256(m2, 2);
  output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c2);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);
  m2 = _mm256_slli_si256(m2, 2);
  output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

  // 5th col iteration

  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c2_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(m2,
                        output_mask); // holds results in positions 0,3,6,9,12
  m1 = _mm256_slli_si256(
      m2, 4); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new); // extract only the 6th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 12);
  m2 = _mm256_add_epi16(m1, m2);
  output_even = _mm256_add_epi16(output_even, m2);

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh1);
  m2 = _mm256_maddubs_epi16(r2, c1_sh1);
  m3 = _mm256_maddubs_epi16(r3, c2_sh1);
  m4 = _mm256_maddubs_epi16(r4, c1_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(m2,
                        output_mask); // holds results in positions 0,3,6,9,12
  m1 = _mm256_slli_si256(
      m2, 4); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new); // extract only the 6th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 12);
  m2 = _mm256_add_epi16(m1, m2);
  output_row0_even = _mm256_add_epi16(output_row0_even, m2);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c2_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(m2,
                        output_mask); // holds results in positions 0,3,6,9,12
  m1 = _mm256_slli_si256(
      m2, 4); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new); // extract only the 6th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 12);
  m2 = _mm256_add_epi16(m1, m2);
  output_row1_even = _mm256_add_epi16(output_row1_even, m2);

  // 6th col iteration
  // row=2
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c2_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m2 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m2);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(
      m2, output_mask_sh1); // holds results in positions 1,4,7,10,13
  m1 = _mm256_slli_si256(
      m2, 2); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new2); // extract only the 7th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 14);
  m2 = _mm256_add_epi16(m1, m2);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh2);
  m2 = _mm256_maddubs_epi16(r2, c1_sh2);
  m3 = _mm256_maddubs_epi16(r3, c2_sh2);
  m4 = _mm256_maddubs_epi16(r4, c1_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(
      m2, output_mask_sh1); // holds results in positions 1,4,7,10,13
  m1 = _mm256_slli_si256(
      m2, 2); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new2); // extract only the 7th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 14);
  m2 = _mm256_add_epi16(m1, m2);
  output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

  // now division follows
  output_row0_even = division(division_case, output_row0_even, f);
  output_row0_odd = division(division_case, output_row0_odd, f);

  // shift odd 1 position and add to even
  output_row0_odd = _mm256_slli_si256(output_row0_odd, 1);
  output_row0_even = _mm256_add_epi8(output_row0_even, output_row0_odd);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c2_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(
      m2, output_mask_sh1); // holds results in positions 1,4,7,10,13
  m1 = _mm256_slli_si256(
      m2, 2); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new2); // extract only the 7th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 14);
  m2 = _mm256_add_epi16(m1, m2);
  output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

  // now division follows
  output_row1_even = division(division_case, output_row1_even, f);
  output_row1_odd = division(division_case, output_row1_odd, f);

  // shift odd 1 position and add to even
  output_row1_odd = _mm256_slli_si256(output_row1_odd, 1);
  output_row1_even = _mm256_add_epi8(output_row1_even, output_row1_odd);

  switch (REMINDER_ITERATIONS) {
  case 1:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    break;
  case 2:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    break;
  case 3:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    break;
  case 4:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    break;
  case 5:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    break;
  case 6:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[N - 3][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    break;
  case 7:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[N - 3][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[N - 3][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    break;
  case 8:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[N - 3][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[N - 3][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[N - 3][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    break;
  case 9:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[N - 3][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[N - 3][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[N - 3][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[N - 3][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    break;
  case 10:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[N - 3][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[N - 3][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[N - 3][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[N - 3][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[N - 3][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    break;
  case 11:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[N - 3][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[N - 3][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[N - 3][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[N - 3][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[N - 3][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[N - 3][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[N - 2][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[N - 1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    break;
  case 12:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[N - 3][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[N - 3][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[N - 3][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[N - 3][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[N - 3][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[N - 3][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[N - 3][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[N - 2][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[N - 2][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[N - 1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[N - 1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    break;
  case 13:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[N - 3][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[N - 3][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[N - 3][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[N - 3][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[N - 3][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[N - 3][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[N - 3][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[N - 3][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[N - 2][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[N - 2][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);
    filt[N - 2][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 12);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[N - 1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[N - 1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    filt[N - 1][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 12);
    break;
  case 14:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[N - 3][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[N - 3][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[N - 3][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[N - 3][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[N - 3][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[N - 3][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[N - 3][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[N - 3][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[N - 3][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[N - 2][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[N - 2][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);
    filt[N - 2][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 12);
    filt[N - 2][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 13);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[N - 1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[N - 1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    filt[N - 1][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 12);
    filt[N - 1][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 13);
    break;
  case 15:
    filt[N - 3][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[N - 3][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[N - 3][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[N - 3][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[N - 3][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[N - 3][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[N - 3][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[N - 3][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[N - 3][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[N - 3][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[N - 3][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[N - 3][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[N - 3][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[N - 3][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);
    filt[N - 3][col + 14] = (unsigned char)_mm256_extract_epi8(output_even, 14);

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[N - 2][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[N - 2][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);
    filt[N - 2][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 12);
    filt[N - 2][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 13);
    filt[N - 2][col + 14] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 14);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[N - 1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[N - 1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    filt[N - 1][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 12);
    filt[N - 1][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 13);
    filt[N - 1][col + 14] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 14);
    break;
  case 16:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    break;
  case 17:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    break;
  case 18:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    break;
  case 19:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    break;
  case 20:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    break;
  case 21:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    break;
  case 22:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[N - 3][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    break;
  case 23:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[N - 3][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[N - 3][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    break;
  case 24:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[N - 3][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[N - 3][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[N - 3][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    break;
  case 25:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[N - 3][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[N - 3][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[N - 3][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[N - 3][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    break;
  case 26:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[N - 3][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[N - 3][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[N - 3][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[N - 3][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[N - 3][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    break;
  case 27:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[N - 3][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[N - 3][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[N - 3][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[N - 3][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[N - 3][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[N - 3][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[N - 2][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[N - 1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    break;
  case 28:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[N - 3][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[N - 3][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[N - 3][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[N - 3][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[N - 3][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[N - 3][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[N - 3][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[N - 2][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[N - 2][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[N - 1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[N - 1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    break;
  case 29:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[N - 3][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[N - 3][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[N - 3][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[N - 3][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[N - 3][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[N - 3][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[N - 3][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[N - 3][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[N - 2][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[N - 2][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);
    filt[N - 2][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 28);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[N - 1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[N - 1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    filt[N - 1][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 28);
    break;
  case 30:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[N - 3][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[N - 3][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[N - 3][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[N - 3][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[N - 3][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[N - 3][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[N - 3][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[N - 3][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    filt[N - 3][col + 29] = (unsigned char)_mm256_extract_epi8(output_even, 29);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[N - 2][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[N - 2][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);
    filt[N - 2][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 28);
    filt[N - 2][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 29);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[N - 1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[N - 1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    filt[N - 1][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 28);
    filt[N - 1][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 29);
    break;
  case 31:
    _mm_storeu_si128((__m128i *)&filt[N - 3][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[N - 3][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[N - 3][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[N - 3][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[N - 3][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[N - 3][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[N - 3][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[N - 3][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[N - 3][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[N - 3][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[N - 3][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[N - 3][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[N - 3][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[N - 3][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    filt[N - 3][col + 29] = (unsigned char)_mm256_extract_epi8(output_even, 29);

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[N - 2][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[N - 2][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);
    filt[N - 2][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 28);
    filt[N - 2][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 29);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[N - 1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[N - 1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    filt[N - 1][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 28);
    filt[N - 1][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 29);

    // the filt[0:2][M-1] pixel will be computed without vectorization
    int newPixel;
    for (int row = N - 3; row <= N - 1; row++) {
      newPixel = 0;
      for (int rowOffset = -2; rowOffset <= 2; rowOffset++) {
        for (int colOffset = -2; colOffset <= 2; colOffset++) {

          if (((row + rowOffset) < N) && ((M - 1 + colOffset) < M) &&
              ((row + rowOffset) >= 0) && ((M - 1 + colOffset) >= 0))
            newPixel += frame1[row + rowOffset][M - 1 + colOffset] *
                        filter[2 + rowOffset][2 + colOffset];
        }
      }
      filt[row][M - 1] = (unsigned char)(newPixel / divisor);
    }

    break;
  default:
    printf("\n something went wrong");
  }

  return 0;
}

// for row=N-2,N-1 only
// row0 refers to N-2, while row1 refers to N-1
int loop_reminder_last_less_div_special_case(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned int col,
    const unsigned int REMINDER_ITERATIONS, const unsigned int division_case,
    const __m256i c0, const __m256i c1, const __m256i c2, const __m256i c0_sh1,
    const __m256i c1_sh1, const __m256i c2_sh1, const __m256i c0_sh2,
    const __m256i c1_sh2, const __m256i c2_sh2, const __m256i f,
    const unsigned short int divisor, signed char **filter) {

  __m256i r1, r2, r3, r4, m0, m1, m2, m3, m4;
  __m256i output_row0_even, output_row1_even, output_row0_odd, output_row1_odd;

  // const __m256i f  =
  // _mm256_set_epi16(52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759,52759);
  // const __m256i mask  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255);
  // const __m256i mask2  =
  // _mm256_set_epi8(0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0,0,0,0,255,0,0);
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  // const __m256i mask4  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0);
  // const __m256i mask5  =
  // _mm256_set_epi8(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,255,0,0,0,0,0,0,0,0,0,0,0,0,0,0);

  const __m256i output_mask = _mm256_set_epi16(
      0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535);
  const __m256i output_mask_sh1 = _mm256_set_epi16(
      0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0);
  // const __m256i output_mask_sh2  = _mm256_set_epi16(0,0,0,
  // 0,65535,0,0,65535,0,0,65535,0,0,65535,0,0);
  const __m256i mask_new =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0);
  const __m256i mask_new2 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0);

  __m256i reminder_mask1, reminder_mask2;

  reminder_mask1 =
      _mm256_load_si256((__m256i *)&reminder_msk1[REMINDER_ITERATIONS - 1][0]);
  reminder_mask2 =
      _mm256_load_si256((__m256i *)&reminder_msk2[REMINDER_ITERATIONS - 1][0]);

  // 1st col iteration
  // load the 4 rows
  r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col - 2]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col - 2]);
  r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 2]);
  r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 2]);

  /*
   * The above load operations load outside of the array bounds; these elements
   * are filled with zeros just after Furthermore, I add two extra zeros in the
   * end to compute col=N-2 and col=N-1. The last value of col is N-1, not N
   */

  // AND r0-r4 with reminder_mask
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);
  r3 = _mm256_and_si256(r3, reminder_mask1);
  r4 = _mm256_and_si256(r4, reminder_mask1);

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0);
  m2 = _mm256_maddubs_epi16(r2, c1);
  m3 = _mm256_maddubs_epi16(r3, c2);
  m4 = _mm256_maddubs_epi16(r4, c1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row0_even = _mm256_and_si256(m2, output_mask);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c2);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row1_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh1);
  m2 = _mm256_maddubs_epi16(r2, c1_sh1);
  m3 = _mm256_maddubs_epi16(r3, c2_sh1);
  m4 = _mm256_maddubs_epi16(r4, c1_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row0_odd = _mm256_and_si256(m2, output_mask);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c2_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_row1_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh2);
  m2 = _mm256_maddubs_epi16(r2, c1_sh2);
  m3 = _mm256_maddubs_epi16(r3, c2_sh2);
  m4 = _mm256_maddubs_epi16(r4, c1_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me
  // ola mazi

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_row0_even = _mm256_add_epi16(output_row0_even, m2);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c2_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto r2 kai meta na kanw diairesi me
  // ola mazi

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_row1_even = _mm256_add_epi16(output_row1_even, m2);

  // 4th col iteration
  // load the 5 rows
  r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col + 3 - 2]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col + 3 - 2]);
  r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col + 3 - 2]);
  r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col + 3 - 2]);

  // AND r0-r4 with reminder_mask
  r1 = _mm256_and_si256(r1, reminder_mask2);
  r2 = _mm256_and_si256(r2, reminder_mask2);
  r3 = _mm256_and_si256(r3, reminder_mask2);
  r4 = _mm256_and_si256(r4, reminder_mask2);

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0);
  m2 = _mm256_maddubs_epi16(r2, c1);
  m3 = _mm256_maddubs_epi16(r3, c2);
  m4 = _mm256_maddubs_epi16(r4, c1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);
  m2 = _mm256_slli_si256(m2, 2);
  output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0);
  m3 = _mm256_maddubs_epi16(r3, c1);
  m4 = _mm256_maddubs_epi16(r4, c2);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,4,7,10,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);
  m2 = _mm256_slli_si256(m2, 2);
  output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

  // 5th col iteration

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh1);
  m2 = _mm256_maddubs_epi16(r2, c1_sh1);
  m3 = _mm256_maddubs_epi16(r3, c2_sh1);
  m4 = _mm256_maddubs_epi16(r4, c1_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(m2,
                        output_mask); // holds results in positions 0,3,6,9,12
  m1 = _mm256_slli_si256(
      m2, 4); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new); // extract only the 6th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 12);
  m2 = _mm256_add_epi16(m1, m2);
  output_row0_even = _mm256_add_epi16(output_row0_even, m2);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0_sh1);
  m3 = _mm256_maddubs_epi16(r3, c1_sh1);
  m4 = _mm256_maddubs_epi16(r4, c2_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // prepei na prosthesw to pixel pu thelw sto m4 kai meta na kanw diairesi me
  // ola mazi
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,0,_mm256_extract_epi16(m0,8),0,0,0,0,0,0);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions

  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(m2,
                        output_mask); // holds results in positions 0,3,6,9,12
  m1 = _mm256_slli_si256(
      m2, 4); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new); // extract only the 6th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 12);
  m2 = _mm256_add_epi16(m1, m2);
  output_row1_even = _mm256_add_epi16(output_row1_even, m2);

  // 6th col iteration

  // row=0
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh2);
  m2 = _mm256_maddubs_epi16(r2, c1_sh2);
  m3 = _mm256_maddubs_epi16(r3, c2_sh2);
  m4 = _mm256_maddubs_epi16(r4, c1_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);
  m0 = _mm256_add_epi16(m0, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(
      m2, output_mask_sh1); // holds results in positions 1,4,7,10,13
  m1 = _mm256_slli_si256(
      m2, 2); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new2); // extract only the 7th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 14);
  m2 = _mm256_add_epi16(m1, m2);
  output_row0_odd = _mm256_add_epi16(output_row0_odd, m2);

  // now division follows
  output_row0_even = division(division_case, output_row0_even, f);
  output_row0_odd = division(division_case, output_row0_odd, f);

  // shift odd 1 position and add to even
  output_row0_odd = _mm256_slli_si256(output_row0_odd, 1);
  output_row0_even = _mm256_add_epi8(output_row0_even, output_row0_odd);

  // row=1
  // multiply with the mask
  m2 = _mm256_maddubs_epi16(r2, c0_sh2);
  m3 = _mm256_maddubs_epi16(r3, c1_sh2);
  m4 = _mm256_maddubs_epi16(r4, c2_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m2, m3);
  m0 = _mm256_add_epi16(m0, m4);

  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);
  // m4=_mm256_set_epi16(0,0,0,0,0,0,0,0,_mm256_extract_epi16(m2,8),0,0,0,0,0,0,0);
  m4 = _mm256_and_si256(m2, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions

  m1 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now.
  m2 = _mm256_and_si256(
      m2, output_mask_sh1); // holds results in positions 1,4,7,10,13
  m1 = _mm256_slli_si256(
      m2, 2); // put results in positions 2,5,8,11,14 , but 8 is not written
  m2 = _mm256_and_si256(m2, mask_new2); // extract only the 7th
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 14);
  m2 = _mm256_add_epi16(m1, m2);
  output_row1_odd = _mm256_add_epi16(output_row1_odd, m2);

  // now division follows
  output_row1_even = division(division_case, output_row1_even, f);
  output_row1_odd = division(division_case, output_row1_odd, f);

  // shift odd 1 position and add to even
  output_row1_odd = _mm256_slli_si256(output_row1_odd, 1);
  output_row1_even = _mm256_add_epi8(output_row1_even, output_row1_odd);

  switch (REMINDER_ITERATIONS) {
  case 1:
    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    break;
  case 2:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    break;
  case 3:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    break;
  case 4:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    break;
  case 5:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    break;
  case 6:
    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    break;
  case 7:
    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    break;
  case 8:
    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    break;
  case 9:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    break;
  case 10:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    break;
  case 11:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[N - 2][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[N - 1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    break;
  case 12:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[N - 2][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[N - 2][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[N - 1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[N - 1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    break;
  case 13:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[N - 2][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[N - 2][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);
    filt[N - 2][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 12);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[N - 1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[N - 1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    filt[N - 1][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 12);
    break;
  case 14:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[N - 2][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[N - 2][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);
    filt[N - 2][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 12);
    filt[N - 2][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 13);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[N - 1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[N - 1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    filt[N - 1][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 12);
    filt[N - 1][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 13);
    break;
  case 15:

    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row0_even, 0);
    filt[N - 2][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 1);
    filt[N - 2][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 2);
    filt[N - 2][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 3);
    filt[N - 2][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 4);
    filt[N - 2][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 5);
    filt[N - 2][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 6);
    filt[N - 2][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 7);
    filt[N - 2][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 8);
    filt[N - 2][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 9);
    filt[N - 2][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 10);
    filt[N - 2][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 11);
    filt[N - 2][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 12);
    filt[N - 2][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 13);
    filt[N - 2][col + 14] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 14);

    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row1_even, 0);
    filt[N - 1][col + 1] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 1);
    filt[N - 1][col + 2] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 2);
    filt[N - 1][col + 3] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 3);
    filt[N - 1][col + 4] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 4);
    filt[N - 1][col + 5] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 5);
    filt[N - 1][col + 6] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 6);
    filt[N - 1][col + 7] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 7);
    filt[N - 1][col + 8] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 8);
    filt[N - 1][col + 9] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 9);
    filt[N - 1][col + 10] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 10);
    filt[N - 1][col + 11] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 11);
    filt[N - 1][col + 12] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 12);
    filt[N - 1][col + 13] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 13);
    filt[N - 1][col + 14] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 14);
    break;
  case 16:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    break;
  case 17:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    break;
  case 18:

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    break;
  case 19:

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    break;
  case 20:

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    break;
  case 21:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    break;
  case 22:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    break;
  case 23:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    break;
  case 24:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    break;
  case 25:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    break;
  case 26:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    break;
  case 27:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[N - 2][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[N - 1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    break;
  case 28:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[N - 2][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[N - 2][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[N - 1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[N - 1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    break;
  case 29:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[N - 2][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[N - 2][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);
    filt[N - 2][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 28);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[N - 1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[N - 1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    filt[N - 1][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 28);
    break;
  case 30:

    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[N - 2][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[N - 2][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);
    filt[N - 2][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 28);
    filt[N - 2][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 29);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[N - 1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[N - 1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    filt[N - 1][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 28);
    filt[N - 1][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 29);
    break;
  case 31:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row0_even, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 16);
    filt[N - 2][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 17);
    filt[N - 2][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 18);
    filt[N - 2][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 19);
    filt[N - 2][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 20);
    filt[N - 2][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 21);
    filt[N - 2][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 22);
    filt[N - 2][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 23);
    filt[N - 2][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 24);
    filt[N - 2][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 25);
    filt[N - 2][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 26);
    filt[N - 2][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 27);
    filt[N - 2][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 28);
    filt[N - 2][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row0_even, 29);

    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row1_even, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 16);
    filt[N - 1][col + 17] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 17);
    filt[N - 1][col + 18] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 18);
    filt[N - 1][col + 19] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 19);
    filt[N - 1][col + 20] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 20);
    filt[N - 1][col + 21] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 21);
    filt[N - 1][col + 22] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 22);
    filt[N - 1][col + 23] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 23);
    filt[N - 1][col + 24] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 24);
    filt[N - 1][col + 25] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 25);
    filt[N - 1][col + 26] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 26);
    filt[N - 1][col + 27] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 27);
    filt[N - 1][col + 28] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 28);
    filt[N - 1][col + 29] =
        (unsigned char)_mm256_extract_epi8(output_row1_even, 29);

    // the filt[0:2][M-1] pixel will be computed without vectorization
    int newPixel;
    for (int row = N - 2; row <= N - 1; row++) {
      newPixel = 0;
      for (int rowOffset = -2; rowOffset <= 2; rowOffset++) {
        for (int colOffset = -2; colOffset <= 2; colOffset++) {

          if (((row + rowOffset) < N) && ((M - 1 + colOffset) < M) &&
              ((row + rowOffset) >= 0) && ((M - 1 + colOffset) >= 0))
            newPixel += frame1[row + rowOffset][M - 1 + colOffset] *
                        filter[2 + rowOffset][2 + colOffset];
        }
      }
      filt[row][M - 1] = (unsigned char)(newPixel / divisor);
    }

    break;
  default:
    printf("\n something went wrong");
  }

  return 0;
}

int loop_reminder_3x3(unsigned char **frame1, unsigned char **filt,
                      const unsigned int M, const unsigned int N,
                      const unsigned int row, const unsigned int col,
                      const unsigned int REMINDER_ITERATIONS,
                      const unsigned int division_case,
                      const unsigned short int divisor, signed char **filter,
                      const __m256i c0, const __m256i c1, const __m256i c0_sh1,
                      const __m256i c1_sh1, const __m256i c0_sh2,
                      const __m256i c1_sh2, const __m256i c0_sh3,
                      const __m256i c1_sh3, const __m256i f) {

  register __m256i r0, r1, r2, m0, m1, m2, m4, output_even, output_odd;
  int newPixel = 0;
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i output_mask =
      _mm256_set_epi16(0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0, 65535);
  const __m256i output_mask_sh1 =
      _mm256_set_epi16(0, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0);

  __m256i reminder_mask1;

  if (REMINDER_ITERATIONS == 0) {
    return 0; // no loop reminder is needed
  }

  reminder_mask1 = _mm256_load_si256(
      (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 1][0]);
  // reminder_mask2=_mm256_load_si256( (__m256i *)
  // &reminder_msk2[REMINDER_ITERATIONS-1][0]);

  // 1st col iteration
  // load the 5 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col - 1]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[row][col - 1]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col - 1]);

  /*
   * The above load operations load outside of the array bounds; these elements
   * are filled with zeros just after Furthermore, I add one extra zero in the
   * end to compute col=N-1. The last value of col is N-1, not N
   */

  // AND r0-r4 with reminder_mask
  r0 = _mm256_and_si256(r0, reminder_mask1);
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);

  // col iteration computes output pixels of    1,5,9,13,17,21,25,29
  //  col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
  //  col+2 iteration computes output pixels of 3,7,11,15,19,23,27
  //  col+3 iteration computes output pixels of 4,8,12,16,20,24,28
  // afterwards, col becomes 30 and repeat the above process

  // 1st col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  // later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit
  // register).
  output_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  output_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_even = _mm256_add_epi16(output_even, m2);

  // 4th col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh3);
  m1 = _mm256_maddubs_epi16(r1, c1_sh3);
  m2 = _mm256_maddubs_epi16(r2, c0_sh3);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  switch (REMINDER_ITERATIONS) {
  case 1:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    break;
  case 2:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    break;
  case 3:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    break;
  case 4:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    break;
  case 5:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    break;
  case 6:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    break;
  case 7:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    break;
  case 8:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    break;
  case 9:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    break;
  case 10:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    break;
  case 11:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    break;
  case 12:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    break;
  case 13:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    break;
  case 14:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);
    break;
  case 15:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);
    filt[row][col + 14] = (unsigned char)_mm256_extract_epi8(output_even, 14);
    break;
  case 16:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    break;
  case 17:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    break;
  case 18:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    break;
  case 19:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    break;
  case 20:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    break;
  case 21:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    break;
  case 22:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    break;
  case 23:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    break;
  case 24:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    break;
  case 25:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    break;
  case 26:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    break;
  case 27:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    break;
  case 28:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    break;
  case 29:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    break;
  case 30:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);

    // the filt[row][col+29] is computed unvectorized
    newPixel = 0;
    newPixel += frame1[row - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[row - 1][M - 1] * filter[0][1];

    newPixel += frame1[row][M - 1 - 1] * filter[1][0];
    newPixel += frame1[row][M - 1] * filter[1][1];

    newPixel += frame1[row + 1][M - 1 - 1] * filter[2][0];
    newPixel += frame1[row + 1][M - 1] * filter[2][1];

    filt[row][M - 1] = (unsigned char)(newPixel / divisor);

    break;

  case 31:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);

    // the filt[row][col+29/30] are computed unvectorized
    newPixel = 0;
    newPixel += frame1[row - 1][M - 2 - 1] * filter[0][0];
    newPixel += frame1[row - 1][M - 2] * filter[0][1];
    newPixel += frame1[row - 1][M - 2 + 1] * filter[0][2];

    newPixel += frame1[row][M - 2 - 1] * filter[1][0];
    newPixel += frame1[row][M - 2] * filter[1][1];
    newPixel += frame1[row][M - 2 + 1] * filter[1][2];

    newPixel += frame1[row + 1][M - 2 - 1] * filter[2][0];
    newPixel += frame1[row + 1][M - 2] * filter[2][1];
    newPixel += frame1[row + 1][M - 2 + 1] * filter[2][2];

    filt[row][M - 2] = (unsigned char)(newPixel / divisor);

    newPixel = 0;
    newPixel += frame1[row - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[row - 1][M - 1] * filter[0][1];

    newPixel += frame1[row][M - 1 - 1] * filter[1][0];
    newPixel += frame1[row][M - 1] * filter[1][1];

    newPixel += frame1[row + 1][M - 1 - 1] * filter[2][0];
    newPixel += frame1[row + 1][M - 1] * filter[2][1];

    filt[row][M - 1] = (unsigned char)(newPixel / divisor);
    break;
  default:
    printf("\nsomething went wrong");
    return -1;
  }

  return 0;
}

int loop_reminder_3x3_first_values(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned int col,
    const unsigned int REMINDER_ITERATIONS, const unsigned int division_case,
    const unsigned short int divisor, signed char **filter, const __m256i c0,
    const __m256i c1, const __m256i c0_sh1, const __m256i c1_sh1,
    const __m256i c0_sh2, const __m256i c1_sh2, const __m256i c0_sh3,
    const __m256i c1_sh3, const __m256i f) {

  register __m256i r0, r1, r2, m0, m1, m2, m4, output_even, output_odd,
      output_row0, output_row1;
  int newPixel;
  unsigned int i;
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i output_mask =
      _mm256_set_epi16(0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0, 65535);
  const __m256i output_mask_sh1 =
      _mm256_set_epi16(0, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0);

  __m256i reminder_mask1;

  if (REMINDER_ITERATIONS == 0) {
    return 0; // no loop reminder is needed
  }

  reminder_mask1 = _mm256_load_si256(
      (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 1][0]);
  // reminder_mask2=_mm256_load_si256( (__m256i *)
  // &reminder_msk2[REMINDER_ITERATIONS-1][0]);

  // 1st col iteration
  // load the 5 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col - 1]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col - 1]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col - 1]);

  /*
   * The above load operations load outside of the array bounds; these elements
   * are filled with zeros just after Furthermore, I add one extra zero in the
   * end to compute col=N-1. The last value of col is N-1, not N
   */

  // AND r0-r4 with reminder_mask
  r0 = _mm256_and_si256(r0, reminder_mask1);
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);

  // col iteration computes output pixels of    1,5,9,13,17,21,25,29
  //  col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
  //  col+2 iteration computes output pixels of 3,7,11,15,19,23,27
  //  col+3 iteration computes output pixels of 4,8,12,16,20,24,28
  // afterwards, col becomes 30 and repeat the above process

  // 1st col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  // later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit
  // register).
  output_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  output_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_even = _mm256_add_epi16(output_even, m2);

  // 4th col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh3);
  m1 = _mm256_maddubs_epi16(r1, c1_sh3);
  m2 = _mm256_maddubs_epi16(r2, c0_sh3);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  output_row1 = output_even;

  // row==0
  // 1st col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c1);
  m1 = _mm256_maddubs_epi16(r1, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  // later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit
  // register).
  output_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c1_sh1);
  m1 = _mm256_maddubs_epi16(r1, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  output_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c1_sh2);
  m1 = _mm256_maddubs_epi16(r1, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_even = _mm256_add_epi16(output_even, m2);

  // 4th col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c1_sh3);
  m1 = _mm256_maddubs_epi16(r1, c0_sh3);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  output_row0 = output_even;

  switch (REMINDER_ITERATIONS) {
  case 1:
    filt[1][col] = (unsigned char)_mm256_extract_epi8(output_row1, 0);
    filt[0][col] = (unsigned char)_mm256_extract_epi8(output_row0, 0);

    break;
  case 2:
    for (i = 0; i <= 1; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 3:
    for (i = 0; i <= 2; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 4:
    for (i = 0; i <= 3; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 5:
    for (i = 0; i <= 4; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 6:
    for (i = 0; i <= 5; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 7:
    for (i = 0; i <= 6; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 8:
    for (i = 0; i <= 7; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 9:
    for (i = 0; i <= 8; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 10:
    for (i = 0; i <= 9; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 11:
    for (i = 0; i <= 10; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 12:
    for (i = 0; i <= 11; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 13:
    for (i = 0; i <= 12; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 14:
    for (i = 0; i <= 13; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 15:
    for (i = 0; i <= 14; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 16:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    break;
  case 17:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels
    filt[1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row1, 16);
    filt[0][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    break;
  case 18:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 17; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 19:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 18; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 20:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 19; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 21:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 20; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 22:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 21; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 23:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 22; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 24:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 23; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 25:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 24; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 26:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 25; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }

    break;
  case 27:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 26; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }

    break;
  case 28:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 27; i++) {
      filt[1][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[0][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 29:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    filt[1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row1, 16);
    filt[1][col + 17] = (unsigned char)_mm256_extract_epi8(output_row1, 17);
    filt[1][col + 18] = (unsigned char)_mm256_extract_epi8(output_row1, 18);
    filt[1][col + 19] = (unsigned char)_mm256_extract_epi8(output_row1, 19);
    filt[1][col + 20] = (unsigned char)_mm256_extract_epi8(output_row1, 20);
    filt[1][col + 21] = (unsigned char)_mm256_extract_epi8(output_row1, 21);
    filt[1][col + 22] = (unsigned char)_mm256_extract_epi8(output_row1, 22);
    filt[1][col + 23] = (unsigned char)_mm256_extract_epi8(output_row1, 23);
    filt[1][col + 24] = (unsigned char)_mm256_extract_epi8(output_row1, 24);
    filt[1][col + 25] = (unsigned char)_mm256_extract_epi8(output_row1, 25);
    filt[1][col + 26] = (unsigned char)_mm256_extract_epi8(output_row1, 26);
    filt[1][col + 27] = (unsigned char)_mm256_extract_epi8(output_row1, 27);
    filt[1][col + 28] = (unsigned char)_mm256_extract_epi8(output_row1, 28);

    filt[0][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    filt[0][col + 17] = (unsigned char)_mm256_extract_epi8(output_row0, 17);
    filt[0][col + 18] = (unsigned char)_mm256_extract_epi8(output_row0, 18);
    filt[0][col + 19] = (unsigned char)_mm256_extract_epi8(output_row0, 19);
    filt[0][col + 20] = (unsigned char)_mm256_extract_epi8(output_row0, 20);
    filt[0][col + 21] = (unsigned char)_mm256_extract_epi8(output_row0, 21);
    filt[0][col + 22] = (unsigned char)_mm256_extract_epi8(output_row0, 22);
    filt[0][col + 23] = (unsigned char)_mm256_extract_epi8(output_row0, 23);
    filt[0][col + 24] = (unsigned char)_mm256_extract_epi8(output_row0, 24);
    filt[0][col + 25] = (unsigned char)_mm256_extract_epi8(output_row0, 25);
    filt[0][col + 26] = (unsigned char)_mm256_extract_epi8(output_row0, 26);
    filt[0][col + 27] = (unsigned char)_mm256_extract_epi8(output_row0, 27);
    filt[0][col + 28] = (unsigned char)_mm256_extract_epi8(output_row0, 28);

    break;
  case 30:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    filt[1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row1, 16);
    filt[1][col + 17] = (unsigned char)_mm256_extract_epi8(output_row1, 17);
    filt[1][col + 18] = (unsigned char)_mm256_extract_epi8(output_row1, 18);
    filt[1][col + 19] = (unsigned char)_mm256_extract_epi8(output_row1, 19);
    filt[1][col + 20] = (unsigned char)_mm256_extract_epi8(output_row1, 20);
    filt[1][col + 21] = (unsigned char)_mm256_extract_epi8(output_row1, 21);
    filt[1][col + 22] = (unsigned char)_mm256_extract_epi8(output_row1, 22);
    filt[1][col + 23] = (unsigned char)_mm256_extract_epi8(output_row1, 23);
    filt[1][col + 24] = (unsigned char)_mm256_extract_epi8(output_row1, 24);
    filt[1][col + 25] = (unsigned char)_mm256_extract_epi8(output_row1, 25);
    filt[1][col + 26] = (unsigned char)_mm256_extract_epi8(output_row1, 26);
    filt[1][col + 27] = (unsigned char)_mm256_extract_epi8(output_row1, 27);
    filt[1][col + 28] = (unsigned char)_mm256_extract_epi8(output_row1, 28);

    filt[0][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    filt[0][col + 17] = (unsigned char)_mm256_extract_epi8(output_row0, 17);
    filt[0][col + 18] = (unsigned char)_mm256_extract_epi8(output_row0, 18);
    filt[0][col + 19] = (unsigned char)_mm256_extract_epi8(output_row0, 19);
    filt[0][col + 20] = (unsigned char)_mm256_extract_epi8(output_row0, 20);
    filt[0][col + 21] = (unsigned char)_mm256_extract_epi8(output_row0, 21);
    filt[0][col + 22] = (unsigned char)_mm256_extract_epi8(output_row0, 22);
    filt[0][col + 23] = (unsigned char)_mm256_extract_epi8(output_row0, 23);
    filt[0][col + 24] = (unsigned char)_mm256_extract_epi8(output_row0, 24);
    filt[0][col + 25] = (unsigned char)_mm256_extract_epi8(output_row0, 25);
    filt[0][col + 26] = (unsigned char)_mm256_extract_epi8(output_row0, 26);
    filt[0][col + 27] = (unsigned char)_mm256_extract_epi8(output_row0, 27);
    filt[0][col + 28] = (unsigned char)_mm256_extract_epi8(output_row0, 28);

    // the filt[row][col+29] is computed unvectorized
    newPixel = 0;
    newPixel += frame1[1 - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[1 - 1][M - 1] * filter[0][1];

    newPixel += frame1[1][M - 1 - 1] * filter[1][0];
    newPixel += frame1[1][M - 1] * filter[1][1];

    newPixel += frame1[1 + 1][M - 1 - 1] * filter[2][0];
    newPixel += frame1[1 + 1][M - 1] * filter[2][1];

    filt[1][M - 1] = (unsigned char)(newPixel / divisor);

    newPixel = 0;
    newPixel += frame1[1 - 1][M - 1 - 1] * filter[1][0];
    newPixel += frame1[1 - 1][M - 1] * filter[1][1];

    newPixel += frame1[1][M - 1 - 1] * filter[2][0];
    newPixel += frame1[1][M - 1] * filter[2][1];

    filt[0][M - 1] = (unsigned char)(newPixel / divisor);

    break;

  case 31:
    _mm_storeu_si128((__m128i *)&filt[1][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[0][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    filt[1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row1, 16);
    filt[1][col + 17] = (unsigned char)_mm256_extract_epi8(output_row1, 17);
    filt[1][col + 18] = (unsigned char)_mm256_extract_epi8(output_row1, 18);
    filt[1][col + 19] = (unsigned char)_mm256_extract_epi8(output_row1, 19);
    filt[1][col + 20] = (unsigned char)_mm256_extract_epi8(output_row1, 20);
    filt[1][col + 21] = (unsigned char)_mm256_extract_epi8(output_row1, 21);
    filt[1][col + 22] = (unsigned char)_mm256_extract_epi8(output_row1, 22);
    filt[1][col + 23] = (unsigned char)_mm256_extract_epi8(output_row1, 23);
    filt[1][col + 24] = (unsigned char)_mm256_extract_epi8(output_row1, 24);
    filt[1][col + 25] = (unsigned char)_mm256_extract_epi8(output_row1, 25);
    filt[1][col + 26] = (unsigned char)_mm256_extract_epi8(output_row1, 26);
    filt[1][col + 27] = (unsigned char)_mm256_extract_epi8(output_row1, 27);
    filt[1][col + 28] = (unsigned char)_mm256_extract_epi8(output_row1, 28);

    filt[0][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    filt[0][col + 17] = (unsigned char)_mm256_extract_epi8(output_row0, 17);
    filt[0][col + 18] = (unsigned char)_mm256_extract_epi8(output_row0, 18);
    filt[0][col + 19] = (unsigned char)_mm256_extract_epi8(output_row0, 19);
    filt[0][col + 20] = (unsigned char)_mm256_extract_epi8(output_row0, 20);
    filt[0][col + 21] = (unsigned char)_mm256_extract_epi8(output_row0, 21);
    filt[0][col + 22] = (unsigned char)_mm256_extract_epi8(output_row0, 22);
    filt[0][col + 23] = (unsigned char)_mm256_extract_epi8(output_row0, 23);
    filt[0][col + 24] = (unsigned char)_mm256_extract_epi8(output_row0, 24);
    filt[0][col + 25] = (unsigned char)_mm256_extract_epi8(output_row0, 25);
    filt[0][col + 26] = (unsigned char)_mm256_extract_epi8(output_row0, 26);
    filt[0][col + 27] = (unsigned char)_mm256_extract_epi8(output_row0, 27);
    filt[0][col + 28] = (unsigned char)_mm256_extract_epi8(output_row0, 28);

    // the filt[1][col+29/30] are computed unvectorized
    newPixel = 0;
    newPixel += frame1[1 - 1][M - 2 - 1] * filter[0][0];
    newPixel += frame1[1 - 1][M - 2] * filter[0][1];
    newPixel += frame1[1 - 1][M - 2 + 1] * filter[0][2];

    newPixel += frame1[1][M - 2 - 1] * filter[1][0];
    newPixel += frame1[1][M - 2] * filter[1][1];
    newPixel += frame1[1][M - 2 + 1] * filter[1][2];

    newPixel += frame1[1 + 1][M - 2 - 1] * filter[2][0];
    newPixel += frame1[1 + 1][M - 2] * filter[2][1];
    newPixel += frame1[1 + 1][M - 2 + 1] * filter[2][2];

    filt[1][M - 2] = (unsigned char)(newPixel / divisor);

    newPixel = 0;
    newPixel += frame1[1 - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[1 - 1][M - 1] * filter[0][1];

    newPixel += frame1[1][M - 1 - 1] * filter[1][0];
    newPixel += frame1[1][M - 1] * filter[1][1];

    newPixel += frame1[1 + 1][M - 1 - 1] * filter[2][0];
    newPixel += frame1[1 + 1][M - 1] * filter[2][1];

    filt[1][M - 1] = (unsigned char)(newPixel / divisor);

    newPixel = 0;

    newPixel += frame1[0][M - 2 - 1] * filter[1][0];
    newPixel += frame1[0][M - 2] * filter[1][1];
    newPixel += frame1[0][M - 2 + 1] * filter[1][2];

    newPixel += frame1[1][M - 2 - 1] * filter[2][0];
    newPixel += frame1[1][M - 2] * filter[2][1];
    newPixel += frame1[1][M - 2 + 1] * filter[2][2];

    filt[0][M - 2] = (unsigned char)(newPixel / divisor);

    newPixel = 0;

    newPixel += frame1[0][M - 1 - 1] * filter[1][0];
    newPixel += frame1[0][M - 1] * filter[1][1];

    newPixel += frame1[1][M - 1 - 1] * filter[2][0];
    newPixel += frame1[1][M - 1] * filter[2][1];

    filt[0][M - 1] = (unsigned char)(newPixel / divisor);

    break;

  default:
    printf("\nsomething went wrong");
    return -1;
  }

  return 0;
}

int loop_reminder_3x3_last_values(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned int col,
    const unsigned int REMINDER_ITERATIONS, const unsigned int division_case,
    const unsigned short int divisor, signed char **filter, const __m256i c0,
    const __m256i c1, const __m256i c0_sh1, const __m256i c1_sh1,
    const __m256i c0_sh2, const __m256i c1_sh2, const __m256i c0_sh3,
    const __m256i c1_sh3, const __m256i f) {

  register __m256i r0, r1, r2, m0, m1, m2, m4, output_even, output_odd,
      output_row0, output_row1;
  int newPixel;
  unsigned int i;
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i output_mask =
      _mm256_set_epi16(0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0, 65535);
  const __m256i output_mask_sh1 =
      _mm256_set_epi16(0, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0);

  __m256i reminder_mask1;

  if (REMINDER_ITERATIONS == 0) {
    return 0; // no loop reminder is needed
  }

  reminder_mask1 = _mm256_load_si256(
      (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 1][0]);
  // reminder_mask2=_mm256_load_si256( (__m256i *)
  // &reminder_msk2[REMINDER_ITERATIONS-1][0]);

  // 1st col iteration
  // load the 5 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col - 1]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 1]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 1]);

  /*
   * The above load operations load outside of the array bounds; these elements
   * are filled with zeros just after Furthermore, I add one extra zero in the
   * end to compute col=N-1. The last value of col is N-1, not N
   */

  // AND r0-r4 with reminder_mask
  r0 = _mm256_and_si256(r0, reminder_mask1);
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);

  // col iteration computes output pixels of    1,5,9,13,17,21,25,29
  //  col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
  //  col+2 iteration computes output pixels of 3,7,11,15,19,23,27
  //  col+3 iteration computes output pixels of 4,8,12,16,20,24,28
  // afterwards, col becomes 30 and repeat the above process

  // 1st col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  // later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit
  // register).
  output_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh1);
  m1 = _mm256_maddubs_epi16(r1, c1_sh1);
  m2 = _mm256_maddubs_epi16(r2, c0_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  output_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration
  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh2);
  m1 = _mm256_maddubs_epi16(r1, c1_sh2);
  m2 = _mm256_maddubs_epi16(r2, c0_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_even = _mm256_add_epi16(output_even, m2);

  // 4th col iteration

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0_sh3);
  m1 = _mm256_maddubs_epi16(r1, c1_sh3);
  m2 = _mm256_maddubs_epi16(r2, c0_sh3);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  output_row1 = output_even;

  // row==N-1
  // row==N-1
  // 1st col iteration

  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0);
  m2 = _mm256_maddubs_epi16(r2, c1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  // later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit
  // register).
  output_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh1);
  m2 = _mm256_maddubs_epi16(r2, c1_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  output_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh2);
  m2 = _mm256_maddubs_epi16(r2, c1_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_even = _mm256_add_epi16(output_even, m2);

  // 4th col iteration

  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh3);
  m2 = _mm256_maddubs_epi16(r2, c1_sh3);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  output_row0 = output_even;

  switch (REMINDER_ITERATIONS) {
  case 1:
    filt[N - 2][col] = (unsigned char)_mm256_extract_epi8(output_row1, 0);
    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row0, 0);

    break;
  case 2:
    for (i = 0; i <= 1; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 3:
    for (i = 0; i <= 2; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 4:
    for (i = 0; i <= 3; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 5:
    for (i = 0; i <= 4; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 6:
    for (i = 0; i <= 5; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 7:
    for (i = 0; i <= 6; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 8:
    for (i = 0; i <= 7; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 9:
    for (i = 0; i <= 8; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 10:
    for (i = 0; i <= 9; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 11:
    for (i = 0; i <= 10; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 12:
    for (i = 0; i <= 11; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 13:
    for (i = 0; i <= 12; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 14:
    for (i = 0; i <= 13; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 15:
    for (i = 0; i <= 14; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 16:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    break;
  case 17:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels
    filt[N - 2][col + 16] = (unsigned char)_mm256_extract_epi8(output_row1, 16);
    filt[N - 1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    break;
  case 18:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 17; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 19:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 18; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 20:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 19; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 21:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 20; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 22:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 21; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 23:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 22; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 24:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 23; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 25:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 24; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 26:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 25; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }

    break;
  case 27:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 26; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }

    break;
  case 28:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 27; i++) {
      filt[N - 2][col + i] = (unsigned char)_mm256_extract_epi8(output_row1, i);
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 29:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    filt[N - 2][col + 16] = (unsigned char)_mm256_extract_epi8(output_row1, 16);
    filt[N - 2][col + 17] = (unsigned char)_mm256_extract_epi8(output_row1, 17);
    filt[N - 2][col + 18] = (unsigned char)_mm256_extract_epi8(output_row1, 18);
    filt[N - 2][col + 19] = (unsigned char)_mm256_extract_epi8(output_row1, 19);
    filt[N - 2][col + 20] = (unsigned char)_mm256_extract_epi8(output_row1, 20);
    filt[N - 2][col + 21] = (unsigned char)_mm256_extract_epi8(output_row1, 21);
    filt[N - 2][col + 22] = (unsigned char)_mm256_extract_epi8(output_row1, 22);
    filt[N - 2][col + 23] = (unsigned char)_mm256_extract_epi8(output_row1, 23);
    filt[N - 2][col + 24] = (unsigned char)_mm256_extract_epi8(output_row1, 24);
    filt[N - 2][col + 25] = (unsigned char)_mm256_extract_epi8(output_row1, 25);
    filt[N - 2][col + 26] = (unsigned char)_mm256_extract_epi8(output_row1, 26);
    filt[N - 2][col + 27] = (unsigned char)_mm256_extract_epi8(output_row1, 27);
    filt[N - 2][col + 28] = (unsigned char)_mm256_extract_epi8(output_row1, 28);

    filt[N - 1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    filt[N - 1][col + 17] = (unsigned char)_mm256_extract_epi8(output_row0, 17);
    filt[N - 1][col + 18] = (unsigned char)_mm256_extract_epi8(output_row0, 18);
    filt[N - 1][col + 19] = (unsigned char)_mm256_extract_epi8(output_row0, 19);
    filt[N - 1][col + 20] = (unsigned char)_mm256_extract_epi8(output_row0, 20);
    filt[N - 1][col + 21] = (unsigned char)_mm256_extract_epi8(output_row0, 21);
    filt[N - 1][col + 22] = (unsigned char)_mm256_extract_epi8(output_row0, 22);
    filt[N - 1][col + 23] = (unsigned char)_mm256_extract_epi8(output_row0, 23);
    filt[N - 1][col + 24] = (unsigned char)_mm256_extract_epi8(output_row0, 24);
    filt[N - 1][col + 25] = (unsigned char)_mm256_extract_epi8(output_row0, 25);
    filt[N - 1][col + 26] = (unsigned char)_mm256_extract_epi8(output_row0, 26);
    filt[N - 1][col + 27] = (unsigned char)_mm256_extract_epi8(output_row0, 27);
    filt[N - 1][col + 28] = (unsigned char)_mm256_extract_epi8(output_row0, 28);

    break;
  case 30:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    filt[N - 2][col + 16] = (unsigned char)_mm256_extract_epi8(output_row1, 16);
    filt[N - 2][col + 17] = (unsigned char)_mm256_extract_epi8(output_row1, 17);
    filt[N - 2][col + 18] = (unsigned char)_mm256_extract_epi8(output_row1, 18);
    filt[N - 2][col + 19] = (unsigned char)_mm256_extract_epi8(output_row1, 19);
    filt[N - 2][col + 20] = (unsigned char)_mm256_extract_epi8(output_row1, 20);
    filt[N - 2][col + 21] = (unsigned char)_mm256_extract_epi8(output_row1, 21);
    filt[N - 2][col + 22] = (unsigned char)_mm256_extract_epi8(output_row1, 22);
    filt[N - 2][col + 23] = (unsigned char)_mm256_extract_epi8(output_row1, 23);
    filt[N - 2][col + 24] = (unsigned char)_mm256_extract_epi8(output_row1, 24);
    filt[N - 2][col + 25] = (unsigned char)_mm256_extract_epi8(output_row1, 25);
    filt[N - 2][col + 26] = (unsigned char)_mm256_extract_epi8(output_row1, 26);
    filt[N - 2][col + 27] = (unsigned char)_mm256_extract_epi8(output_row1, 27);
    filt[N - 2][col + 28] = (unsigned char)_mm256_extract_epi8(output_row1, 28);

    filt[N - 1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    filt[N - 1][col + 17] = (unsigned char)_mm256_extract_epi8(output_row0, 17);
    filt[N - 1][col + 18] = (unsigned char)_mm256_extract_epi8(output_row0, 18);
    filt[N - 1][col + 19] = (unsigned char)_mm256_extract_epi8(output_row0, 19);
    filt[N - 1][col + 20] = (unsigned char)_mm256_extract_epi8(output_row0, 20);
    filt[N - 1][col + 21] = (unsigned char)_mm256_extract_epi8(output_row0, 21);
    filt[N - 1][col + 22] = (unsigned char)_mm256_extract_epi8(output_row0, 22);
    filt[N - 1][col + 23] = (unsigned char)_mm256_extract_epi8(output_row0, 23);
    filt[N - 1][col + 24] = (unsigned char)_mm256_extract_epi8(output_row0, 24);
    filt[N - 1][col + 25] = (unsigned char)_mm256_extract_epi8(output_row0, 25);
    filt[N - 1][col + 26] = (unsigned char)_mm256_extract_epi8(output_row0, 26);
    filt[N - 1][col + 27] = (unsigned char)_mm256_extract_epi8(output_row0, 27);
    filt[N - 1][col + 28] = (unsigned char)_mm256_extract_epi8(output_row0, 28);

    // the filt[row][col+29] is computed unvectorized
    newPixel = 0;
    newPixel += frame1[N - 2 - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[N - 2 - 1][M - 1] * filter[0][1];

    newPixel += frame1[N - 2][M - 1 - 1] * filter[1][0];
    newPixel += frame1[N - 2][M - 1] * filter[1][1];

    newPixel += frame1[N - 2 + 1][M - 1 - 1] * filter[2][0];
    newPixel += frame1[N - 2 + 1][M - 1] * filter[2][1];

    filt[N - 2][M - 1] = (unsigned char)(newPixel / divisor);

    newPixel = 0;
    newPixel += frame1[N - 1 - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[N - 1 - 1][M - 1] * filter[0][1];

    newPixel += frame1[N - 1][M - 1 - 1] * filter[1][0];
    newPixel += frame1[N - 1][M - 1] * filter[1][1];

    filt[N - 1][M - 1] = (unsigned char)(newPixel / divisor);

    break;

  case 31:
    _mm_storeu_si128((__m128i *)&filt[N - 2][col],
                     _mm256_extractf128_si256(
                         output_row1, 0)); // store low 128bit - 16pixels
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    filt[N - 2][col + 16] = (unsigned char)_mm256_extract_epi8(output_row1, 16);
    filt[N - 2][col + 17] = (unsigned char)_mm256_extract_epi8(output_row1, 17);
    filt[N - 2][col + 18] = (unsigned char)_mm256_extract_epi8(output_row1, 18);
    filt[N - 2][col + 19] = (unsigned char)_mm256_extract_epi8(output_row1, 19);
    filt[N - 2][col + 20] = (unsigned char)_mm256_extract_epi8(output_row1, 20);
    filt[N - 2][col + 21] = (unsigned char)_mm256_extract_epi8(output_row1, 21);
    filt[N - 2][col + 22] = (unsigned char)_mm256_extract_epi8(output_row1, 22);
    filt[N - 2][col + 23] = (unsigned char)_mm256_extract_epi8(output_row1, 23);
    filt[N - 2][col + 24] = (unsigned char)_mm256_extract_epi8(output_row1, 24);
    filt[N - 2][col + 25] = (unsigned char)_mm256_extract_epi8(output_row1, 25);
    filt[N - 2][col + 26] = (unsigned char)_mm256_extract_epi8(output_row1, 26);
    filt[N - 2][col + 27] = (unsigned char)_mm256_extract_epi8(output_row1, 27);
    filt[N - 2][col + 28] = (unsigned char)_mm256_extract_epi8(output_row1, 28);

    filt[N - 1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    filt[N - 1][col + 17] = (unsigned char)_mm256_extract_epi8(output_row0, 17);
    filt[N - 1][col + 18] = (unsigned char)_mm256_extract_epi8(output_row0, 18);
    filt[N - 1][col + 19] = (unsigned char)_mm256_extract_epi8(output_row0, 19);
    filt[N - 1][col + 20] = (unsigned char)_mm256_extract_epi8(output_row0, 20);
    filt[N - 1][col + 21] = (unsigned char)_mm256_extract_epi8(output_row0, 21);
    filt[N - 1][col + 22] = (unsigned char)_mm256_extract_epi8(output_row0, 22);
    filt[N - 1][col + 23] = (unsigned char)_mm256_extract_epi8(output_row0, 23);
    filt[N - 1][col + 24] = (unsigned char)_mm256_extract_epi8(output_row0, 24);
    filt[N - 1][col + 25] = (unsigned char)_mm256_extract_epi8(output_row0, 25);
    filt[N - 1][col + 26] = (unsigned char)_mm256_extract_epi8(output_row0, 26);
    filt[N - 1][col + 27] = (unsigned char)_mm256_extract_epi8(output_row0, 27);
    filt[N - 1][col + 28] = (unsigned char)_mm256_extract_epi8(output_row0, 28);

    // the filt[N-2][col+29/30] are computed unvectorized
    newPixel = 0;
    newPixel += frame1[N - 2 - 1][M - 2 - 1] * filter[0][0];
    newPixel += frame1[N - 2 - 1][M - 2] * filter[0][1];
    newPixel += frame1[N - 2 - 1][M - 2 + 1] * filter[0][2];

    newPixel += frame1[N - 2][M - 2 - 1] * filter[1][0];
    newPixel += frame1[N - 2][M - 2] * filter[1][1];
    newPixel += frame1[N - 2][M - 2 + 1] * filter[1][2];

    newPixel += frame1[N - 2 + 1][M - 2 - 1] * filter[2][0];
    newPixel += frame1[N - 2 + 1][M - 2] * filter[2][1];
    newPixel += frame1[N - 2 + 1][M - 2 + 1] * filter[2][2];

    filt[N - 2][M - 2] = (unsigned char)(newPixel / divisor);

    newPixel = 0;
    newPixel += frame1[N - 2 - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[N - 2 - 1][M - 1] * filter[0][1];

    newPixel += frame1[N - 2][M - 1 - 1] * filter[1][0];
    newPixel += frame1[N - 2][M - 1] * filter[1][1];

    newPixel += frame1[N - 2 + 1][M - 1 - 1] * filter[2][0];
    newPixel += frame1[N - 2 + 1][M - 1] * filter[2][1];

    filt[N - 2][M - 1] = (unsigned char)(newPixel / divisor);

    newPixel = 0;

    newPixel += frame1[N - 1 - 1][M - 2 - 1] * filter[0][0];
    newPixel += frame1[N - 1 - 1][M - 2] * filter[0][1];
    newPixel += frame1[N - 1 - 1][M - 2 + 1] * filter[0][2];

    newPixel += frame1[N - 1][M - 2 - 1] * filter[1][0];
    newPixel += frame1[N - 1][M - 2] * filter[1][1];
    newPixel += frame1[N - 1][M - 2 + 1] * filter[1][2];

    filt[N - 1][M - 2] = (unsigned char)(newPixel / divisor);

    newPixel = 0;

    newPixel += frame1[N - 1 - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[N - 1 - 1][M - 1] * filter[0][1];

    newPixel += frame1[N - 1][M - 1 - 1] * filter[1][0];
    newPixel += frame1[N - 1][M - 1] * filter[1][1];

    filt[N - 1][M - 1] = (unsigned char)(newPixel / divisor);

    break;

  default:
    printf("\nsomething went wrong");
    return -1;
  }

  return 0;
}

int loop_reminder_3x3_last_row_only(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned int col,
    const unsigned int REMINDER_ITERATIONS, const unsigned int division_case,
    const unsigned short int divisor, signed char **filter, const __m256i c0,
    const __m256i c1, const __m256i c0_sh1, const __m256i c1_sh1,
    const __m256i c0_sh2, const __m256i c1_sh2, const __m256i c0_sh3,
    const __m256i c1_sh3, const __m256i f) {

  register __m256i r1, r2, m0, m1, m2, m4, output_even, output_odd, output_row0;
  int newPixel;
  unsigned int i;
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i output_mask =
      _mm256_set_epi16(0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0, 65535);
  const __m256i output_mask_sh1 =
      _mm256_set_epi16(0, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0);

  __m256i reminder_mask1;

  if (REMINDER_ITERATIONS == 0) {
    return 0; // no loop reminder is needed
  }

  reminder_mask1 = _mm256_load_si256(
      (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 1][0]);
  // reminder_mask2=_mm256_load_si256( (__m256i *)
  // &reminder_msk2[REMINDER_ITERATIONS-1][0]);

  // 1st col iteration
  r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 1]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 1]);

  /*
   * The above load operations load outside of the array bounds; these elements
   * are filled with zeros just after Furthermore, I add one extra zero in the
   * end to compute col=N-1. The last value of col is N-1, not N
   */

  // AND r0-r4 with reminder_mask
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);

  // row==N-1
  // row==N-1
  // 1st col iteration

  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0);
  m2 = _mm256_maddubs_epi16(r2, c1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  // later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit
  // register).
  output_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh1);
  m2 = _mm256_maddubs_epi16(r2, c1_sh1);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  output_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration
  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh2);
  m2 = _mm256_maddubs_epi16(r2, c1_sh2);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_even = _mm256_add_epi16(output_even, m2);

  // 4th col iteration

  // multiply with the mask
  m1 = _mm256_maddubs_epi16(r1, c0_sh3);
  m2 = _mm256_maddubs_epi16(r2, c1_sh3);

  // vertical add
  m0 = _mm256_add_epi16(m1, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 14); // shift 7 short int positions or 14 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions
  // 1,3,5,7,9,11,13. keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask_sh1);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  output_row0 = output_even;

  switch (REMINDER_ITERATIONS) {
  case 1:
    filt[N - 1][col] = (unsigned char)_mm256_extract_epi8(output_row0, 0);

    break;
  case 2:
    for (i = 0; i <= 1; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 3:
    for (i = 0; i <= 2; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 4:
    for (i = 0; i <= 3; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 5:
    for (i = 0; i <= 4; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 6:
    for (i = 0; i <= 5; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 7:
    for (i = 0; i <= 6; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 8:
    for (i = 0; i <= 7; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 9:
    for (i = 0; i <= 8; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 10:
    for (i = 0; i <= 9; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 11:
    for (i = 0; i <= 10; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 12:
    for (i = 0; i <= 11; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 13:
    for (i = 0; i <= 12; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 14:
    for (i = 0; i <= 13; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 15:
    for (i = 0; i <= 14; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 16:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    break;
  case 17:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels
    filt[N - 1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    break;
  case 18:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 17; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 19:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 18; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 20:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 19; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 21:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 20; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 22:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 21; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 23:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 22; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 24:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 23; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 25:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 24; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 26:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 25; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }

    break;
  case 27:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 26; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }

    break;
  case 28:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    for (i = 16; i <= 27; i++) {
      filt[N - 1][col + i] = (unsigned char)_mm256_extract_epi8(output_row0, i);
    }
    break;
  case 29:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    filt[N - 1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    filt[N - 1][col + 17] = (unsigned char)_mm256_extract_epi8(output_row0, 17);
    filt[N - 1][col + 18] = (unsigned char)_mm256_extract_epi8(output_row0, 18);
    filt[N - 1][col + 19] = (unsigned char)_mm256_extract_epi8(output_row0, 19);
    filt[N - 1][col + 20] = (unsigned char)_mm256_extract_epi8(output_row0, 20);
    filt[N - 1][col + 21] = (unsigned char)_mm256_extract_epi8(output_row0, 21);
    filt[N - 1][col + 22] = (unsigned char)_mm256_extract_epi8(output_row0, 22);
    filt[N - 1][col + 23] = (unsigned char)_mm256_extract_epi8(output_row0, 23);
    filt[N - 1][col + 24] = (unsigned char)_mm256_extract_epi8(output_row0, 24);
    filt[N - 1][col + 25] = (unsigned char)_mm256_extract_epi8(output_row0, 25);
    filt[N - 1][col + 26] = (unsigned char)_mm256_extract_epi8(output_row0, 26);
    filt[N - 1][col + 27] = (unsigned char)_mm256_extract_epi8(output_row0, 27);
    filt[N - 1][col + 28] = (unsigned char)_mm256_extract_epi8(output_row0, 28);

    break;
  case 30:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    filt[N - 1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    filt[N - 1][col + 17] = (unsigned char)_mm256_extract_epi8(output_row0, 17);
    filt[N - 1][col + 18] = (unsigned char)_mm256_extract_epi8(output_row0, 18);
    filt[N - 1][col + 19] = (unsigned char)_mm256_extract_epi8(output_row0, 19);
    filt[N - 1][col + 20] = (unsigned char)_mm256_extract_epi8(output_row0, 20);
    filt[N - 1][col + 21] = (unsigned char)_mm256_extract_epi8(output_row0, 21);
    filt[N - 1][col + 22] = (unsigned char)_mm256_extract_epi8(output_row0, 22);
    filt[N - 1][col + 23] = (unsigned char)_mm256_extract_epi8(output_row0, 23);
    filt[N - 1][col + 24] = (unsigned char)_mm256_extract_epi8(output_row0, 24);
    filt[N - 1][col + 25] = (unsigned char)_mm256_extract_epi8(output_row0, 25);
    filt[N - 1][col + 26] = (unsigned char)_mm256_extract_epi8(output_row0, 26);
    filt[N - 1][col + 27] = (unsigned char)_mm256_extract_epi8(output_row0, 27);
    filt[N - 1][col + 28] = (unsigned char)_mm256_extract_epi8(output_row0, 28);

    // the filt[row][col+29] is computed unvectorized

    newPixel = 0;
    newPixel += frame1[N - 1 - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[N - 1 - 1][M - 1] * filter[0][1];

    newPixel += frame1[N - 1][M - 1 - 1] * filter[1][0];
    newPixel += frame1[N - 1][M - 1] * filter[1][1];

    filt[N - 1][M - 1] = (unsigned char)(newPixel / divisor);

    break;

  case 31:
    _mm_storeu_si128((__m128i *)&filt[N - 1][col],
                     _mm256_extractf128_si256(
                         output_row0, 0)); // store low 128bit - 16pixels

    filt[N - 1][col + 16] = (unsigned char)_mm256_extract_epi8(output_row0, 16);
    filt[N - 1][col + 17] = (unsigned char)_mm256_extract_epi8(output_row0, 17);
    filt[N - 1][col + 18] = (unsigned char)_mm256_extract_epi8(output_row0, 18);
    filt[N - 1][col + 19] = (unsigned char)_mm256_extract_epi8(output_row0, 19);
    filt[N - 1][col + 20] = (unsigned char)_mm256_extract_epi8(output_row0, 20);
    filt[N - 1][col + 21] = (unsigned char)_mm256_extract_epi8(output_row0, 21);
    filt[N - 1][col + 22] = (unsigned char)_mm256_extract_epi8(output_row0, 22);
    filt[N - 1][col + 23] = (unsigned char)_mm256_extract_epi8(output_row0, 23);
    filt[N - 1][col + 24] = (unsigned char)_mm256_extract_epi8(output_row0, 24);
    filt[N - 1][col + 25] = (unsigned char)_mm256_extract_epi8(output_row0, 25);
    filt[N - 1][col + 26] = (unsigned char)_mm256_extract_epi8(output_row0, 26);
    filt[N - 1][col + 27] = (unsigned char)_mm256_extract_epi8(output_row0, 27);
    filt[N - 1][col + 28] = (unsigned char)_mm256_extract_epi8(output_row0, 28);

    // the filt[N-2][col+29/30] are computed unvectorized

    newPixel = 0;

    newPixel += frame1[N - 1 - 1][M - 2 - 1] * filter[0][0];
    newPixel += frame1[N - 1 - 1][M - 2] * filter[0][1];
    newPixel += frame1[N - 1 - 1][M - 2 + 1] * filter[0][2];

    newPixel += frame1[N - 1][M - 2 - 1] * filter[1][0];
    newPixel += frame1[N - 1][M - 2] * filter[1][1];
    newPixel += frame1[N - 1][M - 2 + 1] * filter[1][2];

    filt[N - 1][M - 2] = (unsigned char)(newPixel / divisor);

    newPixel = 0;

    newPixel += frame1[N - 1 - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[N - 1 - 1][M - 1] * filter[0][1];

    newPixel += frame1[N - 1][M - 1 - 1] * filter[1][0];
    newPixel += frame1[N - 1][M - 1] * filter[1][1];

    filt[N - 1][M - 1] = (unsigned char)(newPixel / divisor);

    break;

  default:
    printf("\nsomething went wrong");
    return -1;
  }

  return 0;
}

void Gaussian_Blur_optimized_3x3_16_reg_blocking(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned short int divisor,
    signed char **filter) {

  const signed char f00 = filter[0][0];
  const signed char f01 = filter[0][1];
  const signed char f02 = filter[0][2];

  const signed char f10 = filter[1][0];
  const signed char f11 = filter[1][1];
  const signed char f12 = filter[1][2];

  const __m256i c0 = _mm256_set_epi8(
      0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0,
      f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00);
  const __m256i c1 = _mm256_set_epi8(
      0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0,
      f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10);

  const __m256i c0_sh1 = _mm256_set_epi8(
      f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0,
      f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0);
  const __m256i c1_sh1 = _mm256_set_epi8(
      f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0,
      f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0);

  const __m256i c0_sh2 = _mm256_set_epi8(
      0, 0, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01,
      f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, 0);
  const __m256i c1_sh2 = _mm256_set_epi8(
      0, 0, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11,
      f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, 0);

  const __m256i c0_sh3 = _mm256_set_epi8(
      0, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00,
      0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, 0, 0);
  const __m256i c1_sh3 = _mm256_set_epi8(
      0, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10,
      0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, 0, 0);

  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i output_mask =
      _mm256_set_epi16(0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0, 65535);
  const __m256i output_mask_sh1 =
      _mm256_set_epi16(0, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0);

  const unsigned int REMINDER_ITERATIONS =
      (M - ((((M - 32) / 30) * 30) + 30)); // M-(last_col_value+30)
  // printf("\n%d",REMINDER_ITERATIONS);

  const unsigned int division_case = prepare_for_division(
      divisor); // determine which is the division case (A, B or C)
  const __m256i f = _mm256_load_si256(
      (__m256i *)&f_vector[0]); // initialize the division vector

  // printf("\n%d %d",division_case,b);

#pragma omp parallel
  {

    int row, col;
    register __m256i r0, r1, r2, r3, r4, m0, m1, m2, m3, m4;
    //__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd;
    ////these are for processing row #0 and #1 only.
    __m256i output_even, output_odd;
    //__m256i m0_prelude_row0,m0_prelude_row1;

    /*---------------------- Gaussian Blur ---------------------------------*/

#pragma omp for schedule(static)
    for (row = -1; row < N; row += 3) {

      if (row == -1) { // special case compute filt[0:1][:]

        for (col = 0; col <= M - 32; col += 30) {

          if (col == 0) {

            // load the 5 rows
            r0 = _mm256_load_si256((__m256i *)&frame1[0][0]);
            r1 = _mm256_load_si256((__m256i *)&frame1[1][0]);
            r2 = _mm256_load_si256((__m256i *)&frame1[2][0]);

            // START - extra code needed for prelude
            m0 = _mm256_slli_si256(r0,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m1 = _mm256_slli_si256(r1,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m2 = _mm256_slli_si256(r2,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning

            // preserve the element being lost because of the shift above
            r0 = _mm256_and_si256(r0, mask_prelude);
            r0 = _mm256_permute2f128_si256(r0, r0, 1);
            r0 = _mm256_srli_si256(r0, 15); // shift 15 elements
            r0 = _mm256_add_epi16(m0, r0);

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 15); // shift 15 elements
            r1 = _mm256_add_epi16(m1, r1);

            r2 = _mm256_and_si256(r2, mask_prelude);
            r2 = _mm256_permute2f128_si256(r2, r2, 1);
            r2 = _mm256_srli_si256(r2, 15); // shift 15 elements
            r2 = _mm256_add_epi16(m2, r2);

            // END - extra code needed for prelude
          } else {
            // load the 5 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col - 1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col - 1]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col - 1]);
          }

          // col iteration computes output pixels of    1,5,9,13,17,21,25,29
          //  col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
          //  col+2 iteration computes output pixels of 3,7,11,15,19,23,27
          //  col+3 iteration computes output pixels of 4,8,12,16,20,24,28
          // afterwards, col becomes 30 and repeat the above process

          // row==1
          // 1st col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0);
          m1 = _mm256_maddubs_epi16(r1, c1);
          m2 = _mm256_maddubs_epi16(r2, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh1);
          m1 = _mm256_maddubs_epi16(r1, c1_sh1);
          m2 = _mm256_maddubs_epi16(r2, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh2);
          m1 = _mm256_maddubs_epi16(r1, c1_sh2);
          m2 = _mm256_maddubs_epi16(r2, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh3);
          m1 = _mm256_maddubs_epi16(r1, c1_sh3);
          m2 = _mm256_maddubs_epi16(r2, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[1][col], output_even);

          // row==0
          // 1st col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c1);
          m1 = _mm256_maddubs_epi16(r1, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c1_sh1);
          m1 = _mm256_maddubs_epi16(r1, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c1_sh2);
          m1 = _mm256_maddubs_epi16(r1, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c1_sh3);
          m1 = _mm256_maddubs_epi16(r1, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[0][col], output_even);
        }
        loop_reminder_3x3_first_values(frame1, filt, M, N, col,
                                       REMINDER_ITERATIONS, division_case,
                                       divisor, filter, c0, c1, c0_sh1, c1_sh1,
                                       c0_sh2, c1_sh2, c0_sh3, c1_sh3, f);

      }

      else if (row == N - 2) { // special case compute filt[N-2:N-1][:]
        // printf("\nddd");
        for (col = 0; col <= M - 32; col += 30) {

          if (col == 0) {

            // load the 5 rows
            r0 = _mm256_load_si256((__m256i *)&frame1[N - 3][0]);
            r1 = _mm256_load_si256((__m256i *)&frame1[N - 2][0]);
            r2 = _mm256_load_si256((__m256i *)&frame1[N - 1][0]);

            // START - extra code needed for prelude
            m0 = _mm256_slli_si256(r0,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m1 = _mm256_slli_si256(r1,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m2 = _mm256_slli_si256(r2,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning

            // preserve the element being lost because of the shift above
            r0 = _mm256_and_si256(r0, mask_prelude);
            r0 = _mm256_permute2f128_si256(r0, r0, 1);
            r0 = _mm256_srli_si256(r0, 15); // shift 15 elements
            r0 = _mm256_add_epi16(m0, r0);

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 15); // shift 15 elements
            r1 = _mm256_add_epi16(m1, r1);

            r2 = _mm256_and_si256(r2, mask_prelude);
            r2 = _mm256_permute2f128_si256(r2, r2, 1);
            r2 = _mm256_srli_si256(r2, 15); // shift 15 elements
            r2 = _mm256_add_epi16(m2, r2);

            // END - extra code needed for prelude
          } else {
            // load the 5 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col - 1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 1]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 1]);
          }

          // col iteration computes output pixels of    1,5,9,13,17,21,25,29
          //  col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
          //  col+2 iteration computes output pixels of 3,7,11,15,19,23,27
          //  col+3 iteration computes output pixels of 4,8,12,16,20,24,28
          // afterwards, col becomes 30 and repeat the above process

          // row==N-2
          // 1st col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0);
          m1 = _mm256_maddubs_epi16(r1, c1);
          m2 = _mm256_maddubs_epi16(r2, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh1);
          m1 = _mm256_maddubs_epi16(r1, c1_sh1);
          m2 = _mm256_maddubs_epi16(r2, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh2);
          m1 = _mm256_maddubs_epi16(r1, c1_sh2);
          m2 = _mm256_maddubs_epi16(r2, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh3);
          m1 = _mm256_maddubs_epi16(r1, c1_sh3);
          m2 = _mm256_maddubs_epi16(r2, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 2][col], output_even);

          // row==N-1
          // 1st col iteration

          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0);
          m2 = _mm256_maddubs_epi16(r2, c1);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh1);
          m2 = _mm256_maddubs_epi16(r2, c1_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh2);
          m2 = _mm256_maddubs_epi16(r2, c1_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh3);
          m2 = _mm256_maddubs_epi16(r2, c1_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 1][col], output_even);
        }
        loop_reminder_3x3_last_values(frame1, filt, M, N, col,
                                      REMINDER_ITERATIONS, division_case,
                                      divisor, filter, c0, c1, c0_sh1, c1_sh1,
                                      c0_sh2, c1_sh2, c0_sh3, c1_sh3, f);

      }

      else if (row == N - 1) { // special case compute filt[N-1][:]

        for (col = 0; col <= M - 32; col += 30) {

          if (col == 0) {

            r1 = _mm256_load_si256((__m256i *)&frame1[N - 2][0]);
            r2 = _mm256_load_si256((__m256i *)&frame1[N - 1][0]);

            // START - extra code needed for prelude
            m1 = _mm256_slli_si256(r1,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m2 = _mm256_slli_si256(r2,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 15); // shift 15 elements
            r1 = _mm256_add_epi16(m1, r1);

            r2 = _mm256_and_si256(r2, mask_prelude);
            r2 = _mm256_permute2f128_si256(r2, r2, 1);
            r2 = _mm256_srli_si256(r2, 15); // shift 15 elements
            r2 = _mm256_add_epi16(m2, r2);

            // END - extra code needed for prelude
          } else {
            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 1]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 1]);
          }

          // row==N-1
          // 1st col iteration

          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0);
          m2 = _mm256_maddubs_epi16(r2, c1);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh1);
          m2 = _mm256_maddubs_epi16(r2, c1_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh2);
          m2 = _mm256_maddubs_epi16(r2, c1_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r1, c0_sh3);
          m2 = _mm256_maddubs_epi16(r2, c1_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 1][col], output_even);
        }
        loop_reminder_3x3_last_row_only(frame1, filt, M, N, col,
                                        REMINDER_ITERATIONS, division_case,
                                        divisor, filter, c0, c1, c0_sh1, c1_sh1,
                                        c0_sh2, c1_sh2, c0_sh3, c1_sh3, f);

      }

      else if (row == N - 3) { // special case compute filt[N-2:N-1][:] and
                               // filt[N-3][:]

        for (col = 0; col <= M - 32; col += 30) {

          if (col == 0) {

            // load the 5 rows
            r0 = _mm256_load_si256((__m256i *)&frame1[N - 4][0]);
            r1 = _mm256_load_si256((__m256i *)&frame1[N - 3][0]);
            r2 = _mm256_load_si256((__m256i *)&frame1[N - 2][0]);
            r3 = _mm256_load_si256((__m256i *)&frame1[N - 1][0]);

            // START - extra code needed for prelude
            m0 = _mm256_slli_si256(r0,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m1 = _mm256_slli_si256(r1,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m2 = _mm256_slli_si256(r2,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m3 = _mm256_slli_si256(r3,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning

            // preserve the element being lost because of the shift above
            r0 = _mm256_and_si256(r0, mask_prelude);
            r0 = _mm256_permute2f128_si256(r0, r0, 1);
            r0 = _mm256_srli_si256(r0, 15); // shift 15 elements
            r0 = _mm256_add_epi16(m0, r0);

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 15); // shift 15 elements
            r1 = _mm256_add_epi16(m1, r1);

            r2 = _mm256_and_si256(r2, mask_prelude);
            r2 = _mm256_permute2f128_si256(r2, r2, 1);
            r2 = _mm256_srli_si256(r2, 15); // shift 15 elements
            r2 = _mm256_add_epi16(m2, r2);

            r3 = _mm256_and_si256(r3, mask_prelude);
            r3 = _mm256_permute2f128_si256(r3, r3, 1);
            r3 = _mm256_srli_si256(r3, 15); // shift 15 elements
            r3 = _mm256_add_epi16(m3, r3);

            // END - extra code needed for prelude
          } else {
            r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col - 1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col - 1]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 1]);
            r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 1]);
          }

          // col iteration computes output pixels of    1,5,9,13,17,21,25,29
          //  col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
          //  col+2 iteration computes output pixels of 3,7,11,15,19,23,27
          //  col+3 iteration computes output pixels of 4,8,12,16,20,24,28
          // afterwards, col becomes 30 and repeat the above process

          // row==N-3
          // 1st col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0);
          m1 = _mm256_maddubs_epi16(r1, c1);
          m2 = _mm256_maddubs_epi16(r2, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh1);
          m1 = _mm256_maddubs_epi16(r1, c1_sh1);
          m2 = _mm256_maddubs_epi16(r2, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh2);
          m1 = _mm256_maddubs_epi16(r1, c1_sh2);
          m2 = _mm256_maddubs_epi16(r2, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh3);
          m1 = _mm256_maddubs_epi16(r1, c1_sh3);
          m2 = _mm256_maddubs_epi16(r2, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 3][col], output_even);

          // row==N-2
          // 1st col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0);
          m1 = _mm256_maddubs_epi16(r2, c1);
          m2 = _mm256_maddubs_epi16(r3, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh1);
          m1 = _mm256_maddubs_epi16(r2, c1_sh1);
          m2 = _mm256_maddubs_epi16(r3, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh2);
          m1 = _mm256_maddubs_epi16(r2, c1_sh2);
          m2 = _mm256_maddubs_epi16(r3, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh3);
          m1 = _mm256_maddubs_epi16(r2, c1_sh3);
          m2 = _mm256_maddubs_epi16(r3, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 2][col], output_even);

          // row==N-1
          // 1st col iteration

          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r2, c0);
          m2 = _mm256_maddubs_epi16(r3, c1);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r2, c0_sh1);
          m2 = _mm256_maddubs_epi16(r3, c1_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r2, c0_sh2);
          m2 = _mm256_maddubs_epi16(r3, c1_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m1 = _mm256_maddubs_epi16(r2, c0_sh3);
          m2 = _mm256_maddubs_epi16(r3, c1_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m1, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[N - 1][col], output_even);
        }
        loop_reminder_3x3_last_values(frame1, filt, M, N, col,
                                      REMINDER_ITERATIONS, division_case,
                                      divisor, filter, c0, c1, c0_sh1, c1_sh1,
                                      c0_sh2, c1_sh2, c0_sh3, c1_sh3, f);
        loop_reminder_3x3(frame1, filt, M, N, N - 3, col, REMINDER_ITERATIONS,
                          division_case, divisor, filter, c0, c1, c0_sh1,
                          c1_sh1, c0_sh2, c1_sh2, c0_sh3, c1_sh3, f);

      }

      else { // main loop

        for (col = 0; col <= M - 32; col += 30) {

          if (col == 0) {

            // load the 5 rows
            r0 = _mm256_load_si256((__m256i *)&frame1[row - 1][0]);
            r1 = _mm256_load_si256((__m256i *)&frame1[row][0]);
            r2 = _mm256_load_si256((__m256i *)&frame1[row + 1][0]);
            r3 = _mm256_load_si256((__m256i *)&frame1[row + 2][0]);
            r4 = _mm256_load_si256((__m256i *)&frame1[row + 3][0]);

            // START - extra code needed for prelude
            m0 = _mm256_slli_si256(r0,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m1 = _mm256_slli_si256(r1,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m2 = _mm256_slli_si256(r2,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m3 = _mm256_slli_si256(r3,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m4 = _mm256_slli_si256(r4,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning

            // preserve the element being lost because of the shift above
            r0 = _mm256_and_si256(r0, mask_prelude);
            r0 = _mm256_permute2f128_si256(r0, r0, 1);
            r0 = _mm256_srli_si256(r0, 15); // shift 15 elements
            r0 = _mm256_add_epi16(m0, r0);

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 15); // shift 15 elements
            r1 = _mm256_add_epi16(m1, r1);

            r2 = _mm256_and_si256(r2, mask_prelude);
            r2 = _mm256_permute2f128_si256(r2, r2, 1);
            r2 = _mm256_srli_si256(r2, 15); // shift 15 elements
            r2 = _mm256_add_epi16(m2, r2);

            r3 = _mm256_and_si256(r3, mask_prelude);
            r3 = _mm256_permute2f128_si256(r3, r3, 1);
            r3 = _mm256_srli_si256(r3, 15); // shift 15 elements
            r3 = _mm256_add_epi16(m3, r3);

            r4 = _mm256_and_si256(r4, mask_prelude);
            r4 = _mm256_permute2f128_si256(r4, r4, 1);
            r4 = _mm256_srli_si256(r4, 15); // shift 15 elements
            r4 = _mm256_add_epi16(m4, r4);

            // END - extra code needed for prelude
          } else {
            // load the 5 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col - 1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[row][col - 1]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col - 1]);
            r3 = _mm256_loadu_si256((__m256i *)&frame1[row + 2][col - 1]);
            r4 = _mm256_loadu_si256((__m256i *)&frame1[row + 3][col - 1]);
          }

          // col iteration computes output pixels of    1,5,9,13,17,21,25,29
          //  col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
          //  col+2 iteration computes output pixels of 3,7,11,15,19,23,27
          //  col+3 iteration computes output pixels of 4,8,12,16,20,24,28
          // afterwards, col becomes 30 and repeat the above process

          // first row
          // 1st col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0);
          m1 = _mm256_maddubs_epi16(r1, c1);
          m2 = _mm256_maddubs_epi16(r2, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh1);
          m1 = _mm256_maddubs_epi16(r1, c1_sh1);
          m2 = _mm256_maddubs_epi16(r2, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh2);
          m1 = _mm256_maddubs_epi16(r1, c1_sh2);
          m2 = _mm256_maddubs_epi16(r2, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, c0_sh3);
          m1 = _mm256_maddubs_epi16(r1, c1_sh3);
          m2 = _mm256_maddubs_epi16(r2, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[row][col], output_even);

          // 2nd row
          // 1st col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0);
          m1 = _mm256_maddubs_epi16(r2, c1);
          m2 = _mm256_maddubs_epi16(r3, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh1);
          m1 = _mm256_maddubs_epi16(r2, c1_sh1);
          m2 = _mm256_maddubs_epi16(r3, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh2);
          m1 = _mm256_maddubs_epi16(r2, c1_sh2);
          m2 = _mm256_maddubs_epi16(r3, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r1, c0_sh3);
          m1 = _mm256_maddubs_epi16(r2, c1_sh3);
          m2 = _mm256_maddubs_epi16(r3, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[row + 1][col], output_even);

          // 3rd row
          // 1st col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r2, c0);
          m1 = _mm256_maddubs_epi16(r3, c1);
          m2 = _mm256_maddubs_epi16(r4, c0);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others. later on they will be stored into 0,4,8,12,16,20,24,28
          // positions (8bit register).
          output_even = _mm256_and_si256(m2, output_mask);

          // 2nd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r2, c0_sh1);
          m1 = _mm256_maddubs_epi16(r3, c1_sh1);
          m2 = _mm256_maddubs_epi16(r4, c0_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          // m2 has 16 16bit values now. the results I need are in positions
          // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
          // others.
          output_odd = _mm256_and_si256(m2, output_mask);

          // 3rd col iteration
          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r2, c0_sh2);
          m1 = _mm256_maddubs_epi16(r3, c1_sh2);
          m2 = _mm256_maddubs_epi16(r4, c0_sh2);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_even = _mm256_add_epi16(output_even, m2);

          // 4th col iteration

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r2, c0_sh3);
          m1 = _mm256_maddubs_epi16(r3, c1_sh3);
          m2 = _mm256_maddubs_epi16(r4, c0_sh3);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);

          // hozizontal additions
          m1 = _mm256_srli_si256(m0, 2);
          m2 = _mm256_add_epi16(m1, m0);

          m4 = _mm256_and_si256(m0, mask3);
          m4 = _mm256_permute2f128_si256(m4, m4, 1);
          m4 = _mm256_slli_si256(
              m4, 14); // shift 7 short int positions or 14 char positions
          m2 = _mm256_add_epi16(m2, m4);

          // m2 has 16 16bit values now. the results I need are in positions
          // 1,3,5,7,9,11,13. keep only those, discard others
          m2 = _mm256_and_si256(m2, output_mask_sh1);
          output_odd = _mm256_add_epi16(output_odd, m2);

          // now division follows
          output_even = division(division_case, output_even, f);
          output_odd = division(division_case, output_odd, f);

          // shift odd 1 position and add to even
          output_odd = _mm256_slli_si256(output_odd, 1);
          output_even = _mm256_add_epi8(output_even, output_odd);

          _mm256_storeu_si256((__m256i *)&filt[row + 2][col], output_even);
        }
        loop_reminder_3x3(frame1, filt, M, N, row, col, REMINDER_ITERATIONS,
                          division_case, divisor, filter, c0, c1, c0_sh1,
                          c1_sh1, c0_sh2, c1_sh2, c0_sh3, c1_sh3, f);
        loop_reminder_3x3(frame1, filt, M, N, row + 1, col, REMINDER_ITERATIONS,
                          division_case, divisor, filter, c0, c1, c0_sh1,
                          c1_sh1, c0_sh2, c1_sh2, c0_sh3, c1_sh3, f);
        loop_reminder_3x3(frame1, filt, M, N, row + 2, col, REMINDER_ITERATIONS,
                          division_case, divisor, filter, c0, c1, c0_sh1,
                          c1_sh1, c0_sh2, c1_sh2, c0_sh3, c1_sh3, f);
      }
    }

  } // end of parallel
}

__m256i fill_zeros(__m256i r0, const __m256i mask_prelude) {

  __m256i m0 =
      _mm256_slli_si256(r0, 4); // shift 4 elements left - equivalent to filling
                                // with four zeros in the beginning

  r0 = _mm256_and_si256(r0, mask_prelude);
  r0 = _mm256_permute2f128_si256(r0, r0, 1);
  r0 = _mm256_srli_si256(r0, 12); // shift by 12 elements
  return _mm256_add_epi16(m0, r0);
}

__m256i fill_3zeros(__m256i r0, const __m256i mask_prelude) {

  __m256i m0 =
      _mm256_slli_si256(r0, 3); // shift 4 elements left - equivalent to filling
                                // with four zeros in the beginning

  r0 = _mm256_and_si256(r0, mask_prelude);
  r0 = _mm256_permute2f128_si256(r0, r0, 1);
  r0 = _mm256_srli_si256(r0, 13); // shift by 12 elements
  return _mm256_add_epi16(m0, r0);
}

__m256i fill_2zeros(__m256i r0, const __m256i mask_prelude) {

  __m256i m0 =
      _mm256_slli_si256(r0, 2); // shift 4 elements left - equivalent to filling
                                // with four zeros in the beginning

  r0 = _mm256_and_si256(r0, mask_prelude);
  r0 = _mm256_permute2f128_si256(r0, r0, 1);
  r0 = _mm256_srli_si256(r0, 14);
  return _mm256_add_epi16(m0, r0);
}

__m256i fill_1zeros(__m256i r0, const __m256i mask_prelude) {

  __m256i m0 =
      _mm256_slli_si256(r0, 1); // shift 4 elements left - equivalent to filling
                                // with four zeros in the beginning

  r0 = _mm256_and_si256(r0, mask_prelude);
  r0 = _mm256_permute2f128_si256(r0, r0, 1);
  r0 = _mm256_srli_si256(r0, 15);
  return _mm256_add_epi16(m0, r0);
}

// this function is computing the row=3 and row=N-4 only
// for row=3 input prelude_7x7_Ymask_3(0,...
// for row=N-4 input prelude_7x7_Ymask_3(N-8,...
void prelude_9x9_16_Ymask_3(const int row, const unsigned int M,
                            unsigned char **frame1, unsigned char *temp,
                            const signed char mask_vector_y[][32],
                            const unsigned int division_case, const __m256i f,
                            const __m256i mask_prelude) {

  __m256i r0, r1, r2, r3, r4, r5, r6, r7, m0, m1, m2, m3, m4, m5, m6, m7, even,
      odd;
  //__m256i ones=_mm256_set1_epi16(1);
  __m256i cy0, cy1, cy2, cy3, cy4, cy5, cy6, cy7, cy0_sh1, cy1_sh1, cy2_sh1,
      cy3_sh1, cy4_sh1, cy5_sh1, cy6_sh1, cy7_sh1;

  if (row != 0) { // if row=N-6
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
    cy7 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
    cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
    cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
    cy7_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);

  } else {
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
    cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
    cy7 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
    cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
    cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);
    cy7_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[17]);
  }

  for (unsigned int col = 0; col <= M - 32; col += 32) {

    //--------------------y filter begins--------------------
    // load the 9 rows
    r0 = _mm256_load_si256((__m256i *)&frame1[row][col]);
    r1 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
    r2 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);
    r3 = _mm256_load_si256((__m256i *)&frame1[row + 3][col]);
    r4 = _mm256_load_si256((__m256i *)&frame1[row + 4][col]);
    r5 = _mm256_load_si256((__m256i *)&frame1[row + 5][col]);
    r6 = _mm256_load_si256((__m256i *)&frame1[row + 6][col]);
    r7 = _mm256_load_si256((__m256i *)&frame1[row + 7][col]);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);
    m4 = _mm256_maddubs_epi16(r4, cy4);
    m5 = _mm256_maddubs_epi16(r5, cy5);
    m6 = _mm256_maddubs_epi16(r6, cy6);
    m7 = _mm256_maddubs_epi16(r7, cy7);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);
    m0 = _mm256_add_epi16(m0, m6);
    m0 = _mm256_add_epi16(m0, m7);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
    m4 = _mm256_maddubs_epi16(r4, cy4_sh1);
    m5 = _mm256_maddubs_epi16(r5, cy5_sh1);
    m6 = _mm256_maddubs_epi16(r6, cy6_sh1);
    m7 = _mm256_maddubs_epi16(r7, cy7_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);
    m0 = _mm256_add_epi16(m0, m6);
    m0 = _mm256_add_epi16(m0, m7);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even
    _mm256_store_si256((__m256i *)&temp[col], r0);
  }
  // y filter ends - r0 has now the data to be processed by x filter
}

// this function is computing the row=2 and row=N-3 only
// for row=3 input prelude_7x7_Ymask_3(0,...
// for row=N-3 input prelude_7x7_Ymask_3(N-7,...
void prelude_9x9_16_Ymask_2(const int row, const unsigned int M,
                            unsigned char **frame1, unsigned char *temp,
                            const signed char mask_vector_y[][32],
                            const unsigned int division_case, const __m256i f,
                            const __m256i mask_prelude) {

  __m256i r0, r1, r2, r3, r4, r5, r6, m0, m1, m2, m3, m4, m5, m6, even, odd;
  //__m256i ones=_mm256_set1_epi16(1);
  __m256i cy0, cy1, cy2, cy3, cy4, cy5, cy6, cy0_sh1, cy1_sh1, cy2_sh1, cy3_sh1,
      cy4_sh1, cy5_sh1, cy6_sh1;

  if (row != 0) { // if row=N-7
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
    cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
    cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);

  } else {
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
    cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
    cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
    cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);
    cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[17]);
  }

  for (unsigned int col = 0; col <= M - 32; col += 32) {

    //--------------------y filter begins--------------------
    // load the 9 rows
    r0 = _mm256_load_si256((__m256i *)&frame1[row][col]);
    r1 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
    r2 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);
    r3 = _mm256_load_si256((__m256i *)&frame1[row + 3][col]);
    r4 = _mm256_load_si256((__m256i *)&frame1[row + 4][col]);
    r5 = _mm256_load_si256((__m256i *)&frame1[row + 5][col]);
    r6 = _mm256_load_si256((__m256i *)&frame1[row + 6][col]);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);
    m4 = _mm256_maddubs_epi16(r4, cy4);
    m5 = _mm256_maddubs_epi16(r5, cy5);
    m6 = _mm256_maddubs_epi16(r6, cy6);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);
    m0 = _mm256_add_epi16(m0, m6);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
    m4 = _mm256_maddubs_epi16(r4, cy4_sh1);
    m5 = _mm256_maddubs_epi16(r5, cy5_sh1);
    m6 = _mm256_maddubs_epi16(r6, cy6_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);
    m0 = _mm256_add_epi16(m0, m6);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even
    _mm256_store_si256((__m256i *)&temp[col], r0);
  }
  // y filter ends - r0 has now the data to be processed by x filter
}

// this function is computing the row=1 and row=N-2 only
// for row=3 input prelude_7x7_Ymask_3(0,...
// for row=N-3 input prelude_7x7_Ymask_3(N-6,...
void prelude_9x9_16_Ymask_1(const int row, const unsigned int M,
                            unsigned char **frame1, unsigned char *temp,
                            const signed char mask_vector_y[][32],
                            const unsigned int division_case, const __m256i f,
                            const __m256i mask_prelude) {

  __m256i r0, r1, r2, r3, r4, r5, m0, m1, m2, m3, m4, m5, even, odd;
  //__m256i ones=_mm256_set1_epi16(1);
  __m256i cy0, cy1, cy2, cy3, cy4, cy5, cy0_sh1, cy1_sh1, cy2_sh1, cy3_sh1,
      cy4_sh1, cy5_sh1;

  if (row != 0) { // if row=N-6
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
    cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);

  } else {
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
    cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);
    cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[17]);
  }

  for (unsigned int col = 0; col <= M - 32; col += 32) {

    //--------------------y filter begins--------------------
    // load the 9 rows
    r0 = _mm256_load_si256((__m256i *)&frame1[row][col]);
    r1 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
    r2 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);
    r3 = _mm256_load_si256((__m256i *)&frame1[row + 3][col]);
    r4 = _mm256_load_si256((__m256i *)&frame1[row + 4][col]);
    r5 = _mm256_load_si256((__m256i *)&frame1[row + 5][col]);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);
    m4 = _mm256_maddubs_epi16(r4, cy4);
    m5 = _mm256_maddubs_epi16(r5, cy5);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
    m4 = _mm256_maddubs_epi16(r4, cy4_sh1);
    m5 = _mm256_maddubs_epi16(r5, cy5_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even
    _mm256_store_si256((__m256i *)&temp[col], r0);
  }
  // y filter ends - r0 has now the data to be processed by x filter
}

// this function is computing the row=0 and row=N-1 only
// for row=3 input prelude_7x7_Ymask_3(0,...
// for row=N-3 input prelude_7x7_Ymask_3(N-5,...
void prelude_9x9_16_Ymask_0(const int row, const unsigned int M,
                            unsigned char **frame1, unsigned char *temp,
                            const signed char mask_vector_y[][32],
                            const unsigned int division_case, const __m256i f,
                            const __m256i mask_prelude) {

  __m256i r0, r1, r2, r3, r4, m0, m1, m2, m3, m4, even, odd;
  //__m256i ones=_mm256_set1_epi16(1);
  __m256i cy0, cy1, cy2, cy3, cy4, cy0_sh1, cy1_sh1, cy2_sh1, cy3_sh1, cy4_sh1;

  if (row != 0) { // if row=N-6
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);

  } else {
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[17]);
  }

  for (unsigned int col = 0; col <= M - 32; col += 32) {

    //--------------------y filter begins--------------------
    // load the 9 rows
    r0 = _mm256_load_si256((__m256i *)&frame1[row][col]);
    r1 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
    r2 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);
    r3 = _mm256_load_si256((__m256i *)&frame1[row + 3][col]);
    r4 = _mm256_load_si256((__m256i *)&frame1[row + 4][col]);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);
    m4 = _mm256_maddubs_epi16(r4, cy4);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
    m4 = _mm256_maddubs_epi16(r4, cy4_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even
    _mm256_store_si256((__m256i *)&temp[col], r0);
  }
  // y filter ends - r0 has now the data to be processed by x filter
}

void prelude_9x9_16_Xmask(unsigned char **frame1, unsigned char **filt,
                          unsigned char *temp, const unsigned int N,
                          const unsigned int M, const int row,
                          const signed char mask_vector_x[][32],
                          const unsigned int division_case, const __m256i f,
                          const unsigned int REMINDER_ITERATIONS_X,
                          const unsigned int REMINDER_ITERATIONS_Y,
                          const signed char mask_vector_y[][32],
                          const unsigned short int divisor_xy,
                          signed char *kernel_x) {

  __m256i r0, m0, m1, m2, m3, m4, even, odd;

  const __m256i cx0 = _mm256_load_si256((__m256i *)&mask_vector_x[0]);
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask3_2 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i mask_prelude =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude_3 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude_2 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude_1 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i mask_16_1 = _mm256_set_epi16(0, 0, 0, 0, 0, 0xffff, 0, 0, 0, 0,
                                             0xffff, 0, 0, 0, 0, 0xffff);
  const __m256i mask_new_1 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0xffff, 0, 0, 0, 0, 0);
  const __m256i mask_16_5 = _mm256_set_epi16(0, 0xffff, 0, 0, 0, 0, 0xffff, 0,
                                             0, 0, 0, 0xffff, 0, 0, 0, 0);
  unsigned int col;

  for (col = 0; col <= M - 32; col += 30) {

    if (col == 0) {

      // 1st col iteration
      r0 = _mm256_load_si256((__m256i *)&temp[0]);
      r0 = fill_zeros(r0, mask_prelude);
      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      even = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others

      // 2nd col iteration
      r0 = _mm256_load_si256((__m256i *)&temp[0]);
      r0 = fill_3zeros(r0, mask_prelude_3);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      odd = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                             // 0,5,10 and discard the others

      // 3rd col iteration
      r0 = _mm256_load_si256((__m256i *)&temp[0]);
      r0 = fill_2zeros(r0, mask_prelude_2);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others
      m0 = _mm256_slli_si256(m0, 2);
      even = _mm256_add_epi16(m0, even);

      // 4th col iteration
      r0 = _mm256_load_si256((__m256i *)&temp[0]);
      r0 = fill_1zeros(r0, mask_prelude_1);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others
      m0 = _mm256_slli_si256(m0, 2);
      odd = _mm256_add_epi16(m0, odd);

      // 5th col iteration
      r0 = _mm256_load_si256((__m256i *)&temp[0]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others
      m0 = _mm256_slli_si256(m0, 4);
      even = _mm256_add_epi16(m0, even);

      // 6th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[1]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others
      m0 = _mm256_slli_si256(m0, 4);
      odd = _mm256_add_epi16(m0, odd);

      // 7th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[2]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others

      // shift the result 3 positions
      m1 = _mm256_slli_si256(m0, 6);
      m2 = _mm256_and_si256(m0, mask_new_1);
      m2 = _mm256_permute2f128_si256(m2, m2, 1);
      m2 = _mm256_srli_si256(m2, 10);
      m0 = _mm256_add_epi16(m2, m1);

      even = _mm256_add_epi16(m0, even);

      // 8th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[3]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others

      // shift the result 3 positions
      m1 = _mm256_slli_si256(m0, 6);
      m2 = _mm256_and_si256(m0, mask_new_1);
      m2 = _mm256_permute2f128_si256(m2, m2, 1);
      m2 = _mm256_srli_si256(m2, 10);
      m0 = _mm256_add_epi16(m2, m1);

      odd = _mm256_add_epi16(m0, odd);

      // 9th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[4]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others

      // shift the result 4 positions
      m1 = _mm256_slli_si256(m0, 8);
      m2 = _mm256_and_si256(m0, mask_new_1);
      m2 = _mm256_permute2f128_si256(m2, m2, 1);
      m2 = _mm256_srli_si256(m2, 8);
      m0 = _mm256_add_epi16(m2, m1);

      even = _mm256_add_epi16(m0, even);

      // 10th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[5]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others

      // shift the result 4 positions
      m1 = _mm256_slli_si256(m0, 8);
      m2 = _mm256_and_si256(m0, mask_new_1);
      m2 = _mm256_permute2f128_si256(m2, m2, 1);
      m2 = _mm256_srli_si256(m2, 8);
      m0 = _mm256_add_epi16(m2, m1);

      odd = _mm256_add_epi16(m0, odd);

      // division
      even = division(division_case, even, f);
      odd = division(division_case, odd, f);

      // blend even , odd into one register
      odd = _mm256_slli_si256(odd, 1);
      even = _mm256_add_epi16(even, odd);

      _mm256_storeu_si256((__m256i *)&filt[row][0], even);

    } else {
      // col iteration computes output pixels of   0,10,20
      // col+1 iteration computes output pixels of 1,11,21
      // col+2 iteration computes output pixels of 2,12,22
      // col+3 iteration computes output pixels of 3,13,23
      // col+4 iteration computes output pixels of 4,14,24
      // col+5 iteration computes output pixels of 5,15,25
      // col+6 iteration computes output pixels of 6,16,26
      // col+7 iteration computes output pixels of 7,17,27
      // col+8 iteration computes output pixels of 8,18,28
      // col+9 iteration computes output pixels of 9,19,29
      // afterwards, col becomes 30 and repeat the above process

      // 1st col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col - 4]);
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      even = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others

      // 2nd col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col - 3]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      odd = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                             // 0,5,10 and discard the others

      // 3rd col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col - 2]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others
      m0 = _mm256_slli_si256(m0, 2);
      even = _mm256_add_epi16(m0, even);

      // 4th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col - 1]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others
      m0 = _mm256_slli_si256(m0, 2);
      odd = _mm256_add_epi16(m0, odd);

      // 5th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others
      m0 = _mm256_slli_si256(m0, 4);
      even = _mm256_add_epi16(m0, even);

      // 6th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others
      m0 = _mm256_slli_si256(m0, 4);
      odd = _mm256_add_epi16(m0, odd);

      // 7th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others

      // shift the result 3 positions
      m1 = _mm256_slli_si256(m0, 6);
      m2 = _mm256_and_si256(m0, mask_new_1);
      m2 = _mm256_permute2f128_si256(m2, m2, 1);
      m2 = _mm256_srli_si256(m2, 10);
      m0 = _mm256_add_epi16(m2, m1);

      even = _mm256_add_epi16(m0, even);

      // 8th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others

      // shift the result 3 positions
      m1 = _mm256_slli_si256(m0, 6);
      m2 = _mm256_and_si256(m0, mask_new_1);
      m2 = _mm256_permute2f128_si256(m2, m2, 1);
      m2 = _mm256_srli_si256(m2, 10);
      m0 = _mm256_add_epi16(m2, m1);

      odd = _mm256_add_epi16(m0, odd);

      // 9th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 4]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others

      // shift the result 4 positions
      m1 = _mm256_slli_si256(m0, 8);
      m2 = _mm256_and_si256(m0, mask_new_1);
      m2 = _mm256_permute2f128_si256(m2, m2, 1);
      m2 = _mm256_srli_si256(m2, 8);
      m0 = _mm256_add_epi16(m2, m1);

      even = _mm256_add_epi16(m0, even);

      // 10th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 5]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add (complex shift is needed)
      m3 = _mm256_srli_si256(m0, 2);
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 14);
      m3 = _mm256_add_epi16(m3, m4);
      m3 = _mm256_add_epi16(m0, m3); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m3, 4);
      m3 = _mm256_add_epi16(m3, m1); // add

      // third horizontal add
      m0 = _mm256_and_si256(m0, mask_16_5);
      m2 = _mm256_srli_si256(m0, 8);
      m4 = _mm256_and_si256(m0, mask3_2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(m4, 8);
      m2 = _mm256_add_epi16(m2, m4);

      m0 = _mm256_add_epi16(m2, m3); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only 0,5,10
                                            // and discard the others

      // shift the result 4 positions
      m1 = _mm256_slli_si256(m0, 8);
      m2 = _mm256_and_si256(m0, mask_new_1);
      m2 = _mm256_permute2f128_si256(m2, m2, 1);
      m2 = _mm256_srli_si256(m2, 8);
      m0 = _mm256_add_epi16(m2, m1);

      odd = _mm256_add_epi16(m0, odd);

      // division
      even = division(division_case, even, f);
      odd = division(division_case, odd, f);

      // blend even , odd into one register
      odd = _mm256_slli_si256(odd, 1);
      even = _mm256_add_epi16(even, odd);

      _mm256_storeu_si256((__m256i *)&filt[row][col], even);
    }
  }

  loop_reminder_9x9_16_blur_X(frame1, filt, temp, N, M, row, col,
                              REMINDER_ITERATIONS_X, division_case,
                              mask_vector_x, f, divisor_xy, kernel_x);
}

void loop_reminder_9x9_16_blur_Y(unsigned char **frame1, unsigned char **filt,
                                 unsigned char *temp, const unsigned int N,
                                 const unsigned int M, const unsigned int row,
                                 const unsigned int REMINDER_ITERATIONS_Y,
                                 const unsigned int division_case,
                                 const signed char mask_vector_y[][32],
                                 const __m256i f,
                                 const unsigned short int divisor_xy) {

  __m256i r0, r1, r2, r3, r4, r5, r6, r7, r8, m0, m1, m2, m3, m4, m5, m6, m7,
      m8, even, odd, reminder_mask;

  if (REMINDER_ITERATIONS_Y == 0) { // no need for computing the y mask
    _mm256_store_si256((__m256i *)&temp[M], _mm256_setzero_si256());
    for (unsigned int g = M + 32; g < M + 32 + 6; g++)
      temp[g] = 0;
  } else {
    __m256i cy0, cy1, cy2, cy3, cy4, cy5, cy6, cy7, cy8, cy0_sh1, cy1_sh1,
        cy2_sh1, cy3_sh1, cy4_sh1, cy5_sh1, cy6_sh1, cy7_sh1, cy8_sh1;

    unsigned int col2 =
        M - REMINDER_ITERATIONS_Y; // this is the col index to load the elements
                                   // for the y mask

    if ((row > 3) && (row < N - 4)) { // main case

      r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 4][col2]);
      r1 = _mm256_loadu_si256((__m256i *)&frame1[row - 3][col2]);
      r2 = _mm256_loadu_si256((__m256i *)&frame1[row - 2][col2]);
      r3 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col2]);
      r4 = _mm256_loadu_si256((__m256i *)&frame1[row][col2]);
      r5 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col2]);
      r6 = _mm256_loadu_si256((__m256i *)&frame1[row + 2][col2]);
      r7 = _mm256_loadu_si256((__m256i *)&frame1[row + 3][col2]);
      r8 = _mm256_loadu_si256((__m256i *)&frame1[row + 4][col2]);

      cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
      cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
      cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
      cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
      cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
      cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
      cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
      cy7 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
      cy8 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);

      cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
      cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
      cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
      cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
      cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
      cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
      cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
      cy7_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);
      cy8_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[17]);

    } else {
      if (row == 0) {
        r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col2]);
        r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[4][col2]);
        r5 = _mm256_setzero_si256();
        r6 = _mm256_setzero_si256();
        r7 = _mm256_setzero_si256();
        r8 = _mm256_setzero_si256();

        cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
        cy5 = _mm256_setzero_si256();
        cy6 = _mm256_setzero_si256();
        cy7 = _mm256_setzero_si256();
        cy8 = _mm256_setzero_si256();

        cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[17]);
        cy5_sh1 = _mm256_setzero_si256();
        cy6_sh1 = _mm256_setzero_si256();
        cy7_sh1 = _mm256_setzero_si256();
        cy8_sh1 = _mm256_setzero_si256();

      } else if (row == 1) {
        r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col2]);
        r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[4][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[5][col2]);
        r6 = _mm256_setzero_si256();
        r7 = _mm256_setzero_si256();
        r8 = _mm256_setzero_si256();

        cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
        cy6 = _mm256_setzero_si256();
        cy7 = _mm256_setzero_si256();
        cy8 = _mm256_setzero_si256();

        cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[17]);
        cy6_sh1 = _mm256_setzero_si256();
        cy7_sh1 = _mm256_setzero_si256();
        cy8_sh1 = _mm256_setzero_si256();
      } else if (row == 2) {
        r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col2]);
        r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[4][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[5][col2]);
        r6 = _mm256_loadu_si256((__m256i *)&frame1[6][col2]);
        r7 = _mm256_setzero_si256();
        r8 = _mm256_setzero_si256();

        cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
        cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
        cy7 = _mm256_setzero_si256();
        cy8 = _mm256_setzero_si256();

        cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);
        cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[17]);
        cy7_sh1 = _mm256_setzero_si256();
        cy8_sh1 = _mm256_setzero_si256();
      } else if (row == 3) {
        r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col2]);
        r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[4][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[5][col2]);
        r6 = _mm256_loadu_si256((__m256i *)&frame1[6][col2]);
        r7 = _mm256_loadu_si256((__m256i *)&frame1[7][col2]);
        r8 = _mm256_setzero_si256();

        cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
        cy7 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
        cy8 = _mm256_setzero_si256();

        cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
        cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);
        cy7_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[17]);
        cy8_sh1 = _mm256_setzero_si256();
      } else if (row == N - 4) {
        r0 = _mm256_setzero_si256();
        r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 8][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 7][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 6][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 5][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col2]);
        r6 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col2]);
        r7 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col2]);
        r8 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col2]);

        cy0 = _mm256_setzero_si256();
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy7 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy8 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);

        cy0_sh1 = _mm256_setzero_si256();
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
        cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
        cy7_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
        cy8_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);

      } else if (row == N - 3) {
        r0 = _mm256_setzero_si256();
        r1 = _mm256_setzero_si256();
        r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 7][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 6][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 5][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col2]);
        r6 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col2]);
        r7 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col2]);
        r8 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col2]);

        cy0 = _mm256_setzero_si256();
        cy1 = _mm256_setzero_si256();
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy7 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy8 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);

        cy0_sh1 = _mm256_setzero_si256();
        cy1_sh1 = _mm256_setzero_si256();
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
        cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
        cy7_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
        cy8_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
      } else if (row == N - 2) {
        r0 = _mm256_setzero_si256();
        r1 = _mm256_setzero_si256();
        r2 = _mm256_setzero_si256();
        r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 6][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 5][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col2]);
        r6 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col2]);
        r7 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col2]);
        r8 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col2]);

        cy0 = _mm256_setzero_si256();
        cy1 = _mm256_setzero_si256();
        cy2 = _mm256_setzero_si256();
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy7 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy8 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);

        cy0_sh1 = _mm256_setzero_si256();
        cy1_sh1 = _mm256_setzero_si256();
        cy2_sh1 = _mm256_setzero_si256();
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
        cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
        cy7_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
        cy8_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
      } else if (row == N - 1) {
        r0 = _mm256_setzero_si256();
        r1 = _mm256_setzero_si256();
        r2 = _mm256_setzero_si256();
        r3 = _mm256_setzero_si256();
        r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 5][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col2]);
        r6 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col2]);
        r7 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col2]);
        r8 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col2]);

        cy0 = _mm256_setzero_si256();
        cy1 = _mm256_setzero_si256();
        cy2 = _mm256_setzero_si256();
        cy3 = _mm256_setzero_si256();
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy7 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy8 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);

        cy0_sh1 = _mm256_setzero_si256();
        cy1_sh1 = _mm256_setzero_si256();
        cy2_sh1 = _mm256_setzero_si256();
        cy3_sh1 = _mm256_setzero_si256();
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
        cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
        cy7_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
        cy8_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
      } else {
        printf("\nsomething went wrong");
        exit(EXIT_FAILURE);
      }
    }

    /*
     * The above load operations load outside of the array bounds; these
     * elements are filled with zeros just after
     */
    reminder_mask = _mm256_load_si256(
        (__m256i *)&reminder_mask_9x9[REMINDER_ITERATIONS_Y - 1][0]);

    // AND r0-r4 with reminder_mask
    r0 = _mm256_and_si256(r0, reminder_mask);
    r1 = _mm256_and_si256(r1, reminder_mask);
    r2 = _mm256_and_si256(r2, reminder_mask);
    r3 = _mm256_and_si256(r3, reminder_mask);
    r4 = _mm256_and_si256(r4, reminder_mask);
    r5 = _mm256_and_si256(r5, reminder_mask);
    r6 = _mm256_and_si256(r6, reminder_mask);
    r7 = _mm256_and_si256(r7, reminder_mask);
    r8 = _mm256_and_si256(r8, reminder_mask);

    //--------------------y filter begins--------------------

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);
    m4 = _mm256_maddubs_epi16(r4, cy4);
    m5 = _mm256_maddubs_epi16(r5, cy5);
    m6 = _mm256_maddubs_epi16(r6, cy6);
    m7 = _mm256_maddubs_epi16(r7, cy7);
    m8 = _mm256_maddubs_epi16(r8, cy8);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);
    m0 = _mm256_add_epi16(m0, m6);
    m0 = _mm256_add_epi16(m0, m7);
    m0 = _mm256_add_epi16(m0, m8);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
    m4 = _mm256_maddubs_epi16(r4, cy4_sh1);
    m5 = _mm256_maddubs_epi16(r5, cy5_sh1);
    m6 = _mm256_maddubs_epi16(r6, cy6_sh1);
    m7 = _mm256_maddubs_epi16(r7, cy7_sh1);
    m8 = _mm256_maddubs_epi16(r8, cy8_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);
    m0 = _mm256_add_epi16(m0, m6);
    m0 = _mm256_add_epi16(m0, m7);
    m0 = _mm256_add_epi16(m0, m8);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even

    _mm256_storeu_si256((__m256i *)&temp[M - REMINDER_ITERATIONS_Y], r0);

    for (int gg = M; gg < M + 32 + 6; gg++)
      temp[gg] = 0;
    // y filter ends - r0 has now the data to be processed by x filter
  }
}

void loop_reminder_9x9_16_blur_X(
    unsigned char **frame1, unsigned char **filt, unsigned char *temp,
    const unsigned int N, const unsigned int M, const unsigned int row,
    const unsigned int col, const unsigned int REMINDER_ITERATIONS_X,
    const unsigned int division_case, const signed char mask_vector_x[][32],
    const __m256i f, const unsigned short int divisor_xy,
    signed char *kernel_x) {

  const __m256i cx0 = _mm256_load_si256((__m256i *)&mask_vector_x[0]);
  int newPixel = 0;
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i mask_16_1 = _mm256_set_epi16(0, 0, 0, 0, 0, 0xffff, 0, 0, 0, 0,
                                             0xffff, 0, 0, 0, 0, 0xffff);

  __m256i r0, m0, m1, m2, m3, m4, even, odd, output1;
  const __m256i mask_new_1 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0xffff, 0, 0, 0, 0, 0);
  const __m256i mask_16_5 = _mm256_set_epi16(0, 0xffff, 0, 0, 0, 0, 0xffff, 0,
                                             0, 0, 0, 0xffff, 0, 0, 0, 0);

  const __m256i mask3_2 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  // multiply by the mask
  r0 = _mm256_loadu_si256((__m256i *)&temp[col - 4]);
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add (complex shift is needed)
  m3 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 14);
  m3 = _mm256_add_epi16(m3, m4);
  m3 = _mm256_add_epi16(m0, m3); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m3, 4);
  m3 = _mm256_add_epi16(m3, m1); // add

  // third horizontal add
  m0 = _mm256_and_si256(m0, mask_16_5);
  m2 = _mm256_srli_si256(m0, 8);
  m4 = _mm256_and_si256(m0, mask3_2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 8);
  m2 = _mm256_add_epi16(m2, m4);

  m0 = _mm256_add_epi16(m2, m3); // add

  even = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,5,10 and discard the others

  // 2nd col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col - 3]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add (complex shift is needed)
  m3 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 14);
  m3 = _mm256_add_epi16(m3, m4);
  m3 = _mm256_add_epi16(m0, m3); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m3, 4);
  m3 = _mm256_add_epi16(m3, m1); // add

  // third horizontal add
  m0 = _mm256_and_si256(m0, mask_16_5);
  m2 = _mm256_srli_si256(m0, 8);
  m4 = _mm256_and_si256(m0, mask3_2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 8);
  m2 = _mm256_add_epi16(m2, m4);

  m0 = _mm256_add_epi16(m2, m3); // add

  odd = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,5,10 and discard the others

  // 3rd col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col - 2]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add (complex shift is needed)
  m3 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 14);
  m3 = _mm256_add_epi16(m3, m4);
  m3 = _mm256_add_epi16(m0, m3); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m3, 4);
  m3 = _mm256_add_epi16(m3, m1); // add

  // third horizontal add
  m0 = _mm256_and_si256(m0, mask_16_5);
  m2 = _mm256_srli_si256(m0, 8);
  m4 = _mm256_and_si256(m0, mask3_2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 8);
  m2 = _mm256_add_epi16(m2, m4);

  m0 = _mm256_add_epi16(m2, m3); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,5,10 and discard the others
  m0 = _mm256_slli_si256(m0, 2);
  even = _mm256_add_epi16(m0, even);

  // 4th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col - 1]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add (complex shift is needed)
  m3 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 14);
  m3 = _mm256_add_epi16(m3, m4);
  m3 = _mm256_add_epi16(m0, m3); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m3, 4);
  m3 = _mm256_add_epi16(m3, m1); // add

  // third horizontal add
  m0 = _mm256_and_si256(m0, mask_16_5);
  m2 = _mm256_srli_si256(m0, 8);
  m4 = _mm256_and_si256(m0, mask3_2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 8);
  m2 = _mm256_add_epi16(m2, m4);

  m0 = _mm256_add_epi16(m2, m3); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,5,10 and discard the others
  m0 = _mm256_slli_si256(m0, 2);
  odd = _mm256_add_epi16(m0, odd);

  // 5th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add (complex shift is needed)
  m3 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 14);
  m3 = _mm256_add_epi16(m3, m4);
  m3 = _mm256_add_epi16(m0, m3); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m3, 4);
  m3 = _mm256_add_epi16(m3, m1); // add

  // third horizontal add
  m0 = _mm256_and_si256(m0, mask_16_5);
  m2 = _mm256_srli_si256(m0, 8);
  m4 = _mm256_and_si256(m0, mask3_2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 8);
  m2 = _mm256_add_epi16(m2, m4);

  m0 = _mm256_add_epi16(m2, m3); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,5,10 and discard the others
  m0 = _mm256_slli_si256(m0, 4);
  even = _mm256_add_epi16(m0, even);

  // 6th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add (complex shift is needed)
  m3 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 14);
  m3 = _mm256_add_epi16(m3, m4);
  m3 = _mm256_add_epi16(m0, m3); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m3, 4);
  m3 = _mm256_add_epi16(m3, m1); // add

  // third horizontal add
  m0 = _mm256_and_si256(m0, mask_16_5);
  m2 = _mm256_srli_si256(m0, 8);
  m4 = _mm256_and_si256(m0, mask3_2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 8);
  m2 = _mm256_add_epi16(m2, m4);

  m0 = _mm256_add_epi16(m2, m3); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,5,10 and discard the others
  m0 = _mm256_slli_si256(m0, 4);
  odd = _mm256_add_epi16(m0, odd);

  // 7th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add (complex shift is needed)
  m3 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 14);
  m3 = _mm256_add_epi16(m3, m4);
  m3 = _mm256_add_epi16(m0, m3); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m3, 4);
  m3 = _mm256_add_epi16(m3, m1); // add

  // third horizontal add
  m0 = _mm256_and_si256(m0, mask_16_5);
  m2 = _mm256_srli_si256(m0, 8);
  m4 = _mm256_and_si256(m0, mask3_2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 8);
  m2 = _mm256_add_epi16(m2, m4);

  m0 = _mm256_add_epi16(m2, m3); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,5,10 and discard the others

  // shift the result 3 positions
  m1 = _mm256_slli_si256(m0, 6);
  m2 = _mm256_and_si256(m0, mask_new_1);
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 10);
  m0 = _mm256_add_epi16(m2, m1);

  even = _mm256_add_epi16(m0, even);

  // 8th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add (complex shift is needed)
  m3 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 14);
  m3 = _mm256_add_epi16(m3, m4);
  m3 = _mm256_add_epi16(m0, m3); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m3, 4);
  m3 = _mm256_add_epi16(m3, m1); // add

  // third horizontal add
  m0 = _mm256_and_si256(m0, mask_16_5);
  m2 = _mm256_srli_si256(m0, 8);
  m4 = _mm256_and_si256(m0, mask3_2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 8);
  m2 = _mm256_add_epi16(m2, m4);

  m0 = _mm256_add_epi16(m2, m3); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,5,10 and discard the others

  // shift the result 3 positions
  m1 = _mm256_slli_si256(m0, 6);
  m2 = _mm256_and_si256(m0, mask_new_1);
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 10);
  m0 = _mm256_add_epi16(m2, m1);

  odd = _mm256_add_epi16(m0, odd);

  // 9th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 4]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add (complex shift is needed)
  m3 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 14);
  m3 = _mm256_add_epi16(m3, m4);
  m3 = _mm256_add_epi16(m0, m3); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m3, 4);
  m3 = _mm256_add_epi16(m3, m1); // add

  // third horizontal add
  m0 = _mm256_and_si256(m0, mask_16_5);
  m2 = _mm256_srli_si256(m0, 8);
  m4 = _mm256_and_si256(m0, mask3_2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 8);
  m2 = _mm256_add_epi16(m2, m4);

  m0 = _mm256_add_epi16(m2, m3); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,5,10 and discard the others

  // shift the result 4 positions
  m1 = _mm256_slli_si256(m0, 8);
  m2 = _mm256_and_si256(m0, mask_new_1);
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 8);
  m0 = _mm256_add_epi16(m2, m1);

  even = _mm256_add_epi16(m0, even);

  // 10th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 5]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add (complex shift is needed)
  m3 = _mm256_srli_si256(m0, 2);
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 14);
  m3 = _mm256_add_epi16(m3, m4);
  m3 = _mm256_add_epi16(m0, m3); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m3, 4);
  m3 = _mm256_add_epi16(m3, m1); // add

  // third horizontal add
  m0 = _mm256_and_si256(m0, mask_16_5);
  m2 = _mm256_srli_si256(m0, 8);
  m4 = _mm256_and_si256(m0, mask3_2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(m4, 8);
  m2 = _mm256_add_epi16(m2, m4);

  m0 = _mm256_add_epi16(m2, m3); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,5,10 and discard the others

  // shift the result 4 positions
  m1 = _mm256_slli_si256(m0, 8);
  m2 = _mm256_and_si256(m0, mask_new_1);
  m2 = _mm256_permute2f128_si256(m2, m2, 1);
  m2 = _mm256_srli_si256(m2, 8);
  m0 = _mm256_add_epi16(m2, m1);

  odd = _mm256_add_epi16(m0, odd);

  // division
  even = division(division_case, even, f);
  odd = division(division_case, odd, f);

  // blend even , odd into one register
  odd = _mm256_slli_si256(odd, 1);
  output1 = _mm256_add_epi16(even, odd);

  //_mm256_storeu_si256( (__m256i *) &filt[row][col],even );

  switch (REMINDER_ITERATIONS_X) {
  case 1:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    break;
  case 2:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    break;
  case 3:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    break;
  case 4:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    break;
  case 5:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    break;
  case 6:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    break;
  case 7:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    break;
  case 8:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    break;
  case 9:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    break;
  case 10:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    break;
  case 11:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    break;
  case 12:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    break;
  case 13:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output1, 12);
    break;
  case 14:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output1, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output1, 13);
    break;
  case 15:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output1, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output1, 13);
    filt[row][col + 14] = (unsigned char)_mm256_extract_epi8(output1, 14);
    break;
  case 16:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    break;
  case 17:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    break;
  case 18:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    break;
  case 19:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    break;
  case 20:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    break;
  case 21:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    break;
  case 22:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    break;
  case 23:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    break;
  case 24:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    break;
  case 25:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    break;
  case 26:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    break;
  case 27:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    break;
  case 28:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    break;
  case 29:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output1, 28);
    break;
  case 30:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output1, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output1, 29);
    break;
  case 31:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output1, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output1, 29);

    newPixel = 0;

    newPixel += temp[col + 30 - 4] * kernel_x[0];
    newPixel += temp[col + 30 - 3] * kernel_x[1];
    newPixel += temp[col + 30 - 2] * kernel_x[2];
    newPixel += temp[col + 30 - 1] * kernel_x[3];
    newPixel += temp[col + 30 - 0] * kernel_x[4];

    filt[row][col + 30] = (unsigned char)(newPixel / divisor_xy);

    break;
  default: // REMINDER IS EITHER 27,28,29,30 OR 31
    printf("\nsomething went wrong");
    exit(EXIT_FAILURE);
  }
}

// this function is computing the row=2 and row=N-3 only
// for row=2 input prelude_7x7_Ymask_3(0,...
// for row=N-3 input prelude_7x7_Ymask_3(N-6,...
void prelude_7x7_16_Ymask_2_new(const int row, const unsigned int M,
                                unsigned char **frame1, unsigned char *temp,
                                const signed char mask_vector_y[][32],
                                const unsigned int division_case,
                                const __m256i f, const __m256i mask_prelude) {

  __m256i r0, r1, r2, r3, r4, r5, m0, m1, m2, m3, m4, m5, even, odd;
  //__m256i ones=_mm256_set1_epi16(1);
  __m256i cy0, cy1, cy2, cy3, cy4, cy5, cy0_sh1, cy1_sh1, cy2_sh1, cy3_sh1,
      cy4_sh1, cy5_sh1;

  if (row != 0) { // if row=N-6
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
    cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);

  } else {
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
    cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
  }

  for (unsigned int col = 0; col <= M - 32; col += 32) {

    //--------------------y filter begins--------------------
    // load the 9 rows
    r0 = _mm256_load_si256((__m256i *)&frame1[row][col]);
    r1 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
    r2 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);
    r3 = _mm256_load_si256((__m256i *)&frame1[row + 3][col]);
    r4 = _mm256_load_si256((__m256i *)&frame1[row + 4][col]);
    r5 = _mm256_load_si256((__m256i *)&frame1[row + 5][col]);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);
    m4 = _mm256_maddubs_epi16(r4, cy4);
    m5 = _mm256_maddubs_epi16(r5, cy5);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
    m4 = _mm256_maddubs_epi16(r4, cy4_sh1);
    m5 = _mm256_maddubs_epi16(r5, cy5_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even
    _mm256_store_si256((__m256i *)&temp[col], r0);
  }
  // y filter ends - r0 has now the data to be processed by x filter
}

// this function is computing the row=1 and row=N-2 only
// for row=1 input prelude_7x7_Ymask_3(0,...
// for row=N-2 input prelude_7x7_Ymask_3(N-5,...
void prelude_7x7_16_Ymask_1_new(const int row, const unsigned int M,
                                unsigned char **frame1, unsigned char *temp,
                                const signed char mask_vector_y[][32],
                                const unsigned int division_case,
                                const __m256i f, const __m256i mask_prelude) {

  __m256i r0, r1, r2, r3, r4, m0, m1, m2, m3, m4, even, odd;
  //__m256i ones=_mm256_set1_epi16(1);
  __m256i cy0, cy1, cy2, cy3, cy4, cy0_sh1, cy1_sh1, cy2_sh1, cy3_sh1, cy4_sh1;

  if (row != 0) { // if row=N-6
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);

  } else {
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
    cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
  }

  for (unsigned int col = 0; col <= M - 32; col += 32) {

    //--------------------y filter begins--------------------
    // load the 9 rows
    r0 = _mm256_load_si256((__m256i *)&frame1[row][col]);
    r1 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
    r2 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);
    r3 = _mm256_load_si256((__m256i *)&frame1[row + 3][col]);
    r4 = _mm256_load_si256((__m256i *)&frame1[row + 4][col]);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);
    m4 = _mm256_maddubs_epi16(r4, cy4);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
    m4 = _mm256_maddubs_epi16(r4, cy4_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even
    _mm256_store_si256((__m256i *)&temp[col], r0);
  }
  // y filter ends - r0 has now the data to be processed by x filter
}

// this function is computing the row=0 and row=N-1 only
// for row=0 input prelude_7x7_Ymask_3(0,...
// for row=N-1 input prelude_7x7_Ymask_3(N-4,...
void prelude_7x7_16_Ymask_0_new(const int row, const unsigned int M,
                                unsigned char **frame1, unsigned char *temp,
                                const signed char mask_vector_y[][32],
                                const unsigned int division_case,
                                const __m256i f, const __m256i mask_prelude) {

  __m256i r0, r1, r2, r3, m0, m1, m2, m3, even, odd;
  //__m256i ones=_mm256_set1_epi16(1);
  __m256i cy0, cy1, cy2, cy3, cy0_sh1, cy1_sh1, cy2_sh1, cy3_sh1;

  if (row != 0) { // if row=N-6
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
  } else {
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
  }

  for (unsigned int col = 0; col <= M - 32; col += 32) {

    //--------------------y filter begins--------------------
    // load the 9 rows
    r0 = _mm256_load_si256((__m256i *)&frame1[row][col]);
    r1 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
    r2 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);
    r3 = _mm256_load_si256((__m256i *)&frame1[row + 3][col]);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even
    _mm256_store_si256((__m256i *)&temp[col], r0);
  }
  // y filter ends - r0 has now the data to be processed by x filter
}

void prelude_7x7_16_Xmask_new(unsigned char **frame1, unsigned char **filt,
                              unsigned char *temp, const unsigned int N,
                              const unsigned int M, const int row,
                              const signed char mask_vector_x[][32],
                              const unsigned int division_case, const __m256i f,
                              const unsigned int REMINDER_ITERATIONS_XY,
                              const signed char mask_vector_y[][32],
                              const unsigned short int divisor_xy,
                              signed char *kernel_x) {

  __m256i r0, m0, m1, even, odd;

  const __m256i cx0 = _mm256_load_si256((__m256i *)&mask_vector_x[0]);

  const __m256i mask_16_1 = _mm256_set_epi16(0, 0, 0, 0xffff, 0, 0, 0, 0xffff,
                                             0, 0, 0, 0xffff, 0, 0, 0, 0xffff);

  const __m256i mask_prelude_3 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude_2 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude_1 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  unsigned int col;

  for (col = 0; col <= M - 32; col += 32) {

    if (col == 0) {

      // 1st col iteration
      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[0]);
      r0 = fill_3zeros(r0, mask_prelude_3);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      even = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

      // 2nd col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[0]);
      r0 = fill_2zeros(r0, mask_prelude_2);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      odd = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                             // 0,4,8,12 and discard the others

      // 3rd col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[0]);
      r0 = fill_1zeros(r0, mask_prelude_1);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE even
      m0 = _mm256_slli_si256(m0, 2);
      even = _mm256_add_epi16(m0, even);

      // 4th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[0]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE odd
      m0 = _mm256_slli_si256(m0, 2);
      odd = _mm256_add_epi16(m0, odd);

      // 5th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[1]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE even
      m0 = _mm256_slli_si256(m0, 4);
      even = _mm256_add_epi16(m0, even);

      // 6th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[2]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE odd
      m0 = _mm256_slli_si256(m0, 4);
      odd = _mm256_add_epi16(m0, odd);

      // 7th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[3]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE even
      m0 = _mm256_slli_si256(m0, 6);
      even = _mm256_add_epi16(m0, even);

      // 8th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[4]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE odd
      m0 = _mm256_slli_si256(m0, 6);
      odd = _mm256_add_epi16(m0, odd);

      even = division(division_case, even, f);
      odd = division(division_case, odd, f);

      odd = _mm256_slli_si256(odd, 1);
      even = _mm256_add_epi8(even, odd);

      _mm256_store_si256((__m256i *)&filt[row][0], even);

    } else {

      // col iteration computes output pixels of   0,8,16,24
      // col+1 iteration computes output pixels of 1,9,17,25
      // col+2 iteration computes output pixels of 2,10,18,26
      // col+3 iteration computes output pixels of 3,11,19,27
      // col+4 iteration computes output pixels of 4,12,20,28
      // col+5 iteration computes output pixels of 5,13,21,29
      // col+6 iteration computes output pixels of 6,14,22,30
      // col+7 iteration computes output pixels of 7,15,23,31
      // afterwards, col becomes 32 and repeat the above process

      // 1st col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col - 3]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      even = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

      // 2nd col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col - 2]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      odd = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                             // 0,4,8,12 and discard the others

      // 3rd col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col - 1]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE even
      m0 = _mm256_slli_si256(m0, 2);
      even = _mm256_add_epi16(m0, even);

      // 4th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE odd
      m0 = _mm256_slli_si256(m0, 2);
      odd = _mm256_add_epi16(m0, odd);

      // 5th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE even
      m0 = _mm256_slli_si256(m0, 4);
      even = _mm256_add_epi16(m0, even);

      // 6th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE odd
      m0 = _mm256_slli_si256(m0, 4);
      odd = _mm256_add_epi16(m0, odd);

      // 7th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE even
      m0 = _mm256_slli_si256(m0, 6);
      even = _mm256_add_epi16(m0, even);

      // 8th col iteration
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 4]);

      // multiply by the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // first horizontal add
      m1 = _mm256_srli_si256(m0, 2);
      m0 = _mm256_add_epi16(m0, m1); // add

      // second horizontal add
      m1 = _mm256_srli_si256(m0, 4);
      m0 = _mm256_add_epi16(m0, m1); // add

      m0 = _mm256_and_si256(m0, mask_16_1); // in 16-bit format keep only
                                            // 0,4,8,12 and discard the others

      // MERGE odd
      m0 = _mm256_slli_si256(m0, 6);
      odd = _mm256_add_epi16(m0, odd);

      even = division(division_case, even, f);
      odd = division(division_case, odd, f);

      odd = _mm256_slli_si256(odd, 1);
      even = _mm256_add_epi8(even, odd);

      _mm256_store_si256((__m256i *)&filt[row][col], even);
    }
  }

  loop_reminder_7x7_16_blur_X(frame1, filt, temp, N, M, row, col,
                              REMINDER_ITERATIONS_XY, division_case,
                              mask_vector_x, f, divisor_xy, kernel_x);
}

void loop_reminder_7x7_16_blur_Y(unsigned char **frame1, unsigned char **filt,
                                 unsigned char *temp, const unsigned int N,
                                 const unsigned int M, const unsigned int row,
                                 const unsigned int REMINDER_ITERATIONS_Y,
                                 const unsigned int division_case,
                                 const signed char mask_vector_y[][32],
                                 const __m256i f,
                                 const unsigned short int divisor_xy) {

  __m256i r0, r1, r2, r3, r4, r5, r6, m0, m1, m2, m3, m4, m5, m6, even, odd,
      reminder_mask;

  if (REMINDER_ITERATIONS_Y == 0) { // no need for computing the y mask
    _mm256_store_si256((__m256i *)&temp[M], _mm256_setzero_si256());
    for (unsigned int g = M + 32; g < M + 32 + 6; g++)
      temp[g] = 0;
  } else {
    __m256i cy0, cy1, cy2, cy3, cy4, cy5, cy6, cy0_sh1, cy1_sh1, cy2_sh1,
        cy3_sh1, cy4_sh1, cy5_sh1, cy6_sh1;

    unsigned int col2 =
        M - REMINDER_ITERATIONS_Y; // this is the col index to load the elements
                                   // for the y mask

    if ((row > 2) && (row < N - 3)) { // main case

      r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 3][col2]);
      r1 = _mm256_loadu_si256((__m256i *)&frame1[row - 2][col2]);
      r2 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col2]);
      r3 = _mm256_loadu_si256((__m256i *)&frame1[row][col2]);
      r4 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col2]);
      r5 = _mm256_loadu_si256((__m256i *)&frame1[row + 2][col2]);
      r6 = _mm256_loadu_si256((__m256i *)&frame1[row + 3][col2]);

      cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
      cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
      cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
      cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
      cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
      cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
      cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);

      cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
      cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
      cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
      cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
      cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
      cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
      cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);

    } else {
      if (row == 0) {
        r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col2]);
        r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col2]);
        r4 = _mm256_setzero_si256();
        r5 = _mm256_setzero_si256();
        r6 = _mm256_setzero_si256();

        cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy4 = _mm256_setzero_si256();
        cy5 = _mm256_setzero_si256();
        cy6 = _mm256_setzero_si256();

        cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
        cy4_sh1 = _mm256_setzero_si256();
        cy5_sh1 = _mm256_setzero_si256();
        cy6_sh1 = _mm256_setzero_si256();

      } else if (row == 1) {
        r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col2]);
        r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[4][col2]);
        r5 = _mm256_setzero_si256();
        r6 = _mm256_setzero_si256();

        cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy5 = _mm256_setzero_si256();
        cy6 = _mm256_setzero_si256();

        cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
        cy5_sh1 = _mm256_setzero_si256();
        cy6_sh1 = _mm256_setzero_si256();

      } else if (row == 2) {
        r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col2]);
        r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[4][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[5][col2]);
        r6 = _mm256_setzero_si256();

        cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy6 = _mm256_setzero_si256();

        cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
        cy6_sh1 = _mm256_setzero_si256();

      }

      else if (row == N - 3) {
        r0 = _mm256_setzero_si256();
        r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 6][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 5][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col2]);
        r6 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col2]);

        cy0 = _mm256_setzero_si256();
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);

        cy0_sh1 = _mm256_setzero_si256();
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
        cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
      } else if (row == N - 2) {
        r0 = _mm256_setzero_si256();
        r1 = _mm256_setzero_si256();
        r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 5][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col2]);
        r6 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col2]);

        cy0 = _mm256_setzero_si256();
        cy1 = _mm256_setzero_si256();
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);

        cy0_sh1 = _mm256_setzero_si256();
        cy1_sh1 = _mm256_setzero_si256();
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
        cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
      } else if (row == N - 1) {
        r0 = _mm256_setzero_si256();
        r1 = _mm256_setzero_si256();
        r2 = _mm256_setzero_si256();
        r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col2]);
        r5 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col2]);
        r6 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col2]);

        cy0 = _mm256_setzero_si256();
        cy1 = _mm256_setzero_si256();
        cy2 = _mm256_setzero_si256();
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);

        cy0_sh1 = _mm256_setzero_si256();
        cy1_sh1 = _mm256_setzero_si256();
        cy2_sh1 = _mm256_setzero_si256();
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
        cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
      } else {
        printf("\nsomething went wrong");
        exit(EXIT_FAILURE);
      }
    }

    /*
     * The above load operations load outside of the array bounds; these
     * elements are filled with zeros just after
     */
    reminder_mask = _mm256_load_si256(
        (__m256i *)&reminder_mask_9x9[REMINDER_ITERATIONS_Y - 1][0]);

    // AND r0-r4 with reminder_mask
    r0 = _mm256_and_si256(r0, reminder_mask);
    r1 = _mm256_and_si256(r1, reminder_mask);
    r2 = _mm256_and_si256(r2, reminder_mask);
    r3 = _mm256_and_si256(r3, reminder_mask);
    r4 = _mm256_and_si256(r4, reminder_mask);
    r5 = _mm256_and_si256(r5, reminder_mask);
    r6 = _mm256_and_si256(r6, reminder_mask);

    //--------------------y filter begins--------------------

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);
    m4 = _mm256_maddubs_epi16(r4, cy4);
    m5 = _mm256_maddubs_epi16(r5, cy5);
    m6 = _mm256_maddubs_epi16(r6, cy6);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);
    m0 = _mm256_add_epi16(m0, m6);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
    m4 = _mm256_maddubs_epi16(r4, cy4_sh1);
    m5 = _mm256_maddubs_epi16(r5, cy5_sh1);
    m6 = _mm256_maddubs_epi16(r6, cy6_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);
    m0 = _mm256_add_epi16(m0, m5);
    m0 = _mm256_add_epi16(m0, m6);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even

    _mm256_storeu_si256((__m256i *)&temp[M - REMINDER_ITERATIONS_Y], r0);

    for (int gg = M; gg < M + 32 + 6; gg++)
      temp[gg] = 0;
    // y filter ends - r0 has now the data to be processed by x filter
  }
}

int loop_reminder_7x7_16_blur_X(
    unsigned char **frame1, unsigned char **filt, unsigned char *temp,
    const unsigned int N, const unsigned int M, const unsigned int row,
    const unsigned int col, const unsigned int REMINDER_ITERATIONS_X,
    const unsigned int division_case, const signed char mask_vector_x[][32],
    const __m256i f, const unsigned short int divisor_xy,
    signed char *kernel_x) {

  const __m256i cx0 = _mm256_load_si256((__m256i *)&mask_vector_x[0]);
  __m256i r0, m0, m1, even, odd, output1;
  int newPixel = 0;
  const __m256i mask_16_1 = _mm256_set_epi16(0, 0, 0, 0xffff, 0, 0, 0, 0xffff,
                                             0, 0, 0, 0xffff, 0, 0, 0, 0xffff);

  if (REMINDER_ITERATIONS_X == 0) {
    return 0;
  }

  // 1st col iteration

  // multiply by the mask
  r0 = _mm256_loadu_si256((__m256i *)&temp[col - 3]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add
  m1 = _mm256_srli_si256(m0, 2);
  m0 = _mm256_add_epi16(m0, m1); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m0, 4);
  m0 = _mm256_add_epi16(m0, m1); // add

  even = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,4,8,12 and discard the others

  // 2nd col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col - 2]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add
  m1 = _mm256_srli_si256(m0, 2);
  m0 = _mm256_add_epi16(m0, m1); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m0, 4);
  m0 = _mm256_add_epi16(m0, m1); // add

  odd = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,4,8,12 and discard the others

  // 3rd col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col - 1]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add
  m1 = _mm256_srli_si256(m0, 2);
  m0 = _mm256_add_epi16(m0, m1); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m0, 4);
  m0 = _mm256_add_epi16(m0, m1); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,4,8,12 and discard the others

  // MERGE even
  m0 = _mm256_slli_si256(m0, 2);
  even = _mm256_add_epi16(m0, even);

  // 4th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add
  m1 = _mm256_srli_si256(m0, 2);
  m0 = _mm256_add_epi16(m0, m1); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m0, 4);
  m0 = _mm256_add_epi16(m0, m1); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,4,8,12 and discard the others

  // MERGE odd
  m0 = _mm256_slli_si256(m0, 2);
  odd = _mm256_add_epi16(m0, odd);

  // 5th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add
  m1 = _mm256_srli_si256(m0, 2);
  m0 = _mm256_add_epi16(m0, m1); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m0, 4);
  m0 = _mm256_add_epi16(m0, m1); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,4,8,12 and discard the others

  // MERGE even
  m0 = _mm256_slli_si256(m0, 4);
  even = _mm256_add_epi16(m0, even);

  // 6th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add
  m1 = _mm256_srli_si256(m0, 2);
  m0 = _mm256_add_epi16(m0, m1); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m0, 4);
  m0 = _mm256_add_epi16(m0, m1); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,4,8,12 and discard the others

  // MERGE odd
  m0 = _mm256_slli_si256(m0, 4);
  odd = _mm256_add_epi16(m0, odd);

  // 7th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add
  m1 = _mm256_srli_si256(m0, 2);
  m0 = _mm256_add_epi16(m0, m1); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m0, 4);
  m0 = _mm256_add_epi16(m0, m1); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,4,8,12 and discard the others

  // MERGE even
  m0 = _mm256_slli_si256(m0, 6);
  even = _mm256_add_epi16(m0, even);

  // 8th col iteration
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 4]);

  // multiply by the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // first horizontal add
  m1 = _mm256_srli_si256(m0, 2);
  m0 = _mm256_add_epi16(m0, m1); // add

  // second horizontal add
  m1 = _mm256_srli_si256(m0, 4);
  m0 = _mm256_add_epi16(m0, m1); // add

  m0 = _mm256_and_si256(
      m0,
      mask_16_1); // in 16-bit format keep only 0,4,8,12 and discard the others

  // MERGE odd
  m0 = _mm256_slli_si256(m0, 6);
  odd = _mm256_add_epi16(m0, odd);

  even = division(division_case, even, f);
  odd = division(division_case, odd, f);

  odd = _mm256_slli_si256(odd, 1);
  output1 = _mm256_add_epi8(even, odd);

  switch (REMINDER_ITERATIONS_X) {
  case 1:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    break;
  case 2:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    break;
  case 3:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    break;
  case 4:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    break;
  case 5:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    break;
  case 6:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    break;
  case 7:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    break;
  case 8:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    break;
  case 9:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    break;
  case 10:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    break;
  case 11:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    break;
  case 12:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    break;
  case 13:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output1, 12);
    break;
  case 14:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output1, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output1, 13);
    break;
  case 15:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output1, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output1, 13);
    filt[row][col + 14] = (unsigned char)_mm256_extract_epi8(output1, 14);
    break;
  case 16:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    break;
  case 17:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    break;
  case 18:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    break;
  case 19:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    break;
  case 20:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    break;
  case 21:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    break;
  case 22:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    break;
  case 23:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    break;
  case 24:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    break;
  case 25:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    break;
  case 26:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    break;
  case 27:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    break;
  case 28:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    break;
  case 29:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output1, 28);
    break;
  case 30:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output1, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output1, 29);
    break;
  case 31:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output1, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output1, 29);

    newPixel = 0;

    newPixel += temp[col + 30 - 4] * kernel_x[0];
    newPixel += temp[col + 30 - 3] * kernel_x[1];
    newPixel += temp[col + 30 - 2] * kernel_x[2];
    newPixel += temp[col + 30 - 1] * kernel_x[3];
    newPixel += temp[col + 30 - 0] * kernel_x[4];

    filt[row][col + 30] = (unsigned char)(newPixel / divisor_xy);

    break;
  default:
    printf("\nsomething went wrong");
    exit(EXIT_FAILURE);
  }

  return 0;
}

void Gaussian_Blur_optimized_5x5_16_seperable(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, signed char *kernel_y, signed char *kernel_x,
    const unsigned short int divisor_xy) {

  const signed char fx0 = kernel_x[0];
  const signed char fx1 = kernel_x[1];
  const signed char fx2 = kernel_x[2];
  const signed char fx3 = kernel_x[3];
  const signed char fx4 = kernel_x[4];

  const signed char fy0 = kernel_y[0];
  const signed char fy1 = kernel_y[1];
  const signed char fy2 = kernel_y[2];
  const signed char fy3 = kernel_y[3];
  const signed char fy4 = kernel_y[4];

  const signed char mask_vector_x[1][32] __attribute__((aligned(64))) = {
      {fx0, fx1, fx2, fx3, fx4, 0,   fx0, fx1, fx2, fx3, fx4,
       0,   fx0, fx1, fx2, fx3, fx4, 0,   fx0, fx1, fx2, fx3,
       fx4, 0,   fx0, fx1, fx2, fx3, fx4, 0,   0,   0},
  };

  const signed char mask_vector_y[10][32] __attribute__((aligned(64))) = {
      {fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0,
       fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0},
      {fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0,
       fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0},
      {fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0,
       fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0},
      {fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0,
       fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0},
      {fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0,
       fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0},

      {0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0,
       0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0},
      {0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1,
       0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1},
      {0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2,
       0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2},
      {0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3,
       0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3},
      {0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4,
       0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4},
  };

  const __m256i cx0 = _mm256_load_si256((__m256i *)&mask_vector_x[0]);

  const __m256i cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
  const __m256i cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
  const __m256i cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
  const __m256i cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
  const __m256i cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);

  const __m256i cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
  const __m256i cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
  const __m256i cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
  const __m256i cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
  const __m256i cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);

  const __m256i mask_prelude2 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude1 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const unsigned int REMINDER_ITERATIONS_X_mask =
      M - ((((M - 32) / 30) * 30) + 30); // M-(last_col_value+30)
  const unsigned int REMINDER_ITERATIONS_Y_mask =
      M - ((((M - 32) / 32) * 32) + 32);

  // printf("\nREMINDER_ITERATIONSX,Y=%u
  // %u",REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask);

  const unsigned int division_case = prepare_for_division(
      divisor_xy); // determine which is the division case (A, B or C)
  const __m256i f = _mm256_load_si256(
      (__m256i *)&f_vector[0]); // initialize the division vector

  const __m256i mask2 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0);
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i output_mask = _mm256_set_epi16(
      0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535);

  // printf("\n%d ",division_case);

#pragma omp parallel
  {

    unsigned int row, col;
    __m256i rr0, r0, r1, r2, r3, r4, m0, m1, m2, m3, m4, output_even,
        output_odd;
    __m256i even, odd;

    unsigned char temp[M + 32 + 3] __attribute__((
        aligned(64))); // temporal storage of the output of y filter.

    /*---------------------- Gaussian Blur ---------------------------------*/

#pragma omp for schedule(static)
    for (row = 1; row <= N - 2; row++) {

      if (row == 1) { // in this case I calculate filt[0][:] and filt[1][:] too.
                      // No extra loads or multiplications are required for this

        prelude_5x5_16_Ymask_0_new(0, M, frame1, temp, mask_vector_y,
                                   division_case, f);
        loop_reminder_5x5_16_blur_Y(frame1, filt, temp, N, M, 0,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_5x5_16_Xmask_new(frame1, filt, temp, N, M, 0, mask_vector_x,
                                 division_case, f, REMINDER_ITERATIONS_X_mask,
                                 mask_vector_y, divisor_xy, kernel_x);

        prelude_5x5_16_Ymask_1_new(0, M, frame1, temp, mask_vector_y,
                                   division_case, f);
        loop_reminder_5x5_16_blur_Y(frame1, filt, temp, N, M, 1,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_5x5_16_Xmask_new(frame1, filt, temp, N, M, 1, mask_vector_x,
                                 division_case, f, REMINDER_ITERATIONS_X_mask,
                                 mask_vector_y, divisor_xy, kernel_x);

      }

      else if (row == N - 2) { // in this case I calculate filt[N-2][:] and
                               // filt[N-1][:] too. Below row0 refers to row=N-2
                               // and row1 refers to row=N-1

        prelude_5x5_16_Ymask_1_new(N - 4, M, frame1, temp, mask_vector_y,
                                   division_case, f);
        loop_reminder_5x5_16_blur_Y(frame1, filt, temp, N, M, N - 2,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_5x5_16_Xmask_new(frame1, filt, temp, N, M, N - 2, mask_vector_x,
                                 division_case, f, REMINDER_ITERATIONS_X_mask,
                                 mask_vector_y, divisor_xy, kernel_x);

        prelude_5x5_16_Ymask_0_new(N - 3, M, frame1, temp, mask_vector_y,
                                   division_case, f);
        loop_reminder_5x5_16_blur_Y(frame1, filt, temp, N, M, N - 1,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_5x5_16_Xmask_new(frame1, filt, temp, N, M, N - 1, mask_vector_x,
                                 division_case, f, REMINDER_ITERATIONS_X_mask,
                                 mask_vector_y, divisor_xy, kernel_x);

      } else { // main loop

        for (col = 0; col <= M - 32; col += 32) {

          // load the 5 rows
          r0 = _mm256_load_si256((__m256i *)&frame1[row - 2][col]);
          r1 = _mm256_load_si256((__m256i *)&frame1[row - 1][col]);
          r2 = _mm256_load_si256((__m256i *)&frame1[row][col]);
          r3 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
          r4 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);

          //--------------------y filter begins--------------------

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, cy0);
          m1 = _mm256_maddubs_epi16(r1, cy1);
          m2 = _mm256_maddubs_epi16(r2, cy2);
          m3 = _mm256_maddubs_epi16(r3, cy3);
          m4 = _mm256_maddubs_epi16(r4, cy4);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          even = division(division_case, m0, f); // even results

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
          m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
          m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
          m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
          m4 = _mm256_maddubs_epi16(r4, cy4_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);

          odd = division(division_case, m0, f); // odd results

          // pack to one register
          odd = _mm256_slli_si256(odd, 1); // shift one position right
          r0 = _mm256_add_epi8(even, odd); // add the odd with the even
          // y filter ends - r0 has now the data to be processed by x filter

          _mm256_store_si256((__m256i *)&temp[col], r0);
        }
        loop_reminder_5x5_16_blur_Y(frame1, filt, temp, N, M, row,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);

        for (col = 0; col <= M - 32; col += 30) {

          if (col == 0) {

            // 1st col iteration

            // multiply by the mask
            rr0 = _mm256_load_si256((__m256i *)&temp[0]);
            r0 = insert_two_zeros_front(rr0, mask_prelude2);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            output_even = _mm256_and_si256(m2, output_mask);

            // 2nd col iteration

            r0 = insert_one_zeros_front(rr0, mask_prelude1);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            output_odd = _mm256_and_si256(m2, output_mask);

            // 3rd col iteration

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(rr0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            m2 = _mm256_and_si256(m2, output_mask);

            m2 = _mm256_slli_si256(m2, 2);
            output_even = _mm256_add_epi16(output_even, m2);

            // 4th col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            m2 = _mm256_and_si256(m2, output_mask);

            m2 = _mm256_slli_si256(m2, 2);
            output_odd = _mm256_add_epi16(output_odd, m2);

            // 5th col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            m2 = _mm256_and_si256(m2, output_mask);

            m3 = _mm256_slli_si256(m2, 4);
            m4 = _mm256_and_si256(m2, mask2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_srli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m3, m4);

            output_even = _mm256_add_epi16(output_even, m2);

            // 6th col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            m2 = _mm256_and_si256(m2, output_mask);

            m3 = _mm256_slli_si256(m2, 4);
            m4 = _mm256_and_si256(m2, mask2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_srli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m3, m4);

            output_odd = _mm256_add_epi16(output_odd, m2);

            // now division follows
            output_even = division(division_case, output_even, f);
            output_odd = division(division_case, output_odd, f);

            // shift odd 1 position and add to even
            output_odd = _mm256_slli_si256(output_odd, 1);
            output_even = _mm256_add_epi8(output_even, output_odd);

            _mm256_store_si256((__m256i *)&filt[row][0], output_even);

          } else {

            // col iteration computes output pixels of   0,6,12,18,24
            // col+1 iteration computes output pixels of 1,7,13,19,25
            // col+2 iteration computes output pixels of 2,8,14,20,26
            // col+3 iteration computes output pixels of 3,9,15,21,27
            // col+4 iteration computes output pixels of 4,10,16,22,28
            // col+5 iteration computes output pixels of 5,11,17,23,29
            // afterwards, col becomes 30 and repeat the above process

            // 1st col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col - 2]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            output_even = _mm256_and_si256(m2, output_mask);

            // 2nd col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col - 1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            output_odd = _mm256_and_si256(m2, output_mask);

            // 3rd col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 0]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            m2 = _mm256_and_si256(m2, output_mask);

            m2 = _mm256_slli_si256(m2, 2);
            output_even = _mm256_add_epi16(output_even, m2);

            // 4th col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            m2 = _mm256_and_si256(m2, output_mask);

            m2 = _mm256_slli_si256(m2, 2);
            output_odd = _mm256_add_epi16(output_odd, m2);

            // 5th col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            m2 = _mm256_and_si256(m2, output_mask);

            m3 = _mm256_slli_si256(m2, 4);
            m4 = _mm256_and_si256(m2, mask2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_srli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m3, m4);

            output_even = _mm256_add_epi16(output_even, m2);

            // 6th col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // 1st hadd
            m1 = _mm256_srli_si256(m0, 2);
            m1 = _mm256_add_epi16(m0, m1);

            // 2nd hadd
            m2 = _mm256_srli_si256(m0, 4);
            m2 = _mm256_add_epi16(m1, m2);

            // after shifts the 15th element is lost as it cannot be propagated
            // to the 16th position (AVX registers are managed as two seperate
            // SSE registers)
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m2, m4);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,3,6,9,12. keep only those, discard others
            m2 = _mm256_and_si256(m2, output_mask);

            m3 = _mm256_slli_si256(m2, 4);
            m4 = _mm256_and_si256(m2, mask2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_srli_si256(
                m4, 12); // shift 6 short int positions or 12 char positions
            m2 = _mm256_add_epi16(m3, m4);

            output_odd = _mm256_add_epi16(output_odd, m2);

            // now division follows
            output_even = division(division_case, output_even, f);
            output_odd = division(division_case, output_odd, f);

            // shift odd 1 position and add to even
            output_odd = _mm256_slli_si256(output_odd, 1);
            output_even = _mm256_add_epi8(output_even, output_odd);

            _mm256_storeu_si256((__m256i *)&filt[row][col], output_even);
          }
        }

        loop_reminder_5x5_16_blur_X(frame1, filt, temp, N, M, row, col,
                                    REMINDER_ITERATIONS_X_mask, division_case,
                                    mask_vector_x, f, divisor_xy, kernel_x);
      }
    }

  } // end of parallel
}

// this function is computing the row=1 and row=N-2 only
// for row=1 input prelude_5x5_Ymask_3(0,...
// for row=N-2 input prelude_5x5_Ymask_3(N-4,...
void prelude_5x5_16_Ymask_1_new(const int row, const unsigned int M,
                                unsigned char **frame1, unsigned char *temp,
                                const signed char mask_vector_y[][32],
                                const unsigned int division_case,
                                const __m256i f) {

  __m256i r0, r1, r2, r3, m0, m1, m2, m3, even, odd;
  //__m256i ones=_mm256_set1_epi16(1);
  __m256i cy0, cy1, cy2, cy3, cy0_sh1, cy1_sh1, cy2_sh1, cy3_sh1;

  if (row != 0) { // if row=N-4
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);

  } else {
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
    cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
  }

  for (unsigned int col = 0; col <= M - 32; col += 32) {

    //--------------------y filter begins--------------------
    // load the 9 rows
    r0 = _mm256_load_si256((__m256i *)&frame1[row][col]);
    r1 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
    r2 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);
    r3 = _mm256_load_si256((__m256i *)&frame1[row + 3][col]);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even
    _mm256_store_si256((__m256i *)&temp[col], r0);
  }
  // y filter ends - r0 has now the data to be processed by x filter
}

// this function is computing the row=0 and row=N-1 only
// for row=0 input prelude_5x5_Ymask_3(0,...
// for row=N-1 input prelude_5x5_Ymask_3(N-3,...
void prelude_5x5_16_Ymask_0_new(const int row, const unsigned int M,
                                unsigned char **frame1, unsigned char *temp,
                                const signed char mask_vector_y[][32],
                                const unsigned int division_case,
                                const __m256i f) {

  __m256i r0, r1, r2, m0, m1, m2, even, odd;
  //__m256i ones=_mm256_set1_epi16(1);
  __m256i cy0, cy1, cy2, cy0_sh1, cy1_sh1, cy2_sh1;

  if (row != 0) { // if row=N-6
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
  } else {
    cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
    cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
    cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);

    cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
    cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
    cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
  }

  for (unsigned int col = 0; col <= M - 32; col += 32) {

    //--------------------y filter begins--------------------
    // load the 9 rows
    r0 = _mm256_load_si256((__m256i *)&frame1[row][col]);
    r1 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
    r2 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even
    _mm256_store_si256((__m256i *)&temp[col], r0);
  }
  // y filter ends - r0 has now the data to be processed by x filter
}

void prelude_5x5_16_Xmask_new(unsigned char **frame1, unsigned char **filt,
                              unsigned char *temp, const unsigned int N,
                              const unsigned int M, const int row,
                              const signed char mask_vector_x[][32],
                              const unsigned int division_case, const __m256i f,
                              const unsigned int REMINDER_ITERATIONS_X,
                              const signed char mask_vector_y[][32],
                              const unsigned short int divisor_xy,
                              signed char *kernel_x) {

  const __m256i cx0 = _mm256_load_si256((__m256i *)&mask_vector_x[0]);

  const __m256i mask_prelude2 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude1 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i mask2 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0);
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i output_mask = _mm256_set_epi16(
      0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535);

  __m256i rr0, r0, m0, m1, m2, m3, m4, output_even, output_odd;

  unsigned int col;

  for (col = 0; col <= M - 32; col += 30) {

    if (col == 0) {

      // 1st col iteration

      // multiply by the mask
      rr0 = _mm256_load_si256((__m256i *)&temp[0]);
      r0 = insert_two_zeros_front(rr0, mask_prelude2);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      output_even = _mm256_and_si256(m2, output_mask);

      // 2nd col iteration

      r0 = insert_one_zeros_front(rr0, mask_prelude1);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      output_odd = _mm256_and_si256(m2, output_mask);

      // 3rd col iteration

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(rr0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      m2 = _mm256_and_si256(m2, output_mask);

      m2 = _mm256_slli_si256(m2, 2);
      output_even = _mm256_add_epi16(output_even, m2);

      // 4th col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      m2 = _mm256_and_si256(m2, output_mask);

      m2 = _mm256_slli_si256(m2, 2);
      output_odd = _mm256_add_epi16(output_odd, m2);

      // 5th col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      m2 = _mm256_and_si256(m2, output_mask);

      m3 = _mm256_slli_si256(m2, 4);
      m4 = _mm256_and_si256(m2, mask2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_srli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m3, m4);

      output_even = _mm256_add_epi16(output_even, m2);

      // 6th col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      m2 = _mm256_and_si256(m2, output_mask);

      m3 = _mm256_slli_si256(m2, 4);
      m4 = _mm256_and_si256(m2, mask2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_srli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m3, m4);

      output_odd = _mm256_add_epi16(output_odd, m2);

      // now division follows
      output_even = division(division_case, output_even, f);
      output_odd = division(division_case, output_odd, f);

      // shift odd 1 position and add to even
      output_odd = _mm256_slli_si256(output_odd, 1);
      output_even = _mm256_add_epi8(output_even, output_odd);

      _mm256_store_si256((__m256i *)&filt[row][0], output_even);

    } else {

      // col iteration computes output pixels of   0,6,12,18,24
      // col+1 iteration computes output pixels of 1,7,13,19,25
      // col+2 iteration computes output pixels of 2,8,14,20,26
      // col+3 iteration computes output pixels of 3,9,15,21,27
      // col+4 iteration computes output pixels of 4,10,16,22,28
      // col+5 iteration computes output pixels of 5,11,17,23,29
      // afterwards, col becomes 30 and repeat the above process

      // 1st col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col - 2]);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      output_even = _mm256_and_si256(m2, output_mask);

      // 2nd col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col - 1]);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      output_odd = _mm256_and_si256(m2, output_mask);

      // 3rd col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 0]);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      m2 = _mm256_and_si256(m2, output_mask);

      m2 = _mm256_slli_si256(m2, 2);
      output_even = _mm256_add_epi16(output_even, m2);

      // 4th col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      m2 = _mm256_and_si256(m2, output_mask);

      m2 = _mm256_slli_si256(m2, 2);
      output_odd = _mm256_add_epi16(output_odd, m2);

      // 5th col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      m2 = _mm256_and_si256(m2, output_mask);

      m3 = _mm256_slli_si256(m2, 4);
      m4 = _mm256_and_si256(m2, mask2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_srli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m3, m4);

      output_even = _mm256_add_epi16(output_even, m2);

      // 6th col iteration

      // multiply by the mask
      r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, cx0);

      // 1st hadd
      m1 = _mm256_srli_si256(m0, 2);
      m1 = _mm256_add_epi16(m0, m1);

      // 2nd hadd
      m2 = _mm256_srli_si256(m0, 4);
      m2 = _mm256_add_epi16(m1, m2);

      // after shifts the 15th element is lost as it cannot be propagated to the
      // 16th position (AVX registers are managed as two seperate SSE registers)
      m4 = _mm256_and_si256(m0, mask3);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_slli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m2, m4);

      // m2 has 16 16bit values now. the results I need are in positions
      // 0,3,6,9,12. keep only those, discard others
      m2 = _mm256_and_si256(m2, output_mask);

      m3 = _mm256_slli_si256(m2, 4);
      m4 = _mm256_and_si256(m2, mask2);
      m4 = _mm256_permute2f128_si256(m4, m4, 1);
      m4 = _mm256_srli_si256(
          m4, 12); // shift 6 short int positions or 12 char positions
      m2 = _mm256_add_epi16(m3, m4);

      output_odd = _mm256_add_epi16(output_odd, m2);

      // now division follows
      output_even = division(division_case, output_even, f);
      output_odd = division(division_case, output_odd, f);

      // shift odd 1 position and add to even
      output_odd = _mm256_slli_si256(output_odd, 1);
      output_even = _mm256_add_epi8(output_even, output_odd);

      _mm256_storeu_si256((__m256i *)&filt[row][col], output_even);
    }
  }

  loop_reminder_5x5_16_blur_X(frame1, filt, temp, N, M, row, col,
                              REMINDER_ITERATIONS_X, division_case,
                              mask_vector_x, f, divisor_xy, kernel_x);
}

void loop_reminder_5x5_16_blur_Y(unsigned char **frame1, unsigned char **filt,
                                 unsigned char *temp, const unsigned int N,
                                 const unsigned int M, const unsigned int row,
                                 const unsigned int REMINDER_ITERATIONS_Y,
                                 const unsigned int division_case,
                                 const signed char mask_vector_y[][32],
                                 const __m256i f,
                                 const unsigned short int divisor_xy) {

  __m256i r0, r1, r2, r3, r4, m0, m1, m2, m3, m4, even, odd, reminder_mask;

  if (REMINDER_ITERATIONS_Y == 0) { // no need for computing the y mask
    _mm256_store_si256((__m256i *)&temp[M], _mm256_setzero_si256());
    for (unsigned int g = M + 32; g < M + 32 + 3; g++)
      temp[g] = 0;
  } else {
    __m256i cy0, cy1, cy2, cy3, cy4, cy0_sh1, cy1_sh1, cy2_sh1, cy3_sh1,
        cy4_sh1;

    unsigned int col2 =
        M - REMINDER_ITERATIONS_Y; // this is the col index to load the elements
                                   // for the y mask

    if ((row > 1) && (row < N - 2)) { // main case

      r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 2][col2]);
      r1 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col2]);
      r2 = _mm256_loadu_si256((__m256i *)&frame1[row - 0][col2]);
      r3 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col2]);
      r4 = _mm256_loadu_si256((__m256i *)&frame1[row + 2][col2]);

      cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
      cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
      cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
      cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
      cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);

      cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
      cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
      cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
      cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
      cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);

    } else {
      if (row == 0) {
        r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col2]);
        r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col2]);
        r3 = _mm256_setzero_si256();
        r4 = _mm256_setzero_si256();

        cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy3 = _mm256_setzero_si256();
        cy4 = _mm256_setzero_si256();

        cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy3_sh1 = _mm256_setzero_si256();
        cy4_sh1 = _mm256_setzero_si256();

      } else if (row == 1) {
        r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col2]);
        r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[2][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[3][col2]);
        r4 = _mm256_setzero_si256();

        cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
        cy4 = _mm256_setzero_si256();

        cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
        cy4_sh1 = _mm256_setzero_si256();
      }

      else if (row == N - 2) {
        r0 = _mm256_setzero_si256();
        r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 4][col2]);
        r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col2]);

        cy0 = _mm256_setzero_si256();
        cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);

        cy0_sh1 = _mm256_setzero_si256();
        cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
      } else if (row == N - 1) {
        r0 = _mm256_setzero_si256();
        r1 = _mm256_setzero_si256();
        r2 = _mm256_loadu_si256((__m256i *)&frame1[N - 3][col2]);
        r3 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col2]);
        r4 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col2]);

        cy0 = _mm256_setzero_si256();
        cy1 = _mm256_setzero_si256();
        cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
        cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
        cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);

        cy0_sh1 = _mm256_setzero_si256();
        cy1_sh1 = _mm256_setzero_si256();
        cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
        cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
        cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
      } else {
        printf("\nsomething went wrong");
        exit(EXIT_FAILURE);
      }
    }

    /*
     * The above load operations load outside of the array bounds; these
     * elements are filled with zeros just after
     */
    reminder_mask = _mm256_load_si256(
        (__m256i *)&reminder_msk1[REMINDER_ITERATIONS_Y - 1][0]);

    // AND r0-r4 with reminder_mask
    r0 = _mm256_and_si256(r0, reminder_mask);
    r1 = _mm256_and_si256(r1, reminder_mask);
    r2 = _mm256_and_si256(r2, reminder_mask);
    r3 = _mm256_and_si256(r3, reminder_mask);
    r4 = _mm256_and_si256(r4, reminder_mask);

    //--------------------y filter begins--------------------

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0);
    m1 = _mm256_maddubs_epi16(r1, cy1);
    m2 = _mm256_maddubs_epi16(r2, cy2);
    m3 = _mm256_maddubs_epi16(r3, cy3);
    m4 = _mm256_maddubs_epi16(r4, cy4);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);

    even = division(division_case, m0, f); // even results

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
    m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
    m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
    m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
    m4 = _mm256_maddubs_epi16(r4, cy4_sh1);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);
    m0 = _mm256_add_epi16(m0, m3);
    m0 = _mm256_add_epi16(m0, m4);

    odd = division(division_case, m0, f); // odd results

    // pack to one register
    odd = _mm256_slli_si256(odd, 1); // shift one position right
    r0 = _mm256_add_epi8(even, odd); // add the odd with the even

    _mm256_storeu_si256((__m256i *)&temp[M - REMINDER_ITERATIONS_Y], r0);

    for (int gg = M; gg < M + 32 + 3; gg++)
      temp[gg] = 0;
    // y filter ends - r0 has now the data to be processed by x filter
  }
}

void loop_reminder_5x5_16_blur_X(
    unsigned char **frame1, unsigned char **filt, unsigned char *temp,
    const unsigned int N, const unsigned int M, const unsigned int row,
    const unsigned int col, const unsigned int REMINDER_ITERATIONS_X,
    const unsigned int division_case, const signed char mask_vector_x[][32],
    const __m256i f, const unsigned short int divisor_xy,
    signed char *kernel_x) {

  const __m256i cx0 = _mm256_load_si256((__m256i *)&mask_vector_x[0]);
  int newPixel = 0;
  const __m256i mask2 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0);
  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i output_mask = _mm256_set_epi16(
      0, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535, 0, 0, 65535);

  __m256i r0, m0, m1, m2, m3, m4, output1, output_even, output_odd;

  // 1st col iteration

  // multiply by the mask
  r0 = _mm256_loadu_si256((__m256i *)&temp[col - 2]);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // 1st hadd
  m1 = _mm256_srli_si256(m0, 2);
  m1 = _mm256_add_epi16(m0, m1);

  // 2nd hadd
  m2 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // after shifts the 15th element is lost as it cannot be propagated to the
  // 16th position (AVX registers are managed as two seperate SSE registers)
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_even = _mm256_and_si256(m2, output_mask);

  // 2nd col iteration

  // multiply by the mask
  r0 = _mm256_loadu_si256((__m256i *)&temp[col - 1]);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // 1st hadd
  m1 = _mm256_srli_si256(m0, 2);
  m1 = _mm256_add_epi16(m0, m1);

  // 2nd hadd
  m2 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // after shifts the 15th element is lost as it cannot be propagated to the
  // 16th position (AVX registers are managed as two seperate SSE registers)
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  output_odd = _mm256_and_si256(m2, output_mask);

  // 3rd col iteration

  // multiply by the mask
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 0]);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // 1st hadd
  m1 = _mm256_srli_si256(m0, 2);
  m1 = _mm256_add_epi16(m0, m1);

  // 2nd hadd
  m2 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // after shifts the 15th element is lost as it cannot be propagated to the
  // 16th position (AVX registers are managed as two seperate SSE registers)
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);

  m2 = _mm256_slli_si256(m2, 2);
  output_even = _mm256_add_epi16(output_even, m2);

  // 4th col iteration

  // multiply by the mask
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // 1st hadd
  m1 = _mm256_srli_si256(m0, 2);
  m1 = _mm256_add_epi16(m0, m1);

  // 2nd hadd
  m2 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // after shifts the 15th element is lost as it cannot be propagated to the
  // 16th position (AVX registers are managed as two seperate SSE registers)
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);

  m2 = _mm256_slli_si256(m2, 2);
  output_odd = _mm256_add_epi16(output_odd, m2);

  // 5th col iteration

  // multiply by the mask
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // 1st hadd
  m1 = _mm256_srli_si256(m0, 2);
  m1 = _mm256_add_epi16(m0, m1);

  // 2nd hadd
  m2 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // after shifts the 15th element is lost as it cannot be propagated to the
  // 16th position (AVX registers are managed as two seperate SSE registers)
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);

  m3 = _mm256_slli_si256(m2, 4);
  m4 = _mm256_and_si256(m2, mask2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_srli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m3, m4);

  output_even = _mm256_add_epi16(output_even, m2);

  // 6th col iteration

  // multiply by the mask
  r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, cx0);

  // 1st hadd
  m1 = _mm256_srli_si256(m0, 2);
  m1 = _mm256_add_epi16(m0, m1);

  // 2nd hadd
  m2 = _mm256_srli_si256(m0, 4);
  m2 = _mm256_add_epi16(m1, m2);

  // after shifts the 15th element is lost as it cannot be propagated to the
  // 16th position (AVX registers are managed as two seperate SSE registers)
  m4 = _mm256_and_si256(m0, mask3);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_slli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m2, m4);

  // m2 has 16 16bit values now. the results I need are in positions 0,3,6,9,12.
  // keep only those, discard others
  m2 = _mm256_and_si256(m2, output_mask);

  m3 = _mm256_slli_si256(m2, 4);
  m4 = _mm256_and_si256(m2, mask2);
  m4 = _mm256_permute2f128_si256(m4, m4, 1);
  m4 = _mm256_srli_si256(
      m4, 12); // shift 6 short int positions or 12 char positions
  m2 = _mm256_add_epi16(m3, m4);

  output_odd = _mm256_add_epi16(output_odd, m2);

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output1 = _mm256_add_epi8(output_even, output_odd);

  switch (REMINDER_ITERATIONS_X) {
  case 1:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    break;
  case 2:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    break;
  case 3:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    break;
  case 4:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    break;
  case 5:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    break;
  case 6:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    break;
  case 7:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    break;
  case 8:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    break;
  case 9:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    break;
  case 10:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    break;
  case 11:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    break;
  case 12:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    break;
  case 13:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output1, 12);
    break;
  case 14:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output1, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output1, 13);
    break;
  case 15:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output1, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output1, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output1, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output1, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output1, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output1, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output1, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output1, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output1, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output1, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output1, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output1, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output1, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output1, 13);
    filt[row][col + 14] = (unsigned char)_mm256_extract_epi8(output1, 14);
    break;
  case 16:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    break;
  case 17:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    break;
  case 18:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    break;
  case 19:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    break;
  case 20:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    break;
  case 21:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    break;
  case 22:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    break;
  case 23:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    break;
  case 24:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    break;
  case 25:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    break;
  case 26:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    break;
  case 27:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    break;
  case 28:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    break;
  case 29:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output1, 28);
    break;
  case 30:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output1, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output1, 29);
    break;
  case 31:
    _mm_storeu_si128(
        (__m128i *)&filt[row][col],
        _mm256_extractf128_si256(output1, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output1, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output1, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output1, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output1, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output1, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output1, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output1, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output1, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output1, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output1, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output1, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output1, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output1, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output1, 29);

    newPixel = 0;

    newPixel += temp[col + 30 - 2] * kernel_x[0];
    newPixel += temp[col + 30 - 1] * kernel_x[1];
    newPixel += temp[col + 30 - 0] * kernel_x[2];

    filt[row][col + 30] = (unsigned char)(newPixel / divisor_xy);

    break;
  default:
    printf("\nsomething went wrong");
    exit(EXIT_FAILURE);
  }
}

void Gaussian_Blur_3x3_16_more_load(unsigned char **frame1,
                                    unsigned char **filt, const unsigned int M,
                                    const unsigned int N,
                                    const unsigned short int divisor,
                                    signed char **filter) {

  const signed char f00 = filter[0][0];
  const signed char f01 = filter[0][1];
  const signed char f02 = filter[0][2];

  const signed char f10 = filter[1][0];
  const signed char f11 = filter[1][1];
  const signed char f12 = filter[1][2];

  const __m256i c0 = _mm256_set_epi8(
      0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0,
      f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00, 0, f02, f01, f00);
  const __m256i c1 = _mm256_set_epi8(
      0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0,
      f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10, 0, f12, f11, f10);

  // const __m256i mask3  =
  // _mm256_set_epi16(0,0,0,0,0,0,0,65535,0,0,0,0,0,0,0,0);
  const __m256i mask_prelude =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i output_mask =
      _mm256_set_epi16(0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0, 65535);
  // const __m256i output_mask_sh1  =
  // _mm256_set_epi16(0,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0,65535,0);

  // REMINDER_ITERATIONS value ranges from 2 to 33
  const unsigned int REMINDER_ITERATIONS =
      (M - ((((M - 32 - 2) / 32) * 32) + 32)); // M-(last_col_value+32)
  // printf("\n%d",REMINDER_ITERATIONS);

  const unsigned int division_case =
      prepare_for_division(divisor); // determine the division case (A, B or C)
  const __m256i f = _mm256_load_si256(
      (__m256i *)&f_vector[0]); // initialize the division vector

  // printf("\n%d %d",division_case,b);

#pragma omp parallel
  {

    unsigned int row, col;
    register __m256i r0, r1, r2, m0, m1, m2, rr0, rr1, rr2;
    //__m256i output_row0_even,output_row1_even,output_row0_odd,output_row1_odd;
    ////these are for processing row #0 and #1 only.
    __m256i output_even, output_odd;
    //__m256i m0_prelude_row0,m0_prelude_row1;

    /*---------------------- Gaussian Blur ---------------------------------*/

#pragma omp for schedule(static)
    for (row = 0; row < N; row++) {

      if (row == 0) { // special case compute filt[0:1][:]

        for (col = 0; col <= M - 32 - 2; col += 32) {

          if (col == 0) {

            // load the 2 rows only
            r0 = _mm256_load_si256((__m256i *)&frame1[0][0]);
            r1 = _mm256_load_si256((__m256i *)&frame1[1][0]);

            // START - extra code needed for prelude
            m0 = _mm256_slli_si256(r0,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m1 = _mm256_slli_si256(r1,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning

            // preserve the element being lost because of the shift above
            r0 = _mm256_and_si256(r0, mask_prelude);
            r0 = _mm256_permute2f128_si256(r0, r0, 1);
            r0 = _mm256_srli_si256(r0, 15); // shift 15 elements
            r0 = _mm256_add_epi16(m0, r0);

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 15); // shift 15 elements
            r1 = _mm256_add_epi16(m1, r1);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c1);
            m1 = _mm256_maddubs_epi16(r1, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_even = _mm256_and_si256(m2, output_mask);

            // 2ND col iteration

            // load the 3 rows
            r0 = _mm256_load_si256((__m256i *)&frame1[0][0]);
            r1 = _mm256_load_si256((__m256i *)&frame1[1][0]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c1);
            m1 = _mm256_maddubs_epi16(r1, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_odd = _mm256_and_si256(m2, output_mask);

            // 3rd col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[0][1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[1][1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c1);
            m1 = _mm256_maddubs_epi16(r1, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_even = _mm256_add_epi16(output_even, m2);

            // 4th col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[0][2]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[1][2]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c1);
            m1 = _mm256_maddubs_epi16(r1, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_odd = _mm256_add_epi16(output_odd, m2);

            // now division follows
            output_even = division(division_case, output_even, f);
            output_odd = division(division_case, output_odd, f);

            // shift odd 1 position and add to even
            output_odd = _mm256_slli_si256(output_odd, 1);
            output_even = _mm256_add_epi8(output_even, output_odd);

            _mm256_store_si256((__m256i *)&filt[0][0], output_even);
            // END - extra code needed for prelude
          } else {

            // col iteration computes output pixels of    1,5,9,13,17,21,25,29
            //  col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
            //  col+2 iteration computes output pixels of 3,7,11,15,19,23,27
            //  col+3 iteration computes output pixels of 4,8,12,16,20,24,28
            // afterwards, col becomes 30 and repeat the above process

            // 1st col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col - 1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col - 1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c1);
            m1 = _mm256_maddubs_epi16(r1, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_even = _mm256_and_si256(m2, output_mask);

            // 2ND col iteration

            // load the 3 rows
            r0 = _mm256_load_si256(
                (__m256i *)&frame1[0][col]); // these loads are always aligned
            r1 = _mm256_load_si256((__m256i *)&frame1[1][col]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c1);
            m1 = _mm256_maddubs_epi16(r1, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_odd = _mm256_and_si256(m2, output_mask);

            // 3rd col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col + 1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col + 1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c1);
            m1 = _mm256_maddubs_epi16(r1, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_even = _mm256_add_epi16(output_even, m2);

            // 4th col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col + 2]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col + 2]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c1);
            m1 = _mm256_maddubs_epi16(r1, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_odd = _mm256_add_epi16(output_odd, m2);

            // now division follows
            output_even = division(division_case, output_even, f);
            output_odd = division(division_case, output_odd, f);

            // shift odd 1 position and add to even
            output_odd = _mm256_slli_si256(output_odd, 1);
            output_even = _mm256_add_epi8(output_even, output_odd);

            _mm256_store_si256((__m256i *)&filt[0][col], output_even);
          }
        }
        loop_reminder_3x3_new_first_last_rows(
            frame1, filt, M, N, 0, col, REMINDER_ITERATIONS, division_case,
            divisor, filter, c0, c1, f);

      }

      else if (row == N - 1) { // special case compute filt[N-2:N-1][:]
        for (col = 0; col <= M - 32 - 2; col += 32) {

          if (col == 0) {

            // load the 3 rows
            r0 = _mm256_load_si256((__m256i *)&frame1[N - 2][0]);
            r1 = _mm256_load_si256((__m256i *)&frame1[N - 1][0]);

            // START - extra code needed for prelude
            m0 = _mm256_slli_si256(r0,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m1 = _mm256_slli_si256(r1,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning

            // preserve the element being lost because of the shift above
            r0 = _mm256_and_si256(r0, mask_prelude);
            r0 = _mm256_permute2f128_si256(r0, r0, 1);
            r0 = _mm256_srli_si256(r0, 15); // shift 15 elements
            r0 = _mm256_add_epi16(m0, r0);

            r1 = _mm256_and_si256(r1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 15); // shift 15 elements
            r1 = _mm256_add_epi16(m1, r1);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_even = _mm256_and_si256(m2, output_mask);

            // 2ND col iteration

            // load the 3 rows
            r0 = _mm256_load_si256((__m256i *)&frame1[N - 2][0]);
            r1 = _mm256_load_si256((__m256i *)&frame1[N - 1][0]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_odd = _mm256_and_si256(m2, output_mask);

            // 3rd col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_even = _mm256_add_epi16(output_even, m2);

            // 4th col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][2]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][2]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_odd = _mm256_add_epi16(output_odd, m2);

            // now division follows
            output_even = division(division_case, output_even, f);
            output_odd = division(division_case, output_odd, f);

            // shift odd 1 position and add to even
            output_odd = _mm256_slli_si256(output_odd, 1);
            output_even = _mm256_add_epi8(output_even, output_odd);

            _mm256_store_si256((__m256i *)&filt[N - 1][0], output_even);
            // END - extra code needed for prelude
          } else {

            // col iteration computes output pixels of    1,5,9,13,17,21,25,29
            //  col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
            //  col+2 iteration computes output pixels of 3,7,11,15,19,23,27
            //  col+3 iteration computes output pixels of 4,8,12,16,20,24,28
            // afterwards, col becomes 30 and repeat the above process

            // 1st col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_even = _mm256_and_si256(m2, output_mask);

            // 2ND col iteration

            // load the 3 rows
            r0 = _mm256_load_si256(
                (__m256i *)&frame1[N - 2]
                                  [col]); // these loads are always aligned
            r1 = _mm256_load_si256((__m256i *)&frame1[N - 1][col]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_odd = _mm256_and_si256(m2, output_mask);

            // 3rd col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col + 1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col + 1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_even = _mm256_add_epi16(output_even, m2);

            // 4th col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col + 2]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col + 2]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_odd = _mm256_add_epi16(output_odd, m2);

            // now division follows
            output_even = division(division_case, output_even, f);
            output_odd = division(division_case, output_odd, f);

            // shift odd 1 position and add to even
            output_odd = _mm256_slli_si256(output_odd, 1);
            output_even = _mm256_add_epi8(output_even, output_odd);

            _mm256_store_si256((__m256i *)&filt[N - 1][col], output_even);
          }
        }
        loop_reminder_3x3_new_first_last_rows(
            frame1, filt, M, N, N - 1, col, REMINDER_ITERATIONS, division_case,
            divisor, filter, c0, c1, f);

      }

      else { // main loop

        for (col = 0; col <= M - 32 - 2; col += 32) {

          if (col == 0) {

            // load the 3 rows
            rr0 = _mm256_load_si256((__m256i *)&frame1[row - 1][0]);
            rr1 = _mm256_load_si256((__m256i *)&frame1[row][0]);
            rr2 = _mm256_load_si256((__m256i *)&frame1[row + 1][0]);

            // START - extra code needed for prelude
            m0 = _mm256_slli_si256(rr0,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m1 = _mm256_slli_si256(rr1,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning
            m2 = _mm256_slli_si256(rr2,
                                   1); // shift 1 elements left - equivalent to
                                       // filling with one zero inthe beginning

            // preserve the element being lost because of the shift above
            r0 = _mm256_and_si256(rr0, mask_prelude);
            r0 = _mm256_permute2f128_si256(r0, r0, 1);
            r0 = _mm256_srli_si256(r0, 15); // shift 15 elements
            r0 = _mm256_add_epi16(m0, r0);

            r1 = _mm256_and_si256(rr1, mask_prelude);
            r1 = _mm256_permute2f128_si256(r1, r1, 1);
            r1 = _mm256_srli_si256(r1, 15); // shift 15 elements
            r1 = _mm256_add_epi16(m1, r1);

            r2 = _mm256_and_si256(rr2, mask_prelude);
            r2 = _mm256_permute2f128_si256(r2, r2, 1);
            r2 = _mm256_srli_si256(r2, 15); // shift 15 elements
            r2 = _mm256_add_epi16(m2, r2);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);
            m2 = _mm256_maddubs_epi16(r2, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);
            m0 = _mm256_add_epi16(m0, m2);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_even = _mm256_and_si256(m2, output_mask);

            // 2ND col iteration

            // load the 3 rows
            // r0=_mm256_load_si256( (__m256i *) &frame1[row-1][0]);
            // r1=_mm256_load_si256( (__m256i *) &frame1[row][0]);
            // r2=_mm256_load_si256( (__m256i *) &frame1[row+1][0]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(rr0, c0);
            m1 = _mm256_maddubs_epi16(rr1, c1);
            m2 = _mm256_maddubs_epi16(rr2, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);
            m0 = _mm256_add_epi16(m0, m2);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_odd = _mm256_and_si256(m2, output_mask);

            // 3rd col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[row][1]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);
            m2 = _mm256_maddubs_epi16(r2, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);
            m0 = _mm256_add_epi16(m0, m2);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_even = _mm256_add_epi16(output_even, m2);

            // 4th col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][2]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[row][2]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][2]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);
            m2 = _mm256_maddubs_epi16(r2, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);
            m0 = _mm256_add_epi16(m0, m2);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_odd = _mm256_add_epi16(output_odd, m2);

            // now division follows
            output_even = division(division_case, output_even, f);
            output_odd = division(division_case, output_odd, f);

            // shift odd 1 position and add to even
            output_odd = _mm256_slli_si256(output_odd, 1);
            output_even = _mm256_add_epi8(output_even, output_odd);

            _mm256_store_si256((__m256i *)&filt[row][0], output_even);
            // END - extra code needed for prelude
          } else {

            // col iteration computes output pixels of    1,5,9,13,17,21,25,29
            //  col+1 iteration computes output pixels of 2,6,10,14,18,22,26,30
            //  col+2 iteration computes output pixels of 3,7,11,15,19,23,27
            //  col+3 iteration computes output pixels of 4,8,12,16,20,24,28
            // afterwards, col becomes 30 and repeat the above process

            // 1st col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col - 1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[row][col - 1]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col - 1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);
            m2 = _mm256_maddubs_epi16(r2, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);
            m0 = _mm256_add_epi16(m0, m2);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_even = _mm256_and_si256(m2, output_mask);

            // 2ND col iteration

            // load the 3 rows
            r0 = _mm256_load_si256(
                (__m256i *)&frame1[row - 1]
                                  [col]); // these loads are always aligned
            r1 = _mm256_load_si256((__m256i *)&frame1[row][col]);
            r2 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);
            m2 = _mm256_maddubs_epi16(r2, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);
            m0 = _mm256_add_epi16(m0, m2);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            output_odd = _mm256_and_si256(m2, output_mask);

            // 3rd col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col + 1]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[row][col + 1]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col + 1]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);
            m2 = _mm256_maddubs_epi16(r2, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);
            m0 = _mm256_add_epi16(m0, m2);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_even = _mm256_add_epi16(output_even, m2);

            // 4th col iteration

            // load the 3 rows
            r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col + 2]);
            r1 = _mm256_loadu_si256((__m256i *)&frame1[row][col + 2]);
            r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col + 2]);

            // multiply with the mask
            m0 = _mm256_maddubs_epi16(r0, c0);
            m1 = _mm256_maddubs_epi16(r1, c1);
            m2 = _mm256_maddubs_epi16(r2, c0);

            // vertical add
            m0 = _mm256_add_epi16(m0, m1);
            m0 = _mm256_add_epi16(m0, m2);

            // hozizontal additions
            m1 = _mm256_srli_si256(m0, 2);
            m2 = _mm256_add_epi16(m1, m0);

            // m2 has 16 16bit values now. the results I need are in positions
            // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those,
            // discard others. later on they will be stored into
            // 0,4,8,12,16,20,24,28 positions (8bit register).
            m2 = _mm256_and_si256(m2, output_mask);
            m2 = _mm256_slli_si256(m2, 2);
            output_odd = _mm256_add_epi16(output_odd, m2);

            // now division follows
            output_even = division(division_case, output_even, f);
            output_odd = division(division_case, output_odd, f);

            // shift odd 1 position and add to even
            output_odd = _mm256_slli_si256(output_odd, 1);
            output_even = _mm256_add_epi8(output_even, output_odd);

            _mm256_store_si256((__m256i *)&filt[row][col], output_even);
          }
        }
        loop_reminder_3x3_new(frame1, filt, M, N, row, col, REMINDER_ITERATIONS,
                              division_case, divisor, filter, c0, c1, f);
      }
    }

  } // end of parallel
}

void Gaussian_Blur_7x7_16_separable(unsigned char **frame1,
                                    unsigned char **filt, const unsigned int M,
                                    const unsigned int N, signed char *kernel_y,
                                    signed char *kernel_x,
                                    const unsigned short int divisor_xy) {

  const signed char fx0 = kernel_x[0];
  const signed char fx1 = kernel_x[1];
  const signed char fx2 = kernel_x[2];
  const signed char fx3 = kernel_x[3];
  const signed char fx4 = kernel_x[4];
  const signed char fx5 = kernel_x[5];
  const signed char fx6 = kernel_x[6];

  const signed char fy0 = kernel_y[0];
  const signed char fy1 = kernel_y[1];
  const signed char fy2 = kernel_y[2];
  const signed char fy3 = kernel_y[3];
  const signed char fy4 = kernel_y[4];
  const signed char fy5 = kernel_y[5];
  const signed char fy6 = kernel_y[6];

  const signed char mask_vector_x[1][32] __attribute__((aligned(64))) = {
      {fx0, fx1, fx2, fx3, fx4, fx5, fx6, 0,   fx0, fx1, fx2,
       fx3, fx4, fx5, fx6, 0,   fx0, fx1, fx2, fx3, fx4, fx5,
       fx6, 0,   fx0, fx1, fx2, fx3, fx4, fx5, fx6, 0},
  };

  const signed char mask_vector_y[14][32] __attribute__((aligned(64))) = {
      {fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0,
       fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0},
      {fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0,
       fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0},
      {fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0,
       fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0},
      {fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0,
       fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0},
      {fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0,
       fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0},
      {fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0,
       fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0},
      {fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0,
       fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0},

      {0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0,
       0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0},
      {0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1,
       0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1},
      {0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2,
       0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2},
      {0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3,
       0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3},
      {0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4,
       0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4},
      {0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5,
       0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5},
      {0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6,
       0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6},
  };

  const __m256i cx0 = _mm256_load_si256((__m256i *)&mask_vector_x[0]);

  const __m256i cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
  const __m256i cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
  const __m256i cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
  const __m256i cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
  const __m256i cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
  const __m256i cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
  const __m256i cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);

  const __m256i cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
  const __m256i cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);
  const __m256i cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
  const __m256i cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
  const __m256i cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
  const __m256i cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
  const __m256i cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);

  const __m256i mask_prelude =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i mask_16_1 = _mm256_set_epi16(0, 0, 0, 0xffff, 0, 0, 0, 0xffff,
                                             0, 0, 0, 0xffff, 0, 0, 0, 0xffff);

  const __m256i mask_prelude_3 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude_2 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude_1 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const unsigned int REMINDER_ITERATIONS_XY_mask =
      M - ((((M - 32) / 32) * 32) + 32);

  // printf("\nREMINDER_ITERATIONS=%u",REMINDER_ITERATIONS_XY_mask);

  const unsigned int division_case = prepare_for_division(
      divisor_xy); // determine which is the division case (A, B or C)
  const __m256i f = _mm256_load_si256(
      (__m256i *)&f_vector[0]); // initialize the division vector
                                // const __m256i ones=_mm256_set1_epi16(1);

  //	const unsigned int division_case_x=prepare_for_division_32(divisor_x);
  ////determine which is the division case (A, B or C) 	const __m256i f_x =
  //_mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division
  // vector printf("\n%d %d",division_case,b);

#pragma omp parallel
  {

    unsigned int row, col;
    __m256i r0, r1, r2, r3, r4, r5, r6, m0, m1, m2, m3, m4, m5, m6, even, odd;
    unsigned char temp[M + 32 + 4] __attribute__((
        aligned(64))); // temporal storage of the output of y filter.

    /*---------------------- Gaussian Blur ---------------------------------*/

#pragma omp for schedule(static)
    for (row = 2; row < N - 2; row++) {

      if (row < 3) {
        prelude_7x7_16_Ymask_0_new(0, M, frame1, temp, mask_vector_y,
                                   division_case, f, mask_prelude);
        loop_reminder_7x7_16_blur_Y(frame1, filt, temp, N, M, 0,
                                    REMINDER_ITERATIONS_XY_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_7x7_16_Xmask_new(frame1, filt, temp, N, M, 0, mask_vector_x,
                                 division_case, f, REMINDER_ITERATIONS_XY_mask,
                                 mask_vector_y, divisor_xy, kernel_x);

        prelude_7x7_16_Ymask_1_new(0, M, frame1, temp, mask_vector_y,
                                   division_case, f, mask_prelude);
        loop_reminder_7x7_16_blur_Y(frame1, filt, temp, N, M, 1,
                                    REMINDER_ITERATIONS_XY_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_7x7_16_Xmask_new(frame1, filt, temp, N, M, 1, mask_vector_x,
                                 division_case, f, REMINDER_ITERATIONS_XY_mask,
                                 mask_vector_y, divisor_xy, kernel_x);

        prelude_7x7_16_Ymask_2_new(0, M, frame1, temp, mask_vector_y,
                                   division_case, f, mask_prelude);
        loop_reminder_7x7_16_blur_Y(frame1, filt, temp, N, M, 2,
                                    REMINDER_ITERATIONS_XY_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_7x7_16_Xmask_new(frame1, filt, temp, N, M, 2, mask_vector_x,
                                 division_case, f, REMINDER_ITERATIONS_XY_mask,
                                 mask_vector_y, divisor_xy, kernel_x);

      }

      else if (row > N - 4) {

        prelude_7x7_16_Ymask_0_new(N - 4, M, frame1, temp, mask_vector_y,
                                   division_case, f, mask_prelude);
        loop_reminder_7x7_16_blur_Y(frame1, filt, temp, N, M, N - 1,
                                    REMINDER_ITERATIONS_XY_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_7x7_16_Xmask_new(frame1, filt, temp, N, M, N - 1, mask_vector_x,
                                 division_case, f, REMINDER_ITERATIONS_XY_mask,
                                 mask_vector_y, divisor_xy, kernel_x);

        prelude_7x7_16_Ymask_1_new(N - 5, M, frame1, temp, mask_vector_y,
                                   division_case, f, mask_prelude);
        loop_reminder_7x7_16_blur_Y(frame1, filt, temp, N, M, N - 2,
                                    REMINDER_ITERATIONS_XY_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_7x7_16_Xmask_new(frame1, filt, temp, N, M, N - 2, mask_vector_x,
                                 division_case, f, REMINDER_ITERATIONS_XY_mask,
                                 mask_vector_y, divisor_xy, kernel_x);

        prelude_7x7_16_Ymask_2_new(N - 6, M, frame1, temp, mask_vector_y,
                                   division_case, f, mask_prelude);
        loop_reminder_7x7_16_blur_Y(frame1, filt, temp, N, M, N - 3,
                                    REMINDER_ITERATIONS_XY_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_7x7_16_Xmask_new(frame1, filt, temp, N, M, N - 3, mask_vector_x,
                                 division_case, f, REMINDER_ITERATIONS_XY_mask,
                                 mask_vector_y, divisor_xy, kernel_x);

      }

      else { // main loop

        for (col = 0; col <= M - 32; col += 32) {

          // load the 7 rows
          r0 = _mm256_load_si256((__m256i *)&frame1[row - 3][col]);
          r1 = _mm256_load_si256((__m256i *)&frame1[row - 2][col]);
          r2 = _mm256_load_si256((__m256i *)&frame1[row - 1][col]);
          r3 = _mm256_load_si256((__m256i *)&frame1[row][col]);
          r4 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
          r5 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);
          r6 = _mm256_load_si256((__m256i *)&frame1[row + 3][col]);

          //--------------------y filter begins--------------------

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, cy0);
          m1 = _mm256_maddubs_epi16(r1, cy1);
          m2 = _mm256_maddubs_epi16(r2, cy2);
          m3 = _mm256_maddubs_epi16(r3, cy3);
          m4 = _mm256_maddubs_epi16(r4, cy4);
          m5 = _mm256_maddubs_epi16(r5, cy5);
          m6 = _mm256_maddubs_epi16(r6, cy6);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);
          m0 = _mm256_add_epi16(m0, m5);
          m0 = _mm256_add_epi16(m0, m6);

          even = division(division_case, m0, f); // even results

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
          m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
          m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
          m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
          m4 = _mm256_maddubs_epi16(r4, cy4_sh1);
          m5 = _mm256_maddubs_epi16(r5, cy5_sh1);
          m6 = _mm256_maddubs_epi16(r6, cy6_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);
          m0 = _mm256_add_epi16(m0, m5);
          m0 = _mm256_add_epi16(m0, m6);

          odd = division(division_case, m0, f); // odd results

          // pack to one register
          odd = _mm256_slli_si256(odd, 1); // shift one position right
          r0 = _mm256_add_epi8(even, odd); // add the odd with the even
          // y filter ends - r0 has now the data to be processed by x filter

          _mm256_store_si256((__m256i *)&temp[col], r0);
        }
        loop_reminder_7x7_16_blur_Y(frame1, filt, temp, N, M, row,
                                    REMINDER_ITERATIONS_XY_mask, division_case,
                                    mask_vector_y, f, divisor_xy);

        for (col = 0; col <= M - 32; col += 32) {

          if (col == 0) {

            // 1st col iteration
            // multiply by the mask
            __m256i rr0 = _mm256_load_si256((__m256i *)&temp[0]);
            r0 = fill_3zeros(rr0, mask_prelude_3);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            even = _mm256_and_si256(
                m0, mask_16_1); // in 16-bit format keep only 0,4,8,12 and
                                // discard the others

            // 2nd col iteration
            // r0=_mm256_load_si256( (__m256i *) &temp[0]);
            r0 = fill_2zeros(rr0, mask_prelude_2);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            odd = _mm256_and_si256(
                m0, mask_16_1); // in 16-bit format keep only 0,4,8,12 and
                                // discard the others

            // 3rd col iteration
            // r0=_mm256_load_si256( (__m256i *) &temp[0]);
            r0 = fill_1zeros(rr0, mask_prelude_1);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE even
            m0 = _mm256_slli_si256(m0, 2);
            even = _mm256_add_epi16(m0, even);

            // 4th col iteration
            // r0=_mm256_load_si256( (__m256i *) &temp[0]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(rr0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE odd
            m0 = _mm256_slli_si256(m0, 2);
            odd = _mm256_add_epi16(m0, odd);

            // 5th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[1]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE even
            m0 = _mm256_slli_si256(m0, 4);
            even = _mm256_add_epi16(m0, even);

            // 6th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[2]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE odd
            m0 = _mm256_slli_si256(m0, 4);
            odd = _mm256_add_epi16(m0, odd);

            // 7th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[3]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE even
            m0 = _mm256_slli_si256(m0, 6);
            even = _mm256_add_epi16(m0, even);

            // 8th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[4]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE odd
            m0 = _mm256_slli_si256(m0, 6);
            odd = _mm256_add_epi16(m0, odd);

            even = division(division_case, even, f);
            odd = division(division_case, odd, f);

            odd = _mm256_slli_si256(odd, 1);
            even = _mm256_add_epi8(even, odd);

            _mm256_store_si256((__m256i *)&filt[row][0], even);

          } else {

            // col iteration computes output pixels of   0,8,16,24
            // col+1 iteration computes output pixels of 1,9,17,25
            // col+2 iteration computes output pixels of 2,10,18,26
            // col+3 iteration computes output pixels of 3,11,19,27
            // col+4 iteration computes output pixels of 4,12,20,28
            // col+5 iteration computes output pixels of 5,13,21,29
            // col+6 iteration computes output pixels of 6,14,22,30
            // col+7 iteration computes output pixels of 7,15,23,31
            // afterwards, col becomes 32 and repeat the above process

            // 1st col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col - 3]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            even = _mm256_and_si256(
                m0, mask_16_1); // in 16-bit format keep only 0,4,8,12 and
                                // discard the others

            // 2nd col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col - 2]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            odd = _mm256_and_si256(
                m0, mask_16_1); // in 16-bit format keep only 0,4,8,12 and
                                // discard the others

            // 3rd col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col - 1]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE even
            m0 = _mm256_slli_si256(m0, 2);
            even = _mm256_add_epi16(m0, even);

            // 4th col iteration
            r0 = _mm256_load_si256((__m256i *)&temp[col]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE odd
            m0 = _mm256_slli_si256(m0, 2);
            odd = _mm256_add_epi16(m0, odd);

            // 5th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE even
            m0 = _mm256_slli_si256(m0, 4);
            even = _mm256_add_epi16(m0, even);

            // 6th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE odd
            m0 = _mm256_slli_si256(m0, 4);
            odd = _mm256_add_epi16(m0, odd);

            // 7th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE even
            m0 = _mm256_slli_si256(m0, 6);
            even = _mm256_add_epi16(m0, even);

            // 8th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 4]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add
            m1 = _mm256_srli_si256(m0, 2);
            m0 = _mm256_add_epi16(m0, m1); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m0, 4);
            m0 = _mm256_add_epi16(m0, m1); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,4,8,12 and discard the others

            // MERGE odd
            m0 = _mm256_slli_si256(m0, 6);
            odd = _mm256_add_epi16(m0, odd);

            even = division(division_case, even, f);
            odd = division(division_case, odd, f);

            odd = _mm256_slli_si256(odd, 1);
            even = _mm256_add_epi8(even, odd);

            _mm256_store_si256((__m256i *)&filt[row][col], even);
          }
        }

        loop_reminder_7x7_16_blur_X(frame1, filt, temp, N, M, row, col,
                                    REMINDER_ITERATIONS_XY_mask, division_case,
                                    mask_vector_x, f, divisor_xy, kernel_x);
      }
    }

  } // end of parallel
}

void Gaussian_Blur_9x9_16_separable(unsigned char **frame1,
                                    unsigned char **filt, const unsigned int M,
                                    const unsigned int N, signed char *kernel_y,
                                    signed char *kernel_x,
                                    const unsigned short int divisor_xy) {

  const signed char fx0 = kernel_x[0];
  const signed char fx1 = kernel_x[1];
  const signed char fx2 = kernel_x[2];
  const signed char fx3 = kernel_x[3];
  const signed char fx4 = kernel_x[4];
  const signed char fx5 = kernel_x[5];
  const signed char fx6 = kernel_x[6];
  const signed char fx7 = kernel_x[7];
  const signed char fx8 = kernel_x[8];

  const signed char fy0 = kernel_y[0];
  const signed char fy1 = kernel_y[1];
  const signed char fy2 = kernel_y[2];
  const signed char fy3 = kernel_y[3];
  const signed char fy4 = kernel_y[4];
  const signed char fy5 = kernel_y[5];
  const signed char fy6 = kernel_y[6];
  const signed char fy7 = kernel_y[7];
  const signed char fy8 = kernel_y[8];

  const signed char mask_vector_x[10][32] __attribute__((aligned(64))) = {
      {fx0, fx1, fx2, fx3, fx4, fx5, fx6, fx7, fx8, 0,   fx0,
       fx1, fx2, fx3, fx4, fx5, fx6, fx7, fx8, 0,   fx0, fx1,
       fx2, fx3, fx4, fx5, fx6, fx7, fx8, 0,   0,   0}};

  const signed char mask_vector_y[18][32] __attribute__((aligned(64))) = {
      {fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0,
       fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0},
      {fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0,
       fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0},
      {fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0,
       fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0},
      {fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0,
       fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0},
      {fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0,
       fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0},
      {fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0,
       fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0},
      {fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0,
       fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0},
      {fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0,
       fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0},
      {fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0,
       fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0},

      {0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0,
       0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0, 0, fy0},
      {0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1,
       0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1, 0, fy1},
      {0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2,
       0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2, 0, fy2},
      {0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3,
       0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3, 0, fy3},
      {0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4,
       0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4, 0, fy4},
      {0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5,
       0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5, 0, fy5},
      {0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6,
       0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6, 0, fy6},
      {0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7,
       0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7, 0, fy7},
      {0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8,
       0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8, 0, fy8},
  };

  const __m256i cx0 = _mm256_load_si256((__m256i *)&mask_vector_x[0]);

  const __m256i cy0 = _mm256_load_si256((__m256i *)&mask_vector_y[0]);
  const __m256i cy1 = _mm256_load_si256((__m256i *)&mask_vector_y[1]);
  const __m256i cy2 = _mm256_load_si256((__m256i *)&mask_vector_y[2]);
  const __m256i cy3 = _mm256_load_si256((__m256i *)&mask_vector_y[3]);
  const __m256i cy4 = _mm256_load_si256((__m256i *)&mask_vector_y[4]);
  const __m256i cy5 = _mm256_load_si256((__m256i *)&mask_vector_y[5]);
  const __m256i cy6 = _mm256_load_si256((__m256i *)&mask_vector_y[6]);
  const __m256i cy7 = _mm256_load_si256((__m256i *)&mask_vector_y[7]);
  const __m256i cy8 = _mm256_load_si256((__m256i *)&mask_vector_y[8]);

  const __m256i cy0_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[9]);
  const __m256i cy1_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[10]);
  const __m256i cy2_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[11]);
  const __m256i cy3_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[12]);
  const __m256i cy4_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[13]);
  const __m256i cy5_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[14]);
  const __m256i cy6_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[15]);
  const __m256i cy7_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[16]);
  const __m256i cy8_sh1 = _mm256_load_si256((__m256i *)&mask_vector_y[17]);

  const __m256i mask3 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask3_2 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 65535, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i mask_prelude =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      255, 255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude_3 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      255, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude_2 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 255,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);
  const __m256i mask_prelude_1 =
      _mm256_set_epi8(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 255, 0, 0,
                      0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);

  const __m256i mask_16_1 = _mm256_set_epi16(0, 0, 0, 0, 0, 0xffff, 0, 0, 0, 0,
                                             0xffff, 0, 0, 0, 0, 0xffff);

  const __m256i mask_new_1 =
      _mm256_set_epi16(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0xffff, 0, 0, 0, 0, 0);

  const __m256i mask_16_5 = _mm256_set_epi16(0, 0xffff, 0, 0, 0, 0, 0xffff, 0,
                                             0, 0, 0, 0xffff, 0, 0, 0, 0);

  const unsigned int REMINDER_ITERATIONS_X_mask =
      M - ((((M - 32) / 30) * 30) + 30); // M-(last_col_value+30)
  const unsigned int REMINDER_ITERATIONS_Y_mask =
      M - ((((M - 32) / 32) * 32) + 32);

  // printf("\nREMINDER_ITERATIONS=%u,%u",REMINDER_ITERATIONS_X_mask,REMINDER_ITERATIONS_Y_mask);

  const unsigned int division_case = prepare_for_division(
      divisor_xy); // determine which is the division case (A, B or C)
  const __m256i f = _mm256_load_si256(
      (__m256i *)&f_vector[0]); // initialize the division vector
                                // const __m256i ones=_mm256_set1_epi16(1);

  //	const unsigned int division_case_x=prepare_for_division_32(divisor_x);
  ////determine which is the division case (A, B or C) 	const __m256i f_x =
  //_mm256_load_si256( (__m256i *) &f_vector_x[0]);// initialize the division
  // vector printf("\n%d ",division_case);

#pragma omp parallel
  {

    unsigned int row, col;
    __m256i r0, r1, r2, r3, r4, r5, r6, r7, r8, m0, m1, m2, m3, m4, m5, m6, m7,
        m8, even, odd;
    //__m256i output1,output2,output3,output4;
    unsigned char temp[M + 32 + 6] __attribute__((
        aligned(64))); // temporal storage of the output of y filter.

    /*---------------------- Gaussian Blur ---------------------------------*/

#pragma omp for schedule(static)
    for (row = 3; row < N - 3; row++) {

      if (row < 4) {
        prelude_9x9_16_Ymask_0(0, M, frame1, temp, mask_vector_y, division_case,
                               f, mask_prelude);
        loop_reminder_9x9_16_blur_Y(frame1, filt, temp, N, M, 0,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_9x9_16_Xmask(frame1, filt, temp, N, M, 0, mask_vector_x,
                             division_case, f, REMINDER_ITERATIONS_X_mask,
                             REMINDER_ITERATIONS_Y_mask, mask_vector_y,
                             divisor_xy, kernel_x);

        prelude_9x9_16_Ymask_1(0, M, frame1, temp, mask_vector_y, division_case,
                               f, mask_prelude);
        loop_reminder_9x9_16_blur_Y(frame1, filt, temp, N, M, 1,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_9x9_16_Xmask(frame1, filt, temp, N, M, 1, mask_vector_x,
                             division_case, f, REMINDER_ITERATIONS_X_mask,
                             REMINDER_ITERATIONS_Y_mask, mask_vector_y,
                             divisor_xy, kernel_x);

        prelude_9x9_16_Ymask_2(0, M, frame1, temp, mask_vector_y, division_case,
                               f, mask_prelude);
        loop_reminder_9x9_16_blur_Y(frame1, filt, temp, N, M, 2,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_9x9_16_Xmask(frame1, filt, temp, N, M, 2, mask_vector_x,
                             division_case, f, REMINDER_ITERATIONS_X_mask,
                             REMINDER_ITERATIONS_Y_mask, mask_vector_y,
                             divisor_xy, kernel_x);

        prelude_9x9_16_Ymask_3(0, M, frame1, temp, mask_vector_y, division_case,
                               f, mask_prelude);
        loop_reminder_9x9_16_blur_Y(frame1, filt, temp, N, M, 3,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_9x9_16_Xmask(frame1, filt, temp, N, M, 3, mask_vector_x,
                             division_case, f, REMINDER_ITERATIONS_X_mask,
                             REMINDER_ITERATIONS_Y_mask, mask_vector_y,
                             divisor_xy, kernel_x);
      }

      else if (row > N - 5) {

        prelude_9x9_16_Ymask_0(N - 5, M, frame1, temp, mask_vector_y,
                               division_case, f, mask_prelude);
        loop_reminder_9x9_16_blur_Y(frame1, filt, temp, N, M, N - 1,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_9x9_16_Xmask(frame1, filt, temp, N, M, N - 1, mask_vector_x,
                             division_case, f, REMINDER_ITERATIONS_X_mask,
                             REMINDER_ITERATIONS_Y_mask, mask_vector_y,
                             divisor_xy, kernel_x);

        prelude_9x9_16_Ymask_1(N - 6, M, frame1, temp, mask_vector_y,
                               division_case, f, mask_prelude);
        loop_reminder_9x9_16_blur_Y(frame1, filt, temp, N, M, N - 2,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_9x9_16_Xmask(frame1, filt, temp, N, M, N - 2, mask_vector_x,
                             division_case, f, REMINDER_ITERATIONS_X_mask,
                             REMINDER_ITERATIONS_Y_mask, mask_vector_y,
                             divisor_xy, kernel_x);

        prelude_9x9_16_Ymask_2(N - 7, M, frame1, temp, mask_vector_y,
                               division_case, f, mask_prelude);
        loop_reminder_9x9_16_blur_Y(frame1, filt, temp, N, M, N - 3,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_9x9_16_Xmask(frame1, filt, temp, N, M, N - 3, mask_vector_x,
                             division_case, f, REMINDER_ITERATIONS_X_mask,
                             REMINDER_ITERATIONS_Y_mask, mask_vector_y,
                             divisor_xy, kernel_x);

        prelude_9x9_16_Ymask_3(N - 8, M, frame1, temp, mask_vector_y,
                               division_case, f, mask_prelude);
        loop_reminder_9x9_16_blur_Y(frame1, filt, temp, N, M, N - 4,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);
        prelude_9x9_16_Xmask(frame1, filt, temp, N, M, N - 4, mask_vector_x,
                             division_case, f, REMINDER_ITERATIONS_X_mask,
                             REMINDER_ITERATIONS_Y_mask, mask_vector_y,
                             divisor_xy, kernel_x);

      }

      else { // main loop

        for (col = 0; col <= M - 32; col += 32) {

          // load the 9 rows
          r0 = _mm256_load_si256((__m256i *)&frame1[row - 4][col]);
          r1 = _mm256_load_si256((__m256i *)&frame1[row - 3][col]);
          r2 = _mm256_load_si256((__m256i *)&frame1[row - 2][col]);
          r3 = _mm256_load_si256((__m256i *)&frame1[row - 1][col]);
          r4 = _mm256_load_si256((__m256i *)&frame1[row][col]);
          r5 = _mm256_load_si256((__m256i *)&frame1[row + 1][col]);
          r6 = _mm256_load_si256((__m256i *)&frame1[row + 2][col]);
          r7 = _mm256_load_si256((__m256i *)&frame1[row + 3][col]);
          r8 = _mm256_load_si256((__m256i *)&frame1[row + 4][col]);

          //--------------------y filter begins--------------------

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, cy0);
          m1 = _mm256_maddubs_epi16(r1, cy1);
          m2 = _mm256_maddubs_epi16(r2, cy2);
          m3 = _mm256_maddubs_epi16(r3, cy3);
          m4 = _mm256_maddubs_epi16(r4, cy4);
          m5 = _mm256_maddubs_epi16(r5, cy5);
          m6 = _mm256_maddubs_epi16(r6, cy6);
          m7 = _mm256_maddubs_epi16(r7, cy7);
          m8 = _mm256_maddubs_epi16(r8, cy8);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);
          m0 = _mm256_add_epi16(m0, m5);
          m0 = _mm256_add_epi16(m0, m6);
          m0 = _mm256_add_epi16(m0, m7);
          m0 = _mm256_add_epi16(m0, m8);

          even = division(division_case, m0, f); // even results

          // multiply with the mask
          m0 = _mm256_maddubs_epi16(r0, cy0_sh1);
          m1 = _mm256_maddubs_epi16(r1, cy1_sh1);
          m2 = _mm256_maddubs_epi16(r2, cy2_sh1);
          m3 = _mm256_maddubs_epi16(r3, cy3_sh1);
          m4 = _mm256_maddubs_epi16(r4, cy4_sh1);
          m5 = _mm256_maddubs_epi16(r5, cy5_sh1);
          m6 = _mm256_maddubs_epi16(r6, cy6_sh1);
          m7 = _mm256_maddubs_epi16(r7, cy7_sh1);
          m8 = _mm256_maddubs_epi16(r8, cy8_sh1);

          // vertical add
          m0 = _mm256_add_epi16(m0, m1);
          m0 = _mm256_add_epi16(m0, m2);
          m0 = _mm256_add_epi16(m0, m3);
          m0 = _mm256_add_epi16(m0, m4);
          m0 = _mm256_add_epi16(m0, m5);
          m0 = _mm256_add_epi16(m0, m6);
          m0 = _mm256_add_epi16(m0, m7);
          m0 = _mm256_add_epi16(m0, m8);

          odd = division(division_case, m0, f); // odd results

          // pack to one register
          odd = _mm256_slli_si256(odd, 1); // shift one position right
          r0 = _mm256_add_epi8(even, odd); // add the odd with the even
          // y filter ends - r0 has now the data to be processed by x filter

          _mm256_store_si256((__m256i *)&temp[col], r0);
        }
        loop_reminder_9x9_16_blur_Y(frame1, filt, temp, N, M, row,
                                    REMINDER_ITERATIONS_Y_mask, division_case,
                                    mask_vector_y, f, divisor_xy);

        for (col = 0; col <= M - 32; col += 30) {

          if (col == 0) {

            // 1st col iteration
            __m256i rr0 = _mm256_load_si256((__m256i *)&temp[0]);
            r0 = fill_zeros(rr0, mask_prelude);
            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            even = _mm256_and_si256(m0,
                                    mask_16_1); // in 16-bit format keep only
                                                // 0,5,10 and discard the others

            // 2nd col iteration
            //	r0=_mm256_load_si256( (__m256i *) &temp[0]);
            r0 = fill_3zeros(rr0, mask_prelude_3);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            odd = _mm256_and_si256(m0,
                                   mask_16_1); // in 16-bit format keep only
                                               // 0,5,10 and discard the others

            // 3rd col iteration
            //	r0=_mm256_load_si256( (__m256i *) &temp[0]);
            r0 = fill_2zeros(rr0, mask_prelude_2);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others
            m0 = _mm256_slli_si256(m0, 2);
            even = _mm256_add_epi16(m0, even);

            // 4th col iteration
            // r0=_mm256_load_si256( (__m256i *) &temp[0]);
            r0 = fill_1zeros(rr0, mask_prelude_1);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others
            m0 = _mm256_slli_si256(m0, 2);
            odd = _mm256_add_epi16(m0, odd);

            // 5th col iteration
            // r0=_mm256_load_si256( (__m256i *) &temp[0]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(rr0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others
            m0 = _mm256_slli_si256(m0, 4);
            even = _mm256_add_epi16(m0, even);

            // 6th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[1]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others
            m0 = _mm256_slli_si256(m0, 4);
            odd = _mm256_add_epi16(m0, odd);

            // 7th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[2]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others

            // shift the result 3 positions
            m1 = _mm256_slli_si256(m0, 6);
            m2 = _mm256_and_si256(m0, mask_new_1);
            m2 = _mm256_permute2f128_si256(m2, m2, 1);
            m2 = _mm256_srli_si256(m2, 10);
            m0 = _mm256_add_epi16(m2, m1);

            even = _mm256_add_epi16(m0, even);

            // 8th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[3]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others

            // shift the result 3 positions
            m1 = _mm256_slli_si256(m0, 6);
            m2 = _mm256_and_si256(m0, mask_new_1);
            m2 = _mm256_permute2f128_si256(m2, m2, 1);
            m2 = _mm256_srli_si256(m2, 10);
            m0 = _mm256_add_epi16(m2, m1);

            odd = _mm256_add_epi16(m0, odd);

            // 9th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[4]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others

            // shift the result 4 positions
            m1 = _mm256_slli_si256(m0, 8);
            m2 = _mm256_and_si256(m0, mask_new_1);
            m2 = _mm256_permute2f128_si256(m2, m2, 1);
            m2 = _mm256_srli_si256(m2, 8);
            m0 = _mm256_add_epi16(m2, m1);

            even = _mm256_add_epi16(m0, even);

            // 10th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[5]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others

            // shift the result 4 positions
            m1 = _mm256_slli_si256(m0, 8);
            m2 = _mm256_and_si256(m0, mask_new_1);
            m2 = _mm256_permute2f128_si256(m2, m2, 1);
            m2 = _mm256_srli_si256(m2, 8);
            m0 = _mm256_add_epi16(m2, m1);

            odd = _mm256_add_epi16(m0, odd);

            // division
            even = division(division_case, even, f);
            odd = division(division_case, odd, f);

            // blend even , odd into one register
            odd = _mm256_slli_si256(odd, 1);
            even = _mm256_add_epi16(even, odd);

            _mm256_storeu_si256((__m256i *)&filt[row][0], even);

          } else {

            // col iteration computes output pixels of   0,10,20
            // col+1 iteration computes output pixels of 1,11,21
            // col+2 iteration computes output pixels of 2,12,22
            // col+3 iteration computes output pixels of 3,13,23
            // col+4 iteration computes output pixels of 4,14,24
            // col+5 iteration computes output pixels of 5,15,25
            // col+6 iteration computes output pixels of 6,16,26
            // col+7 iteration computes output pixels of 7,17,27
            // col+8 iteration computes output pixels of 8,18,28
            // col+9 iteration computes output pixels of 9,19,29
            // afterwards, col becomes 30 and repeat the above process

            // 1st col iteration

            // multiply by the mask
            r0 = _mm256_loadu_si256((__m256i *)&temp[col - 4]);
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            even = _mm256_and_si256(m0,
                                    mask_16_1); // in 16-bit format keep only
                                                // 0,5,10 and discard the others

            // 2nd col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col - 3]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            odd = _mm256_and_si256(m0,
                                   mask_16_1); // in 16-bit format keep only
                                               // 0,5,10 and discard the others

            // 3rd col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col - 2]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others
            m0 = _mm256_slli_si256(m0, 2);
            even = _mm256_add_epi16(m0, even);

            // 4th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col - 1]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others
            m0 = _mm256_slli_si256(m0, 2);
            odd = _mm256_add_epi16(m0, odd);

            // 5th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others
            m0 = _mm256_slli_si256(m0, 4);
            even = _mm256_add_epi16(m0, even);

            // 6th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 1]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others
            m0 = _mm256_slli_si256(m0, 4);
            odd = _mm256_add_epi16(m0, odd);

            // 7th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 2]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others

            // shift the result 3 positions
            m1 = _mm256_slli_si256(m0, 6);
            m2 = _mm256_and_si256(m0, mask_new_1);
            m2 = _mm256_permute2f128_si256(m2, m2, 1);
            m2 = _mm256_srli_si256(m2, 10);
            m0 = _mm256_add_epi16(m2, m1);

            even = _mm256_add_epi16(m0, even);

            // 8th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 3]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others

            // shift the result 3 positions
            m1 = _mm256_slli_si256(m0, 6);
            m2 = _mm256_and_si256(m0, mask_new_1);
            m2 = _mm256_permute2f128_si256(m2, m2, 1);
            m2 = _mm256_srli_si256(m2, 10);
            m0 = _mm256_add_epi16(m2, m1);

            odd = _mm256_add_epi16(m0, odd);

            // 9th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 4]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others

            // shift the result 4 positions
            m1 = _mm256_slli_si256(m0, 8);
            m2 = _mm256_and_si256(m0, mask_new_1);
            m2 = _mm256_permute2f128_si256(m2, m2, 1);
            m2 = _mm256_srli_si256(m2, 8);
            m0 = _mm256_add_epi16(m2, m1);

            even = _mm256_add_epi16(m0, even);

            // 10th col iteration
            r0 = _mm256_loadu_si256((__m256i *)&temp[col + 5]);

            // multiply by the mask
            m0 = _mm256_maddubs_epi16(r0, cx0);

            // first horizontal add (complex shift is needed)
            m3 = _mm256_srli_si256(m0, 2);
            m4 = _mm256_and_si256(m0, mask3);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 14);
            m3 = _mm256_add_epi16(m3, m4);
            m3 = _mm256_add_epi16(m0, m3); // add

            // second horizontal add
            m1 = _mm256_srli_si256(m3, 4);
            m3 = _mm256_add_epi16(m3, m1); // add

            // third horizontal add
            m0 = _mm256_and_si256(m0, mask_16_5);
            m2 = _mm256_srli_si256(m0, 8);
            m4 = _mm256_and_si256(m0, mask3_2);
            m4 = _mm256_permute2f128_si256(m4, m4, 1);
            m4 = _mm256_slli_si256(m4, 8);
            m2 = _mm256_add_epi16(m2, m4);

            m0 = _mm256_add_epi16(m2, m3); // add

            m0 = _mm256_and_si256(m0,
                                  mask_16_1); // in 16-bit format keep only
                                              // 0,5,10 and discard the others

            // shift the result 4 positions
            m1 = _mm256_slli_si256(m0, 8);
            m2 = _mm256_and_si256(m0, mask_new_1);
            m2 = _mm256_permute2f128_si256(m2, m2, 1);
            m2 = _mm256_srli_si256(m2, 8);
            m0 = _mm256_add_epi16(m2, m1);

            odd = _mm256_add_epi16(m0, odd);

            // division
            even = division(division_case, even, f);
            odd = division(division_case, odd, f);

            // blend even , odd into one register
            odd = _mm256_slli_si256(odd, 1);
            even = _mm256_add_epi16(even, odd);

            _mm256_storeu_si256((__m256i *)&filt[row][col], even);
          }
        }

        loop_reminder_9x9_16_blur_X(frame1, filt, temp, N, M, row, col,
                                    REMINDER_ITERATIONS_X_mask, division_case,
                                    mask_vector_x, f, divisor_xy, kernel_x);
      }
    }

  } // end of parallel
}

int loop_reminder_3x3_new(unsigned char **frame1, unsigned char **filt,
                          const unsigned int M, const unsigned int N,
                          const unsigned int row, const unsigned int col,
                          const unsigned int REMINDER_ITERATIONS,
                          const unsigned int division_case,
                          const unsigned short int divisor,
                          signed char **filter, const __m256i c0,
                          const __m256i c1, const __m256i f) {

  register __m256i r0, r1, r2, m0, m1, m2, output_even, output_odd;
  int newPixel = 0;
  const __m256i output_mask =
      _mm256_set_epi16(0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0, 65535);

  __m256i reminder_mask1;

  // 1st col iteration

  // load the 3 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col - 1]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[row][col - 1]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col - 1]);

  // AND r0-r3 with reminder_mask
  reminder_mask1 = _mm256_load_si256(
      (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 1][0]);
  r0 = _mm256_and_si256(r0, reminder_mask1);
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  // later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit
  // register).
  output_even = _mm256_and_si256(m2, output_mask);

  // 2ND col iteration

  // load the 3 rows
  r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col]);
  r1 = _mm256_loadu_si256((__m256i *)&frame1[row][col]);
  r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col]);

  // AND r0-r3 with reminder_mask
  reminder_mask1 = _mm256_load_si256(
      (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 2][0]);
  r0 = _mm256_and_si256(r0, reminder_mask1);
  r1 = _mm256_and_si256(r1, reminder_mask1);
  r2 = _mm256_and_si256(r2, reminder_mask1);

  // multiply with the mask
  m0 = _mm256_maddubs_epi16(r0, c0);
  m1 = _mm256_maddubs_epi16(r1, c1);
  m2 = _mm256_maddubs_epi16(r2, c0);

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);
  m0 = _mm256_add_epi16(m0, m2);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  // later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit
  // register).
  output_odd = _mm256_and_si256(m2, output_mask);

  if (REMINDER_ITERATIONS >
      2) { // this prevents  reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]) access
           // -1
    // 3rd col iteration
    // load the 3 rows
    r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col + 1]);
    r1 = _mm256_loadu_si256((__m256i *)&frame1[row][col + 1]);
    r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col + 1]);

    // AND r0-r3 with reminder_mask
    reminder_mask1 = _mm256_load_si256(
        (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 3][0]);
    r0 = _mm256_and_si256(r0, reminder_mask1);
    r1 = _mm256_and_si256(r1, reminder_mask1);
    r2 = _mm256_and_si256(r2, reminder_mask1);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, c0);
    m1 = _mm256_maddubs_epi16(r1, c1);
    m2 = _mm256_maddubs_epi16(r2, c0);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);

    // hozizontal additions
    m1 = _mm256_srli_si256(m0, 2);
    m2 = _mm256_add_epi16(m1, m0);

    // m2 has 16 16bit values now. the results I need are in positions
    // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
    // others. later on they will be stored into 0,4,8,12,16,20,24,28 positions
    // (8bit register).
    m2 = _mm256_and_si256(m2, output_mask);
    m2 = _mm256_slli_si256(m2, 2);
    output_even = _mm256_add_epi16(output_even, m2);
  }

  if (REMINDER_ITERATIONS > 3) {
    // 4th col iteration

    // load the 3 rows
    r0 = _mm256_loadu_si256((__m256i *)&frame1[row - 1][col + 2]);
    r1 = _mm256_loadu_si256((__m256i *)&frame1[row][col + 2]);
    r2 = _mm256_loadu_si256((__m256i *)&frame1[row + 1][col + 2]);

    // AND r0-r3 with reminder_mask
    reminder_mask1 = _mm256_load_si256(
        (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 4][0]);
    r0 = _mm256_and_si256(r0, reminder_mask1);
    r1 = _mm256_and_si256(r1, reminder_mask1);
    r2 = _mm256_and_si256(r2, reminder_mask1);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, c0);
    m1 = _mm256_maddubs_epi16(r1, c1);
    m2 = _mm256_maddubs_epi16(r2, c0);

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);
    m0 = _mm256_add_epi16(m0, m2);

    // hozizontal additions
    m1 = _mm256_srli_si256(m0, 2);
    m2 = _mm256_add_epi16(m1, m0);

    // m2 has 16 16bit values now. the results I need are in positions
    // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
    // others. later on they will be stored into 0,4,8,12,16,20,24,28 positions
    // (8bit register).
    m2 = _mm256_and_si256(m2, output_mask);
    m2 = _mm256_slli_si256(m2, 2);
    output_odd = _mm256_add_epi16(output_odd, m2);
  }

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  switch (REMINDER_ITERATIONS) {

  case 2:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    break;
  case 3:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    break;
  case 4:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    break;
  case 5:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    break;
  case 6:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    break;
  case 7:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    break;
  case 8:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    break;
  case 9:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    break;
  case 10:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    break;
  case 11:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    break;
  case 12:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    break;
  case 13:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    break;
  case 14:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);
    break;
  case 15:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);
    filt[row][col + 14] = (unsigned char)_mm256_extract_epi8(output_even, 14);
    break;
  case 16:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    break;
  case 17:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    break;
  case 18:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    break;
  case 19:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    break;
  case 20:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    break;
  case 21:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    break;
  case 22:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    break;
  case 23:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    break;
  case 24:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    break;
  case 25:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    break;
  case 26:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    break;
  case 27:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    break;
  case 28:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    break;
  case 29:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    break;
  case 30:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output_even, 29);

    break;

  case 31:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output_even, 29);
    filt[row][col + 30] = (unsigned char)_mm256_extract_epi8(output_even, 30);

    break;
  case 32:
    _mm256_storeu_si256((__m256i *)&filt[row][col], output_even);

    break;
  case 33:
    _mm256_storeu_si256((__m256i *)&filt[row][col], output_even);

    // the filt[row][col+32] is computed unvectorized
    newPixel = 0;
    newPixel += frame1[row - 1][M - 1 - 1] * filter[0][0];
    newPixel += frame1[row - 1][M - 1] * filter[0][1];

    newPixel += frame1[row][M - 1 - 1] * filter[1][0];
    newPixel += frame1[row][M - 1] * filter[1][1];

    newPixel += frame1[row + 1][M - 1 - 1] * filter[2][0];
    newPixel += frame1[row + 1][M - 1] * filter[2][1];

    filt[row][M - 1] = (unsigned char)(newPixel / divisor);
    break;
  default:
    printf("\nsomething went wrong");
    return -1;
  }

  return 0;
}

// this routine works only for row=0 and row=N-1
int loop_reminder_3x3_new_first_last_rows(
    unsigned char **frame1, unsigned char **filt, const unsigned int M,
    const unsigned int N, const unsigned int row, const unsigned int col,
    const unsigned int REMINDER_ITERATIONS, const unsigned int division_case,
    const unsigned short int divisor, signed char **filter, const __m256i c0,
    const __m256i c1, const __m256i f) {

  register __m256i r0, r1, m0, m1, m2, output_even, output_odd;
  int newPixel = 0;
  const __m256i output_mask =
      _mm256_set_epi16(0, 65535, 0, 65535, 0, 65535, 0, 65535, 0, 65535, 0,
                       65535, 0, 65535, 0, 65535);

  __m256i reminder_mask1;

  // 1st col iteration
  if (row == 0) {
    r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col - 1]);
    r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col - 1]);
    reminder_mask1 = _mm256_load_si256(
        (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 1][0]);
    r0 = _mm256_and_si256(r0, reminder_mask1);
    r1 = _mm256_and_si256(r1, reminder_mask1);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, c1);
    m1 = _mm256_maddubs_epi16(r1, c0);
  } else { // if row=N-1
    r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col - 1]);
    r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col - 1]);
    reminder_mask1 = _mm256_load_si256(
        (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 1][0]);
    r0 = _mm256_and_si256(r0, reminder_mask1);
    r1 = _mm256_and_si256(r1, reminder_mask1);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, c0);
    m1 = _mm256_maddubs_epi16(r1, c1);
  }

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  // later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit
  // register).
  output_even = _mm256_and_si256(m2, output_mask);

  // 2ND col iteration
  if (row == 0) {
    r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col]);
    r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col]);
    reminder_mask1 = _mm256_load_si256(
        (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 2][0]);
    r0 = _mm256_and_si256(r0, reminder_mask1);
    r1 = _mm256_and_si256(r1, reminder_mask1);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, c1);
    m1 = _mm256_maddubs_epi16(r1, c0);
  } else { // if row=N-1
    r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col]);
    r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col]);
    reminder_mask1 = _mm256_load_si256(
        (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 2][0]);
    r0 = _mm256_and_si256(r0, reminder_mask1);
    r1 = _mm256_and_si256(r1, reminder_mask1);

    // multiply with the mask
    m0 = _mm256_maddubs_epi16(r0, c0);
    m1 = _mm256_maddubs_epi16(r1, c1);
  }

  // vertical add
  m0 = _mm256_add_epi16(m0, m1);

  // hozizontal additions
  m1 = _mm256_srli_si256(m0, 2);
  m2 = _mm256_add_epi16(m1, m0);

  // m2 has 16 16bit values now. the results I need are in positions
  // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard others.
  // later on they will be stored into 0,4,8,12,16,20,24,28 positions (8bit
  // register).
  output_odd = _mm256_and_si256(m2, output_mask);

  if (REMINDER_ITERATIONS >
      2) { // this prevents  reminder_msk1_3x3[REMINDER_ITERATIONS-3][0]) access
           // -1
    // 3rd col iteration
    if (row == 0) {
      r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col + 1]);
      r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col + 1]);
      reminder_mask1 = _mm256_load_si256(
          (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 3][0]);
      r0 = _mm256_and_si256(r0, reminder_mask1);
      r1 = _mm256_and_si256(r1, reminder_mask1);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, c1);
      m1 = _mm256_maddubs_epi16(r1, c0);
    } else { // if row=N-1
      r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col + 1]);
      r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col + 1]);
      reminder_mask1 = _mm256_load_si256(
          (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 3][0]);
      r0 = _mm256_and_si256(r0, reminder_mask1);
      r1 = _mm256_and_si256(r1, reminder_mask1);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, c0);
      m1 = _mm256_maddubs_epi16(r1, c1);
    }

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);

    // hozizontal additions
    m1 = _mm256_srli_si256(m0, 2);
    m2 = _mm256_add_epi16(m1, m0);

    // m2 has 16 16bit values now. the results I need are in positions
    // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
    // others. later on they will be stored into 0,4,8,12,16,20,24,28 positions
    // (8bit register).
    m2 = _mm256_and_si256(m2, output_mask);
    m2 = _mm256_slli_si256(m2, 2);
    output_even = _mm256_add_epi16(output_even, m2);
  }

  if (REMINDER_ITERATIONS > 3) {
    // 4th col iteration

    if (row == 0) {
      r0 = _mm256_loadu_si256((__m256i *)&frame1[0][col + 2]);
      r1 = _mm256_loadu_si256((__m256i *)&frame1[1][col + 2]);
      reminder_mask1 = _mm256_load_si256(
          (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 4][0]);
      r0 = _mm256_and_si256(r0, reminder_mask1);
      r1 = _mm256_and_si256(r1, reminder_mask1);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, c1);
      m1 = _mm256_maddubs_epi16(r1, c0);
    } else { // if row=N-1
      r0 = _mm256_loadu_si256((__m256i *)&frame1[N - 2][col + 2]);
      r1 = _mm256_loadu_si256((__m256i *)&frame1[N - 1][col + 2]);
      reminder_mask1 = _mm256_load_si256(
          (__m256i *)&reminder_msk1_3x3[REMINDER_ITERATIONS - 4][0]);
      r0 = _mm256_and_si256(r0, reminder_mask1);
      r1 = _mm256_and_si256(r1, reminder_mask1);

      // multiply with the mask
      m0 = _mm256_maddubs_epi16(r0, c0);
      m1 = _mm256_maddubs_epi16(r1, c1);
    }

    // vertical add
    m0 = _mm256_add_epi16(m0, m1);

    // hozizontal additions
    m1 = _mm256_srli_si256(m0, 2);
    m2 = _mm256_add_epi16(m1, m0);

    // m2 has 16 16bit values now. the results I need are in positions
    // 0,2,4,6,8,10,12,14 in the 16bit register. keep only those, discard
    // others. later on they will be stored into 0,4,8,12,16,20,24,28 positions
    // (8bit register).
    m2 = _mm256_and_si256(m2, output_mask);
    m2 = _mm256_slli_si256(m2, 2);
    output_odd = _mm256_add_epi16(output_odd, m2);
  }

  // now division follows
  output_even = division(division_case, output_even, f);
  output_odd = division(division_case, output_odd, f);

  // shift odd 1 position and add to even
  output_odd = _mm256_slli_si256(output_odd, 1);
  output_even = _mm256_add_epi8(output_even, output_odd);

  switch (REMINDER_ITERATIONS) {

  case 2:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    break;
  case 3:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    break;
  case 4:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    break;
  case 5:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    break;
  case 6:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    break;
  case 7:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    break;
  case 8:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    break;
  case 9:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    break;
  case 10:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    break;
  case 11:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    break;
  case 12:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    break;
  case 13:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    break;
  case 14:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);
    break;
  case 15:
    filt[row][col] = (unsigned char)_mm256_extract_epi8(output_even, 0);
    filt[row][col + 1] = (unsigned char)_mm256_extract_epi8(output_even, 1);
    filt[row][col + 2] = (unsigned char)_mm256_extract_epi8(output_even, 2);
    filt[row][col + 3] = (unsigned char)_mm256_extract_epi8(output_even, 3);
    filt[row][col + 4] = (unsigned char)_mm256_extract_epi8(output_even, 4);
    filt[row][col + 5] = (unsigned char)_mm256_extract_epi8(output_even, 5);
    filt[row][col + 6] = (unsigned char)_mm256_extract_epi8(output_even, 6);
    filt[row][col + 7] = (unsigned char)_mm256_extract_epi8(output_even, 7);
    filt[row][col + 8] = (unsigned char)_mm256_extract_epi8(output_even, 8);
    filt[row][col + 9] = (unsigned char)_mm256_extract_epi8(output_even, 9);
    filt[row][col + 10] = (unsigned char)_mm256_extract_epi8(output_even, 10);
    filt[row][col + 11] = (unsigned char)_mm256_extract_epi8(output_even, 11);
    filt[row][col + 12] = (unsigned char)_mm256_extract_epi8(output_even, 12);
    filt[row][col + 13] = (unsigned char)_mm256_extract_epi8(output_even, 13);
    filt[row][col + 14] = (unsigned char)_mm256_extract_epi8(output_even, 14);
    break;
  case 16:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    break;
  case 17:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    break;
  case 18:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    break;
  case 19:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    break;
  case 20:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    break;
  case 21:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    break;
  case 22:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    break;
  case 23:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    break;
  case 24:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    break;
  case 25:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    break;
  case 26:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    break;
  case 27:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    break;
  case 28:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    break;
  case 29:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    break;
  case 30:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output_even, 29);

    break;

  case 31:
    _mm_storeu_si128((__m128i *)&filt[row][col],
                     _mm256_extractf128_si256(
                         output_even, 0)); // store low 128bit - 16pixels
    filt[row][col + 16] = (unsigned char)_mm256_extract_epi8(output_even, 16);
    filt[row][col + 17] = (unsigned char)_mm256_extract_epi8(output_even, 17);
    filt[row][col + 18] = (unsigned char)_mm256_extract_epi8(output_even, 18);
    filt[row][col + 19] = (unsigned char)_mm256_extract_epi8(output_even, 19);
    filt[row][col + 20] = (unsigned char)_mm256_extract_epi8(output_even, 20);
    filt[row][col + 21] = (unsigned char)_mm256_extract_epi8(output_even, 21);
    filt[row][col + 22] = (unsigned char)_mm256_extract_epi8(output_even, 22);
    filt[row][col + 23] = (unsigned char)_mm256_extract_epi8(output_even, 23);
    filt[row][col + 24] = (unsigned char)_mm256_extract_epi8(output_even, 24);
    filt[row][col + 25] = (unsigned char)_mm256_extract_epi8(output_even, 25);
    filt[row][col + 26] = (unsigned char)_mm256_extract_epi8(output_even, 26);
    filt[row][col + 27] = (unsigned char)_mm256_extract_epi8(output_even, 27);
    filt[row][col + 28] = (unsigned char)_mm256_extract_epi8(output_even, 28);
    filt[row][col + 29] = (unsigned char)_mm256_extract_epi8(output_even, 29);
    filt[row][col + 30] = (unsigned char)_mm256_extract_epi8(output_even, 30);

    break;
  case 32:
    _mm256_storeu_si256((__m256i *)&filt[row][col], output_even);

    break;
  case 33:
    _mm256_storeu_si256((__m256i *)&filt[row][col], output_even);

    // the filt[row][col+32] is computed unvectorized
    newPixel = 0;

    if (row == N - 1) {
      newPixel += frame1[row - 1][M - 1 - 1] * filter[0][0];
      newPixel += frame1[row - 1][M - 1] * filter[0][1];
    }

    newPixel += frame1[row][M - 1 - 1] * filter[1][0];
    newPixel += frame1[row][M - 1] * filter[1][1];

    if (row == 0) {
      newPixel += frame1[row + 1][M - 1 - 1] * filter[2][0];
      newPixel += frame1[row + 1][M - 1] * filter[2][1];
    }

    filt[row][M - 1] = (unsigned char)(newPixel / divisor);
    break;
  default:
    printf("\nsomething went wrong");
    return -1;
  }

  return 0;
}

inline __m256i insert_two_zeros_front(__m256i input, __m256i mask_prelude) {

  __m256i m0 =
      _mm256_slli_si256(input, 2); // shift 3 elements left - equivalent to
                                   // filling with two zeros inthe beginning

  __m256i r0 = _mm256_and_si256(input, mask_prelude);
  r0 = _mm256_permute2f128_si256(r0, r0, 1);
  r0 = _mm256_srli_si256(r0, 14);
  return _mm256_add_epi16(m0, r0);
}

inline __m256i insert_one_zeros_front(__m256i input, __m256i mask_prelude) {

  __m256i m0 =
      _mm256_slli_si256(input, 1); // shift 3 elements left - equivalent to
                                   // filling with two zeros inthe beginning

  __m256i r0 = _mm256_and_si256(input, mask_prelude);
  r0 = _mm256_permute2f128_si256(r0, r0, 1);
  r0 = _mm256_srli_si256(r0, 15);
  return _mm256_add_epi16(m0, r0);
}

} // namespace cv
#endif // HAVE_VK_SMOOTH
